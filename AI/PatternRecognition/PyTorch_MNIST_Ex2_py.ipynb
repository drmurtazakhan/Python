{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PaQg9EirEZi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RO6virngrZd8",
    "outputId": "86c893e0-8877-4f97-9880-9d4656338fb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 15.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 469kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.24MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.78MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.MNIST(root=\".\", train=True, download=True, transform=ToTensor())\n",
    "\n",
    "test_data = datasets.MNIST(root=\".\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miQlO40srfv9"
   },
   "outputs": [],
   "source": [
    "count_train = len(training_data)\n",
    "count_test = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "-BhlVOOIrlZa",
    "outputId": "56dfda22-1d91-4809-c1ee-1f2655fa4b34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7b001514ec20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot a random image from training data\n",
    "fig1 = plt.figure(figsize=(8, 8))\n",
    "sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "img, label = training_data[sample_idx]\n",
    "plt.title(label)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "6YBRc3JsrnZS",
    "outputId": "2dca6ae7-a93c-48c7-ee13-f74c44e80066"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot n*n random images from training data in a grid\n",
    "n = 3\n",
    "fig2 = plt.figure(figsize=(8, 8))\n",
    "cols, rows = n, n\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(count_train, size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    ax2 = fig2.add_subplot(rows, cols, i)\n",
    "    ax2.title.set_text(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbEbmlmcrtZp"
   },
   "outputs": [],
   "source": [
    "## use the DataLoader to iterate over the dataset in mini-batches\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaded_train = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "loaded_test = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xIyuAGpr0YB"
   },
   "outputs": [],
   "source": [
    "## Build Neural Networks\n",
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mJ7MeKvr2FJ",
    "outputId": "7353464a-6e9a-4db5-a0c7-db32e657aff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##  instantiates  model:\n",
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0HQ0ur2r766"
   },
   "outputs": [],
   "source": [
    "## Define Cross-entropy is a common loss function\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_jnXl7Sr-s9"
   },
   "outputs": [],
   "source": [
    "## set an optimization algorithm\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TefO755sB5R"
   },
   "outputs": [],
   "source": [
    "## Training the Neural Network\n",
    "## looping through the data one batch at a time, using the optimizer to adjust the model, and computing the prediction and the loss\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        train_loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch:\n",
    "            train_loss, current = train_loss.item(), batch * len(X)\n",
    "            print(f\"Training loss: {train_loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYYAIzTBsEmp"
   },
   "outputs": [],
   "source": [
    "## test function, which computes the accuracy and the loss\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbHrol8KsIsK",
    "outputId": "e6ac41e6-7017-4418-c546-b62c31bb1c9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.259551  [   64/60000]\n",
      "Training loss: 2.270809  [  128/60000]\n",
      "Training loss: 2.242649  [  192/60000]\n",
      "Training loss: 2.260238  [  256/60000]\n",
      "Training loss: 2.253704  [  320/60000]\n",
      "Training loss: 2.255743  [  384/60000]\n",
      "Training loss: 2.261823  [  448/60000]\n",
      "Training loss: 2.250483  [  512/60000]\n",
      "Training loss: 2.249640  [  576/60000]\n",
      "Training loss: 2.259945  [  640/60000]\n",
      "Training loss: 2.260254  [  704/60000]\n",
      "Training loss: 2.250154  [  768/60000]\n",
      "Training loss: 2.257313  [  832/60000]\n",
      "Training loss: 2.266354  [  896/60000]\n",
      "Training loss: 2.261247  [  960/60000]\n",
      "Training loss: 2.263268  [ 1024/60000]\n",
      "Training loss: 2.255866  [ 1088/60000]\n",
      "Training loss: 2.257094  [ 1152/60000]\n",
      "Training loss: 2.252412  [ 1216/60000]\n",
      "Training loss: 2.258961  [ 1280/60000]\n",
      "Training loss: 2.261002  [ 1344/60000]\n",
      "Training loss: 2.260253  [ 1408/60000]\n",
      "Training loss: 2.262609  [ 1472/60000]\n",
      "Training loss: 2.261450  [ 1536/60000]\n",
      "Training loss: 2.256626  [ 1600/60000]\n",
      "Training loss: 2.261574  [ 1664/60000]\n",
      "Training loss: 2.260465  [ 1728/60000]\n",
      "Training loss: 2.247277  [ 1792/60000]\n",
      "Training loss: 2.256635  [ 1856/60000]\n",
      "Training loss: 2.269407  [ 1920/60000]\n",
      "Training loss: 2.246954  [ 1984/60000]\n",
      "Training loss: 2.261195  [ 2048/60000]\n",
      "Training loss: 2.260221  [ 2112/60000]\n",
      "Training loss: 2.257374  [ 2176/60000]\n",
      "Training loss: 2.249715  [ 2240/60000]\n",
      "Training loss: 2.254253  [ 2304/60000]\n",
      "Training loss: 2.254303  [ 2368/60000]\n",
      "Training loss: 2.256912  [ 2432/60000]\n",
      "Training loss: 2.257755  [ 2496/60000]\n",
      "Training loss: 2.260563  [ 2560/60000]\n",
      "Training loss: 2.250567  [ 2624/60000]\n",
      "Training loss: 2.256677  [ 2688/60000]\n",
      "Training loss: 2.247053  [ 2752/60000]\n",
      "Training loss: 2.268889  [ 2816/60000]\n",
      "Training loss: 2.262342  [ 2880/60000]\n",
      "Training loss: 2.260445  [ 2944/60000]\n",
      "Training loss: 2.256529  [ 3008/60000]\n",
      "Training loss: 2.257262  [ 3072/60000]\n",
      "Training loss: 2.254576  [ 3136/60000]\n",
      "Training loss: 2.253670  [ 3200/60000]\n",
      "Training loss: 2.254658  [ 3264/60000]\n",
      "Training loss: 2.256731  [ 3328/60000]\n",
      "Training loss: 2.258346  [ 3392/60000]\n",
      "Training loss: 2.259813  [ 3456/60000]\n",
      "Training loss: 2.266668  [ 3520/60000]\n",
      "Training loss: 2.253930  [ 3584/60000]\n",
      "Training loss: 2.253802  [ 3648/60000]\n",
      "Training loss: 2.264196  [ 3712/60000]\n",
      "Training loss: 2.265798  [ 3776/60000]\n",
      "Training loss: 2.253982  [ 3840/60000]\n",
      "Training loss: 2.251782  [ 3904/60000]\n",
      "Training loss: 2.259431  [ 3968/60000]\n",
      "Training loss: 2.245565  [ 4032/60000]\n",
      "Training loss: 2.245840  [ 4096/60000]\n",
      "Training loss: 2.241173  [ 4160/60000]\n",
      "Training loss: 2.244643  [ 4224/60000]\n",
      "Training loss: 2.263329  [ 4288/60000]\n",
      "Training loss: 2.264361  [ 4352/60000]\n",
      "Training loss: 2.260016  [ 4416/60000]\n",
      "Training loss: 2.265757  [ 4480/60000]\n",
      "Training loss: 2.257774  [ 4544/60000]\n",
      "Training loss: 2.263994  [ 4608/60000]\n",
      "Training loss: 2.258649  [ 4672/60000]\n",
      "Training loss: 2.253104  [ 4736/60000]\n",
      "Training loss: 2.254938  [ 4800/60000]\n",
      "Training loss: 2.251045  [ 4864/60000]\n",
      "Training loss: 2.251854  [ 4928/60000]\n",
      "Training loss: 2.257624  [ 4992/60000]\n",
      "Training loss: 2.264085  [ 5056/60000]\n",
      "Training loss: 2.253544  [ 5120/60000]\n",
      "Training loss: 2.266958  [ 5184/60000]\n",
      "Training loss: 2.250081  [ 5248/60000]\n",
      "Training loss: 2.253678  [ 5312/60000]\n",
      "Training loss: 2.251126  [ 5376/60000]\n",
      "Training loss: 2.259706  [ 5440/60000]\n",
      "Training loss: 2.253171  [ 5504/60000]\n",
      "Training loss: 2.266370  [ 5568/60000]\n",
      "Training loss: 2.255642  [ 5632/60000]\n",
      "Training loss: 2.254709  [ 5696/60000]\n",
      "Training loss: 2.252634  [ 5760/60000]\n",
      "Training loss: 2.250396  [ 5824/60000]\n",
      "Training loss: 2.252916  [ 5888/60000]\n",
      "Training loss: 2.254293  [ 5952/60000]\n",
      "Training loss: 2.251906  [ 6016/60000]\n",
      "Training loss: 2.254590  [ 6080/60000]\n",
      "Training loss: 2.247727  [ 6144/60000]\n",
      "Training loss: 2.247077  [ 6208/60000]\n",
      "Training loss: 2.263325  [ 6272/60000]\n",
      "Training loss: 2.260316  [ 6336/60000]\n",
      "Training loss: 2.253901  [ 6400/60000]\n",
      "Training loss: 2.251994  [ 6464/60000]\n",
      "Training loss: 2.252478  [ 6528/60000]\n",
      "Training loss: 2.257229  [ 6592/60000]\n",
      "Training loss: 2.257461  [ 6656/60000]\n",
      "Training loss: 2.245915  [ 6720/60000]\n",
      "Training loss: 2.256476  [ 6784/60000]\n",
      "Training loss: 2.263388  [ 6848/60000]\n",
      "Training loss: 2.258361  [ 6912/60000]\n",
      "Training loss: 2.252585  [ 6976/60000]\n",
      "Training loss: 2.248486  [ 7040/60000]\n",
      "Training loss: 2.250410  [ 7104/60000]\n",
      "Training loss: 2.246469  [ 7168/60000]\n",
      "Training loss: 2.244186  [ 7232/60000]\n",
      "Training loss: 2.254133  [ 7296/60000]\n",
      "Training loss: 2.256995  [ 7360/60000]\n",
      "Training loss: 2.247224  [ 7424/60000]\n",
      "Training loss: 2.253278  [ 7488/60000]\n",
      "Training loss: 2.261950  [ 7552/60000]\n",
      "Training loss: 2.253235  [ 7616/60000]\n",
      "Training loss: 2.258310  [ 7680/60000]\n",
      "Training loss: 2.245374  [ 7744/60000]\n",
      "Training loss: 2.243959  [ 7808/60000]\n",
      "Training loss: 2.243978  [ 7872/60000]\n",
      "Training loss: 2.242395  [ 7936/60000]\n",
      "Training loss: 2.245955  [ 8000/60000]\n",
      "Training loss: 2.259692  [ 8064/60000]\n",
      "Training loss: 2.248885  [ 8128/60000]\n",
      "Training loss: 2.249783  [ 8192/60000]\n",
      "Training loss: 2.257439  [ 8256/60000]\n",
      "Training loss: 2.259216  [ 8320/60000]\n",
      "Training loss: 2.255820  [ 8384/60000]\n",
      "Training loss: 2.258114  [ 8448/60000]\n",
      "Training loss: 2.250399  [ 8512/60000]\n",
      "Training loss: 2.253325  [ 8576/60000]\n",
      "Training loss: 2.259493  [ 8640/60000]\n",
      "Training loss: 2.265702  [ 8704/60000]\n",
      "Training loss: 2.255510  [ 8768/60000]\n",
      "Training loss: 2.258834  [ 8832/60000]\n",
      "Training loss: 2.243766  [ 8896/60000]\n",
      "Training loss: 2.250223  [ 8960/60000]\n",
      "Training loss: 2.252508  [ 9024/60000]\n",
      "Training loss: 2.255052  [ 9088/60000]\n",
      "Training loss: 2.244192  [ 9152/60000]\n",
      "Training loss: 2.257765  [ 9216/60000]\n",
      "Training loss: 2.246004  [ 9280/60000]\n",
      "Training loss: 2.245222  [ 9344/60000]\n",
      "Training loss: 2.247279  [ 9408/60000]\n",
      "Training loss: 2.255492  [ 9472/60000]\n",
      "Training loss: 2.248968  [ 9536/60000]\n",
      "Training loss: 2.250698  [ 9600/60000]\n",
      "Training loss: 2.256718  [ 9664/60000]\n",
      "Training loss: 2.253457  [ 9728/60000]\n",
      "Training loss: 2.243850  [ 9792/60000]\n",
      "Training loss: 2.246593  [ 9856/60000]\n",
      "Training loss: 2.253667  [ 9920/60000]\n",
      "Training loss: 2.248420  [ 9984/60000]\n",
      "Training loss: 2.251674  [10048/60000]\n",
      "Training loss: 2.252793  [10112/60000]\n",
      "Training loss: 2.250696  [10176/60000]\n",
      "Training loss: 2.253351  [10240/60000]\n",
      "Training loss: 2.259737  [10304/60000]\n",
      "Training loss: 2.241402  [10368/60000]\n",
      "Training loss: 2.261810  [10432/60000]\n",
      "Training loss: 2.250776  [10496/60000]\n",
      "Training loss: 2.245023  [10560/60000]\n",
      "Training loss: 2.250281  [10624/60000]\n",
      "Training loss: 2.242627  [10688/60000]\n",
      "Training loss: 2.252672  [10752/60000]\n",
      "Training loss: 2.252476  [10816/60000]\n",
      "Training loss: 2.255215  [10880/60000]\n",
      "Training loss: 2.252541  [10944/60000]\n",
      "Training loss: 2.256963  [11008/60000]\n",
      "Training loss: 2.244862  [11072/60000]\n",
      "Training loss: 2.252142  [11136/60000]\n",
      "Training loss: 2.245744  [11200/60000]\n",
      "Training loss: 2.250019  [11264/60000]\n",
      "Training loss: 2.248770  [11328/60000]\n",
      "Training loss: 2.248854  [11392/60000]\n",
      "Training loss: 2.253374  [11456/60000]\n",
      "Training loss: 2.252004  [11520/60000]\n",
      "Training loss: 2.251992  [11584/60000]\n",
      "Training loss: 2.241538  [11648/60000]\n",
      "Training loss: 2.250513  [11712/60000]\n",
      "Training loss: 2.252857  [11776/60000]\n",
      "Training loss: 2.237863  [11840/60000]\n",
      "Training loss: 2.247341  [11904/60000]\n",
      "Training loss: 2.229908  [11968/60000]\n",
      "Training loss: 2.247244  [12032/60000]\n",
      "Training loss: 2.246999  [12096/60000]\n",
      "Training loss: 2.250498  [12160/60000]\n",
      "Training loss: 2.236864  [12224/60000]\n",
      "Training loss: 2.261218  [12288/60000]\n",
      "Training loss: 2.236649  [12352/60000]\n",
      "Training loss: 2.251988  [12416/60000]\n",
      "Training loss: 2.254999  [12480/60000]\n",
      "Training loss: 2.258497  [12544/60000]\n",
      "Training loss: 2.248309  [12608/60000]\n",
      "Training loss: 2.239637  [12672/60000]\n",
      "Training loss: 2.246974  [12736/60000]\n",
      "Training loss: 2.246444  [12800/60000]\n",
      "Training loss: 2.254008  [12864/60000]\n",
      "Training loss: 2.239826  [12928/60000]\n",
      "Training loss: 2.256875  [12992/60000]\n",
      "Training loss: 2.240491  [13056/60000]\n",
      "Training loss: 2.251821  [13120/60000]\n",
      "Training loss: 2.252403  [13184/60000]\n",
      "Training loss: 2.245188  [13248/60000]\n",
      "Training loss: 2.252065  [13312/60000]\n",
      "Training loss: 2.242674  [13376/60000]\n",
      "Training loss: 2.252275  [13440/60000]\n",
      "Training loss: 2.246354  [13504/60000]\n",
      "Training loss: 2.240661  [13568/60000]\n",
      "Training loss: 2.255284  [13632/60000]\n",
      "Training loss: 2.250418  [13696/60000]\n",
      "Training loss: 2.242266  [13760/60000]\n",
      "Training loss: 2.249647  [13824/60000]\n",
      "Training loss: 2.246043  [13888/60000]\n",
      "Training loss: 2.245190  [13952/60000]\n",
      "Training loss: 2.250716  [14016/60000]\n",
      "Training loss: 2.255281  [14080/60000]\n",
      "Training loss: 2.249820  [14144/60000]\n",
      "Training loss: 2.255185  [14208/60000]\n",
      "Training loss: 2.246133  [14272/60000]\n",
      "Training loss: 2.266502  [14336/60000]\n",
      "Training loss: 2.241776  [14400/60000]\n",
      "Training loss: 2.253408  [14464/60000]\n",
      "Training loss: 2.234314  [14528/60000]\n",
      "Training loss: 2.251382  [14592/60000]\n",
      "Training loss: 2.251609  [14656/60000]\n",
      "Training loss: 2.245309  [14720/60000]\n",
      "Training loss: 2.235962  [14784/60000]\n",
      "Training loss: 2.239614  [14848/60000]\n",
      "Training loss: 2.247746  [14912/60000]\n",
      "Training loss: 2.252592  [14976/60000]\n",
      "Training loss: 2.244399  [15040/60000]\n",
      "Training loss: 2.249746  [15104/60000]\n",
      "Training loss: 2.245256  [15168/60000]\n",
      "Training loss: 2.245698  [15232/60000]\n",
      "Training loss: 2.243586  [15296/60000]\n",
      "Training loss: 2.261354  [15360/60000]\n",
      "Training loss: 2.240857  [15424/60000]\n",
      "Training loss: 2.252600  [15488/60000]\n",
      "Training loss: 2.245447  [15552/60000]\n",
      "Training loss: 2.243843  [15616/60000]\n",
      "Training loss: 2.238778  [15680/60000]\n",
      "Training loss: 2.246610  [15744/60000]\n",
      "Training loss: 2.235922  [15808/60000]\n",
      "Training loss: 2.243110  [15872/60000]\n",
      "Training loss: 2.247417  [15936/60000]\n",
      "Training loss: 2.240263  [16000/60000]\n",
      "Training loss: 2.244305  [16064/60000]\n",
      "Training loss: 2.240620  [16128/60000]\n",
      "Training loss: 2.240715  [16192/60000]\n",
      "Training loss: 2.244601  [16256/60000]\n",
      "Training loss: 2.247037  [16320/60000]\n",
      "Training loss: 2.244314  [16384/60000]\n",
      "Training loss: 2.256985  [16448/60000]\n",
      "Training loss: 2.256954  [16512/60000]\n",
      "Training loss: 2.246864  [16576/60000]\n",
      "Training loss: 2.238283  [16640/60000]\n",
      "Training loss: 2.249476  [16704/60000]\n",
      "Training loss: 2.246965  [16768/60000]\n",
      "Training loss: 2.244780  [16832/60000]\n",
      "Training loss: 2.243419  [16896/60000]\n",
      "Training loss: 2.247929  [16960/60000]\n",
      "Training loss: 2.246609  [17024/60000]\n",
      "Training loss: 2.247339  [17088/60000]\n",
      "Training loss: 2.249002  [17152/60000]\n",
      "Training loss: 2.235750  [17216/60000]\n",
      "Training loss: 2.236988  [17280/60000]\n",
      "Training loss: 2.246916  [17344/60000]\n",
      "Training loss: 2.246075  [17408/60000]\n",
      "Training loss: 2.232007  [17472/60000]\n",
      "Training loss: 2.237062  [17536/60000]\n",
      "Training loss: 2.244667  [17600/60000]\n",
      "Training loss: 2.249538  [17664/60000]\n",
      "Training loss: 2.244762  [17728/60000]\n",
      "Training loss: 2.240115  [17792/60000]\n",
      "Training loss: 2.247770  [17856/60000]\n",
      "Training loss: 2.246426  [17920/60000]\n",
      "Training loss: 2.235205  [17984/60000]\n",
      "Training loss: 2.245075  [18048/60000]\n",
      "Training loss: 2.250785  [18112/60000]\n",
      "Training loss: 2.238236  [18176/60000]\n",
      "Training loss: 2.245422  [18240/60000]\n",
      "Training loss: 2.241544  [18304/60000]\n",
      "Training loss: 2.242427  [18368/60000]\n",
      "Training loss: 2.232430  [18432/60000]\n",
      "Training loss: 2.250541  [18496/60000]\n",
      "Training loss: 2.246410  [18560/60000]\n",
      "Training loss: 2.249987  [18624/60000]\n",
      "Training loss: 2.240043  [18688/60000]\n",
      "Training loss: 2.235795  [18752/60000]\n",
      "Training loss: 2.240858  [18816/60000]\n",
      "Training loss: 2.249233  [18880/60000]\n",
      "Training loss: 2.245680  [18944/60000]\n",
      "Training loss: 2.242118  [19008/60000]\n",
      "Training loss: 2.237755  [19072/60000]\n",
      "Training loss: 2.244256  [19136/60000]\n",
      "Training loss: 2.255608  [19200/60000]\n",
      "Training loss: 2.254074  [19264/60000]\n",
      "Training loss: 2.250208  [19328/60000]\n",
      "Training loss: 2.256586  [19392/60000]\n",
      "Training loss: 2.238738  [19456/60000]\n",
      "Training loss: 2.243517  [19520/60000]\n",
      "Training loss: 2.243199  [19584/60000]\n",
      "Training loss: 2.239897  [19648/60000]\n",
      "Training loss: 2.244617  [19712/60000]\n",
      "Training loss: 2.239802  [19776/60000]\n",
      "Training loss: 2.241697  [19840/60000]\n",
      "Training loss: 2.240243  [19904/60000]\n",
      "Training loss: 2.235006  [19968/60000]\n",
      "Training loss: 2.242821  [20032/60000]\n",
      "Training loss: 2.245100  [20096/60000]\n",
      "Training loss: 2.247031  [20160/60000]\n",
      "Training loss: 2.235446  [20224/60000]\n",
      "Training loss: 2.229185  [20288/60000]\n",
      "Training loss: 2.235106  [20352/60000]\n",
      "Training loss: 2.248873  [20416/60000]\n",
      "Training loss: 2.243724  [20480/60000]\n",
      "Training loss: 2.246912  [20544/60000]\n",
      "Training loss: 2.237838  [20608/60000]\n",
      "Training loss: 2.238301  [20672/60000]\n",
      "Training loss: 2.235637  [20736/60000]\n",
      "Training loss: 2.219957  [20800/60000]\n",
      "Training loss: 2.243810  [20864/60000]\n",
      "Training loss: 2.246774  [20928/60000]\n",
      "Training loss: 2.240382  [20992/60000]\n",
      "Training loss: 2.240904  [21056/60000]\n",
      "Training loss: 2.247096  [21120/60000]\n",
      "Training loss: 2.245610  [21184/60000]\n",
      "Training loss: 2.243187  [21248/60000]\n",
      "Training loss: 2.239058  [21312/60000]\n",
      "Training loss: 2.232939  [21376/60000]\n",
      "Training loss: 2.233462  [21440/60000]\n",
      "Training loss: 2.247693  [21504/60000]\n",
      "Training loss: 2.234486  [21568/60000]\n",
      "Training loss: 2.240278  [21632/60000]\n",
      "Training loss: 2.239143  [21696/60000]\n",
      "Training loss: 2.241544  [21760/60000]\n",
      "Training loss: 2.240794  [21824/60000]\n",
      "Training loss: 2.237427  [21888/60000]\n",
      "Training loss: 2.236522  [21952/60000]\n",
      "Training loss: 2.240376  [22016/60000]\n",
      "Training loss: 2.237711  [22080/60000]\n",
      "Training loss: 2.241291  [22144/60000]\n",
      "Training loss: 2.243549  [22208/60000]\n",
      "Training loss: 2.251043  [22272/60000]\n",
      "Training loss: 2.245087  [22336/60000]\n",
      "Training loss: 2.240082  [22400/60000]\n",
      "Training loss: 2.237099  [22464/60000]\n",
      "Training loss: 2.248444  [22528/60000]\n",
      "Training loss: 2.242161  [22592/60000]\n",
      "Training loss: 2.235488  [22656/60000]\n",
      "Training loss: 2.231263  [22720/60000]\n",
      "Training loss: 2.244849  [22784/60000]\n",
      "Training loss: 2.238753  [22848/60000]\n",
      "Training loss: 2.245273  [22912/60000]\n",
      "Training loss: 2.243427  [22976/60000]\n",
      "Training loss: 2.235821  [23040/60000]\n",
      "Training loss: 2.245223  [23104/60000]\n",
      "Training loss: 2.242954  [23168/60000]\n",
      "Training loss: 2.238921  [23232/60000]\n",
      "Training loss: 2.239419  [23296/60000]\n",
      "Training loss: 2.233557  [23360/60000]\n",
      "Training loss: 2.250065  [23424/60000]\n",
      "Training loss: 2.247329  [23488/60000]\n",
      "Training loss: 2.243546  [23552/60000]\n",
      "Training loss: 2.241033  [23616/60000]\n",
      "Training loss: 2.235102  [23680/60000]\n",
      "Training loss: 2.246364  [23744/60000]\n",
      "Training loss: 2.232934  [23808/60000]\n",
      "Training loss: 2.219912  [23872/60000]\n",
      "Training loss: 2.233170  [23936/60000]\n",
      "Training loss: 2.247486  [24000/60000]\n",
      "Training loss: 2.235682  [24064/60000]\n",
      "Training loss: 2.241668  [24128/60000]\n",
      "Training loss: 2.234925  [24192/60000]\n",
      "Training loss: 2.238708  [24256/60000]\n",
      "Training loss: 2.247614  [24320/60000]\n",
      "Training loss: 2.223511  [24384/60000]\n",
      "Training loss: 2.238521  [24448/60000]\n",
      "Training loss: 2.231832  [24512/60000]\n",
      "Training loss: 2.240537  [24576/60000]\n",
      "Training loss: 2.229084  [24640/60000]\n",
      "Training loss: 2.241136  [24704/60000]\n",
      "Training loss: 2.228438  [24768/60000]\n",
      "Training loss: 2.241921  [24832/60000]\n",
      "Training loss: 2.242511  [24896/60000]\n",
      "Training loss: 2.239855  [24960/60000]\n",
      "Training loss: 2.231246  [25024/60000]\n",
      "Training loss: 2.234951  [25088/60000]\n",
      "Training loss: 2.242516  [25152/60000]\n",
      "Training loss: 2.248007  [25216/60000]\n",
      "Training loss: 2.244046  [25280/60000]\n",
      "Training loss: 2.245169  [25344/60000]\n",
      "Training loss: 2.246433  [25408/60000]\n",
      "Training loss: 2.238107  [25472/60000]\n",
      "Training loss: 2.235996  [25536/60000]\n",
      "Training loss: 2.244995  [25600/60000]\n",
      "Training loss: 2.231499  [25664/60000]\n",
      "Training loss: 2.237422  [25728/60000]\n",
      "Training loss: 2.233262  [25792/60000]\n",
      "Training loss: 2.236797  [25856/60000]\n",
      "Training loss: 2.245635  [25920/60000]\n",
      "Training loss: 2.230279  [25984/60000]\n",
      "Training loss: 2.245231  [26048/60000]\n",
      "Training loss: 2.243705  [26112/60000]\n",
      "Training loss: 2.242102  [26176/60000]\n",
      "Training loss: 2.245855  [26240/60000]\n",
      "Training loss: 2.239913  [26304/60000]\n",
      "Training loss: 2.229141  [26368/60000]\n",
      "Training loss: 2.235669  [26432/60000]\n",
      "Training loss: 2.234087  [26496/60000]\n",
      "Training loss: 2.242584  [26560/60000]\n",
      "Training loss: 2.222358  [26624/60000]\n",
      "Training loss: 2.232553  [26688/60000]\n",
      "Training loss: 2.245437  [26752/60000]\n",
      "Training loss: 2.232506  [26816/60000]\n",
      "Training loss: 2.229963  [26880/60000]\n",
      "Training loss: 2.236518  [26944/60000]\n",
      "Training loss: 2.241240  [27008/60000]\n",
      "Training loss: 2.230497  [27072/60000]\n",
      "Training loss: 2.243548  [27136/60000]\n",
      "Training loss: 2.235287  [27200/60000]\n",
      "Training loss: 2.241202  [27264/60000]\n",
      "Training loss: 2.246236  [27328/60000]\n",
      "Training loss: 2.240871  [27392/60000]\n",
      "Training loss: 2.237786  [27456/60000]\n",
      "Training loss: 2.239561  [27520/60000]\n",
      "Training loss: 2.241099  [27584/60000]\n",
      "Training loss: 2.241085  [27648/60000]\n",
      "Training loss: 2.234194  [27712/60000]\n",
      "Training loss: 2.237269  [27776/60000]\n",
      "Training loss: 2.238737  [27840/60000]\n",
      "Training loss: 2.230310  [27904/60000]\n",
      "Training loss: 2.234926  [27968/60000]\n",
      "Training loss: 2.236378  [28032/60000]\n",
      "Training loss: 2.236705  [28096/60000]\n",
      "Training loss: 2.232733  [28160/60000]\n",
      "Training loss: 2.242076  [28224/60000]\n",
      "Training loss: 2.241376  [28288/60000]\n",
      "Training loss: 2.245505  [28352/60000]\n",
      "Training loss: 2.232706  [28416/60000]\n",
      "Training loss: 2.233311  [28480/60000]\n",
      "Training loss: 2.221228  [28544/60000]\n",
      "Training loss: 2.245165  [28608/60000]\n",
      "Training loss: 2.233482  [28672/60000]\n",
      "Training loss: 2.240888  [28736/60000]\n",
      "Training loss: 2.235125  [28800/60000]\n",
      "Training loss: 2.237080  [28864/60000]\n",
      "Training loss: 2.226186  [28928/60000]\n",
      "Training loss: 2.235193  [28992/60000]\n",
      "Training loss: 2.229818  [29056/60000]\n",
      "Training loss: 2.240494  [29120/60000]\n",
      "Training loss: 2.240516  [29184/60000]\n",
      "Training loss: 2.228696  [29248/60000]\n",
      "Training loss: 2.244349  [29312/60000]\n",
      "Training loss: 2.233114  [29376/60000]\n",
      "Training loss: 2.219203  [29440/60000]\n",
      "Training loss: 2.227040  [29504/60000]\n",
      "Training loss: 2.242038  [29568/60000]\n",
      "Training loss: 2.235026  [29632/60000]\n",
      "Training loss: 2.235102  [29696/60000]\n",
      "Training loss: 2.232193  [29760/60000]\n",
      "Training loss: 2.235055  [29824/60000]\n",
      "Training loss: 2.238471  [29888/60000]\n",
      "Training loss: 2.230800  [29952/60000]\n",
      "Training loss: 2.228599  [30016/60000]\n",
      "Training loss: 2.233408  [30080/60000]\n",
      "Training loss: 2.225394  [30144/60000]\n",
      "Training loss: 2.225250  [30208/60000]\n",
      "Training loss: 2.232253  [30272/60000]\n",
      "Training loss: 2.227648  [30336/60000]\n",
      "Training loss: 2.222103  [30400/60000]\n",
      "Training loss: 2.241399  [30464/60000]\n",
      "Training loss: 2.232333  [30528/60000]\n",
      "Training loss: 2.226218  [30592/60000]\n",
      "Training loss: 2.227055  [30656/60000]\n",
      "Training loss: 2.232279  [30720/60000]\n",
      "Training loss: 2.241584  [30784/60000]\n",
      "Training loss: 2.232091  [30848/60000]\n",
      "Training loss: 2.229638  [30912/60000]\n",
      "Training loss: 2.226728  [30976/60000]\n",
      "Training loss: 2.235234  [31040/60000]\n",
      "Training loss: 2.221464  [31104/60000]\n",
      "Training loss: 2.238002  [31168/60000]\n",
      "Training loss: 2.237083  [31232/60000]\n",
      "Training loss: 2.242308  [31296/60000]\n",
      "Training loss: 2.235157  [31360/60000]\n",
      "Training loss: 2.238589  [31424/60000]\n",
      "Training loss: 2.221182  [31488/60000]\n",
      "Training loss: 2.228538  [31552/60000]\n",
      "Training loss: 2.214958  [31616/60000]\n",
      "Training loss: 2.219388  [31680/60000]\n",
      "Training loss: 2.223311  [31744/60000]\n",
      "Training loss: 2.238033  [31808/60000]\n",
      "Training loss: 2.231141  [31872/60000]\n",
      "Training loss: 2.236403  [31936/60000]\n",
      "Training loss: 2.229789  [32000/60000]\n",
      "Training loss: 2.221813  [32064/60000]\n",
      "Training loss: 2.220895  [32128/60000]\n",
      "Training loss: 2.230450  [32192/60000]\n",
      "Training loss: 2.238224  [32256/60000]\n",
      "Training loss: 2.230671  [32320/60000]\n",
      "Training loss: 2.232147  [32384/60000]\n",
      "Training loss: 2.224276  [32448/60000]\n",
      "Training loss: 2.230498  [32512/60000]\n",
      "Training loss: 2.228427  [32576/60000]\n",
      "Training loss: 2.216829  [32640/60000]\n",
      "Training loss: 2.220708  [32704/60000]\n",
      "Training loss: 2.234356  [32768/60000]\n",
      "Training loss: 2.223177  [32832/60000]\n",
      "Training loss: 2.227429  [32896/60000]\n",
      "Training loss: 2.222959  [32960/60000]\n",
      "Training loss: 2.223737  [33024/60000]\n",
      "Training loss: 2.234355  [33088/60000]\n",
      "Training loss: 2.234287  [33152/60000]\n",
      "Training loss: 2.239837  [33216/60000]\n",
      "Training loss: 2.232268  [33280/60000]\n",
      "Training loss: 2.208925  [33344/60000]\n",
      "Training loss: 2.220550  [33408/60000]\n",
      "Training loss: 2.227939  [33472/60000]\n",
      "Training loss: 2.232150  [33536/60000]\n",
      "Training loss: 2.231805  [33600/60000]\n",
      "Training loss: 2.234698  [33664/60000]\n",
      "Training loss: 2.228471  [33728/60000]\n",
      "Training loss: 2.240365  [33792/60000]\n",
      "Training loss: 2.229749  [33856/60000]\n",
      "Training loss: 2.220801  [33920/60000]\n",
      "Training loss: 2.233148  [33984/60000]\n",
      "Training loss: 2.226453  [34048/60000]\n",
      "Training loss: 2.236820  [34112/60000]\n",
      "Training loss: 2.223373  [34176/60000]\n",
      "Training loss: 2.229605  [34240/60000]\n",
      "Training loss: 2.223733  [34304/60000]\n",
      "Training loss: 2.238608  [34368/60000]\n",
      "Training loss: 2.220657  [34432/60000]\n",
      "Training loss: 2.239616  [34496/60000]\n",
      "Training loss: 2.232161  [34560/60000]\n",
      "Training loss: 2.229805  [34624/60000]\n",
      "Training loss: 2.225327  [34688/60000]\n",
      "Training loss: 2.238515  [34752/60000]\n",
      "Training loss: 2.230447  [34816/60000]\n",
      "Training loss: 2.234640  [34880/60000]\n",
      "Training loss: 2.215476  [34944/60000]\n",
      "Training loss: 2.239559  [35008/60000]\n",
      "Training loss: 2.228409  [35072/60000]\n",
      "Training loss: 2.234004  [35136/60000]\n",
      "Training loss: 2.236027  [35200/60000]\n",
      "Training loss: 2.231630  [35264/60000]\n",
      "Training loss: 2.229489  [35328/60000]\n",
      "Training loss: 2.238515  [35392/60000]\n",
      "Training loss: 2.218225  [35456/60000]\n",
      "Training loss: 2.223454  [35520/60000]\n",
      "Training loss: 2.218507  [35584/60000]\n",
      "Training loss: 2.211281  [35648/60000]\n",
      "Training loss: 2.232134  [35712/60000]\n",
      "Training loss: 2.217849  [35776/60000]\n",
      "Training loss: 2.222401  [35840/60000]\n",
      "Training loss: 2.211029  [35904/60000]\n",
      "Training loss: 2.240151  [35968/60000]\n",
      "Training loss: 2.224230  [36032/60000]\n",
      "Training loss: 2.220040  [36096/60000]\n",
      "Training loss: 2.219452  [36160/60000]\n",
      "Training loss: 2.232330  [36224/60000]\n",
      "Training loss: 2.231330  [36288/60000]\n",
      "Training loss: 2.218467  [36352/60000]\n",
      "Training loss: 2.228684  [36416/60000]\n",
      "Training loss: 2.220009  [36480/60000]\n",
      "Training loss: 2.225876  [36544/60000]\n",
      "Training loss: 2.221229  [36608/60000]\n",
      "Training loss: 2.233676  [36672/60000]\n",
      "Training loss: 2.231383  [36736/60000]\n",
      "Training loss: 2.224034  [36800/60000]\n",
      "Training loss: 2.232527  [36864/60000]\n",
      "Training loss: 2.221366  [36928/60000]\n",
      "Training loss: 2.229470  [36992/60000]\n",
      "Training loss: 2.233486  [37056/60000]\n",
      "Training loss: 2.235139  [37120/60000]\n",
      "Training loss: 2.223758  [37184/60000]\n",
      "Training loss: 2.212068  [37248/60000]\n",
      "Training loss: 2.223134  [37312/60000]\n",
      "Training loss: 2.212036  [37376/60000]\n",
      "Training loss: 2.229363  [37440/60000]\n",
      "Training loss: 2.221613  [37504/60000]\n",
      "Training loss: 2.224443  [37568/60000]\n",
      "Training loss: 2.218618  [37632/60000]\n",
      "Training loss: 2.232273  [37696/60000]\n",
      "Training loss: 2.211883  [37760/60000]\n",
      "Training loss: 2.228254  [37824/60000]\n",
      "Training loss: 2.232652  [37888/60000]\n",
      "Training loss: 2.213537  [37952/60000]\n",
      "Training loss: 2.221081  [38016/60000]\n",
      "Training loss: 2.235577  [38080/60000]\n",
      "Training loss: 2.223450  [38144/60000]\n",
      "Training loss: 2.227993  [38208/60000]\n",
      "Training loss: 2.219972  [38272/60000]\n",
      "Training loss: 2.227231  [38336/60000]\n",
      "Training loss: 2.222730  [38400/60000]\n",
      "Training loss: 2.231658  [38464/60000]\n",
      "Training loss: 2.230270  [38528/60000]\n",
      "Training loss: 2.235738  [38592/60000]\n",
      "Training loss: 2.214586  [38656/60000]\n",
      "Training loss: 2.241205  [38720/60000]\n",
      "Training loss: 2.235087  [38784/60000]\n",
      "Training loss: 2.221906  [38848/60000]\n",
      "Training loss: 2.221703  [38912/60000]\n",
      "Training loss: 2.222492  [38976/60000]\n",
      "Training loss: 2.231194  [39040/60000]\n",
      "Training loss: 2.218529  [39104/60000]\n",
      "Training loss: 2.221801  [39168/60000]\n",
      "Training loss: 2.230561  [39232/60000]\n",
      "Training loss: 2.225052  [39296/60000]\n",
      "Training loss: 2.224107  [39360/60000]\n",
      "Training loss: 2.214934  [39424/60000]\n",
      "Training loss: 2.239212  [39488/60000]\n",
      "Training loss: 2.224744  [39552/60000]\n",
      "Training loss: 2.232082  [39616/60000]\n",
      "Training loss: 2.237113  [39680/60000]\n",
      "Training loss: 2.222613  [39744/60000]\n",
      "Training loss: 2.225737  [39808/60000]\n",
      "Training loss: 2.216496  [39872/60000]\n",
      "Training loss: 2.213002  [39936/60000]\n",
      "Training loss: 2.217566  [40000/60000]\n",
      "Training loss: 2.221230  [40064/60000]\n",
      "Training loss: 2.210490  [40128/60000]\n",
      "Training loss: 2.205341  [40192/60000]\n",
      "Training loss: 2.222390  [40256/60000]\n",
      "Training loss: 2.218783  [40320/60000]\n",
      "Training loss: 2.242836  [40384/60000]\n",
      "Training loss: 2.216332  [40448/60000]\n",
      "Training loss: 2.217296  [40512/60000]\n",
      "Training loss: 2.222144  [40576/60000]\n",
      "Training loss: 2.228406  [40640/60000]\n",
      "Training loss: 2.224651  [40704/60000]\n",
      "Training loss: 2.209143  [40768/60000]\n",
      "Training loss: 2.227121  [40832/60000]\n",
      "Training loss: 2.221069  [40896/60000]\n",
      "Training loss: 2.233817  [40960/60000]\n",
      "Training loss: 2.215188  [41024/60000]\n",
      "Training loss: 2.231309  [41088/60000]\n",
      "Training loss: 2.222296  [41152/60000]\n",
      "Training loss: 2.230653  [41216/60000]\n",
      "Training loss: 2.226770  [41280/60000]\n",
      "Training loss: 2.217183  [41344/60000]\n",
      "Training loss: 2.223601  [41408/60000]\n",
      "Training loss: 2.217202  [41472/60000]\n",
      "Training loss: 2.224034  [41536/60000]\n",
      "Training loss: 2.217519  [41600/60000]\n",
      "Training loss: 2.207860  [41664/60000]\n",
      "Training loss: 2.227065  [41728/60000]\n",
      "Training loss: 2.225095  [41792/60000]\n",
      "Training loss: 2.225338  [41856/60000]\n",
      "Training loss: 2.226715  [41920/60000]\n",
      "Training loss: 2.216101  [41984/60000]\n",
      "Training loss: 2.227958  [42048/60000]\n",
      "Training loss: 2.213864  [42112/60000]\n",
      "Training loss: 2.218907  [42176/60000]\n",
      "Training loss: 2.225189  [42240/60000]\n",
      "Training loss: 2.208200  [42304/60000]\n",
      "Training loss: 2.223310  [42368/60000]\n",
      "Training loss: 2.224823  [42432/60000]\n",
      "Training loss: 2.216466  [42496/60000]\n",
      "Training loss: 2.208945  [42560/60000]\n",
      "Training loss: 2.218446  [42624/60000]\n",
      "Training loss: 2.222750  [42688/60000]\n",
      "Training loss: 2.225202  [42752/60000]\n",
      "Training loss: 2.213450  [42816/60000]\n",
      "Training loss: 2.214620  [42880/60000]\n",
      "Training loss: 2.225361  [42944/60000]\n",
      "Training loss: 2.209635  [43008/60000]\n",
      "Training loss: 2.226525  [43072/60000]\n",
      "Training loss: 2.224794  [43136/60000]\n",
      "Training loss: 2.214988  [43200/60000]\n",
      "Training loss: 2.208421  [43264/60000]\n",
      "Training loss: 2.219662  [43328/60000]\n",
      "Training loss: 2.220900  [43392/60000]\n",
      "Training loss: 2.213094  [43456/60000]\n",
      "Training loss: 2.211405  [43520/60000]\n",
      "Training loss: 2.229746  [43584/60000]\n",
      "Training loss: 2.209025  [43648/60000]\n",
      "Training loss: 2.221440  [43712/60000]\n",
      "Training loss: 2.217825  [43776/60000]\n",
      "Training loss: 2.220647  [43840/60000]\n",
      "Training loss: 2.226875  [43904/60000]\n",
      "Training loss: 2.228453  [43968/60000]\n",
      "Training loss: 2.220720  [44032/60000]\n",
      "Training loss: 2.217555  [44096/60000]\n",
      "Training loss: 2.214138  [44160/60000]\n",
      "Training loss: 2.231918  [44224/60000]\n",
      "Training loss: 2.207146  [44288/60000]\n",
      "Training loss: 2.198758  [44352/60000]\n",
      "Training loss: 2.221273  [44416/60000]\n",
      "Training loss: 2.223689  [44480/60000]\n",
      "Training loss: 2.213252  [44544/60000]\n",
      "Training loss: 2.197850  [44608/60000]\n",
      "Training loss: 2.216423  [44672/60000]\n",
      "Training loss: 2.219900  [44736/60000]\n",
      "Training loss: 2.213670  [44800/60000]\n",
      "Training loss: 2.226601  [44864/60000]\n",
      "Training loss: 2.218031  [44928/60000]\n",
      "Training loss: 2.214304  [44992/60000]\n",
      "Training loss: 2.224562  [45056/60000]\n",
      "Training loss: 2.215232  [45120/60000]\n",
      "Training loss: 2.214408  [45184/60000]\n",
      "Training loss: 2.207659  [45248/60000]\n",
      "Training loss: 2.222724  [45312/60000]\n",
      "Training loss: 2.218357  [45376/60000]\n",
      "Training loss: 2.208013  [45440/60000]\n",
      "Training loss: 2.215789  [45504/60000]\n",
      "Training loss: 2.215552  [45568/60000]\n",
      "Training loss: 2.220802  [45632/60000]\n",
      "Training loss: 2.214932  [45696/60000]\n",
      "Training loss: 2.212224  [45760/60000]\n",
      "Training loss: 2.220897  [45824/60000]\n",
      "Training loss: 2.222485  [45888/60000]\n",
      "Training loss: 2.220055  [45952/60000]\n",
      "Training loss: 2.220575  [46016/60000]\n",
      "Training loss: 2.210887  [46080/60000]\n",
      "Training loss: 2.212321  [46144/60000]\n",
      "Training loss: 2.215680  [46208/60000]\n",
      "Training loss: 2.213737  [46272/60000]\n",
      "Training loss: 2.216707  [46336/60000]\n",
      "Training loss: 2.203119  [46400/60000]\n",
      "Training loss: 2.222645  [46464/60000]\n",
      "Training loss: 2.216464  [46528/60000]\n",
      "Training loss: 2.218804  [46592/60000]\n",
      "Training loss: 2.212716  [46656/60000]\n",
      "Training loss: 2.220357  [46720/60000]\n",
      "Training loss: 2.220818  [46784/60000]\n",
      "Training loss: 2.216316  [46848/60000]\n",
      "Training loss: 2.217228  [46912/60000]\n",
      "Training loss: 2.217311  [46976/60000]\n",
      "Training loss: 2.208286  [47040/60000]\n",
      "Training loss: 2.195540  [47104/60000]\n",
      "Training loss: 2.226189  [47168/60000]\n",
      "Training loss: 2.225175  [47232/60000]\n",
      "Training loss: 2.211745  [47296/60000]\n",
      "Training loss: 2.227132  [47360/60000]\n",
      "Training loss: 2.202604  [47424/60000]\n",
      "Training loss: 2.217904  [47488/60000]\n",
      "Training loss: 2.215726  [47552/60000]\n",
      "Training loss: 2.203771  [47616/60000]\n",
      "Training loss: 2.205618  [47680/60000]\n",
      "Training loss: 2.214736  [47744/60000]\n",
      "Training loss: 2.210299  [47808/60000]\n",
      "Training loss: 2.224352  [47872/60000]\n",
      "Training loss: 2.219904  [47936/60000]\n",
      "Training loss: 2.215584  [48000/60000]\n",
      "Training loss: 2.210604  [48064/60000]\n",
      "Training loss: 2.206944  [48128/60000]\n",
      "Training loss: 2.191973  [48192/60000]\n",
      "Training loss: 2.200701  [48256/60000]\n",
      "Training loss: 2.212514  [48320/60000]\n",
      "Training loss: 2.212944  [48384/60000]\n",
      "Training loss: 2.215912  [48448/60000]\n",
      "Training loss: 2.212275  [48512/60000]\n",
      "Training loss: 2.197575  [48576/60000]\n",
      "Training loss: 2.225301  [48640/60000]\n",
      "Training loss: 2.219499  [48704/60000]\n",
      "Training loss: 2.202371  [48768/60000]\n",
      "Training loss: 2.222555  [48832/60000]\n",
      "Training loss: 2.214781  [48896/60000]\n",
      "Training loss: 2.213829  [48960/60000]\n",
      "Training loss: 2.199818  [49024/60000]\n",
      "Training loss: 2.196574  [49088/60000]\n",
      "Training loss: 2.219906  [49152/60000]\n",
      "Training loss: 2.208187  [49216/60000]\n",
      "Training loss: 2.220351  [49280/60000]\n",
      "Training loss: 2.212829  [49344/60000]\n",
      "Training loss: 2.214224  [49408/60000]\n",
      "Training loss: 2.211723  [49472/60000]\n",
      "Training loss: 2.194579  [49536/60000]\n",
      "Training loss: 2.220944  [49600/60000]\n",
      "Training loss: 2.205793  [49664/60000]\n",
      "Training loss: 2.214098  [49728/60000]\n",
      "Training loss: 2.208122  [49792/60000]\n",
      "Training loss: 2.220041  [49856/60000]\n",
      "Training loss: 2.235744  [49920/60000]\n",
      "Training loss: 2.200966  [49984/60000]\n",
      "Training loss: 2.220398  [50048/60000]\n",
      "Training loss: 2.222557  [50112/60000]\n",
      "Training loss: 2.197700  [50176/60000]\n",
      "Training loss: 2.214266  [50240/60000]\n",
      "Training loss: 2.204730  [50304/60000]\n",
      "Training loss: 2.215348  [50368/60000]\n",
      "Training loss: 2.215288  [50432/60000]\n",
      "Training loss: 2.205615  [50496/60000]\n",
      "Training loss: 2.214313  [50560/60000]\n",
      "Training loss: 2.201895  [50624/60000]\n",
      "Training loss: 2.220948  [50688/60000]\n",
      "Training loss: 2.215026  [50752/60000]\n",
      "Training loss: 2.213514  [50816/60000]\n",
      "Training loss: 2.213039  [50880/60000]\n",
      "Training loss: 2.207941  [50944/60000]\n",
      "Training loss: 2.230484  [51008/60000]\n",
      "Training loss: 2.194407  [51072/60000]\n",
      "Training loss: 2.200578  [51136/60000]\n",
      "Training loss: 2.217685  [51200/60000]\n",
      "Training loss: 2.203506  [51264/60000]\n",
      "Training loss: 2.207159  [51328/60000]\n",
      "Training loss: 2.222846  [51392/60000]\n",
      "Training loss: 2.217441  [51456/60000]\n",
      "Training loss: 2.206733  [51520/60000]\n",
      "Training loss: 2.203359  [51584/60000]\n",
      "Training loss: 2.213111  [51648/60000]\n",
      "Training loss: 2.198610  [51712/60000]\n",
      "Training loss: 2.208825  [51776/60000]\n",
      "Training loss: 2.203746  [51840/60000]\n",
      "Training loss: 2.205685  [51904/60000]\n",
      "Training loss: 2.205885  [51968/60000]\n",
      "Training loss: 2.218246  [52032/60000]\n",
      "Training loss: 2.204784  [52096/60000]\n",
      "Training loss: 2.196123  [52160/60000]\n",
      "Training loss: 2.204833  [52224/60000]\n",
      "Training loss: 2.212928  [52288/60000]\n",
      "Training loss: 2.221273  [52352/60000]\n",
      "Training loss: 2.209532  [52416/60000]\n",
      "Training loss: 2.211976  [52480/60000]\n",
      "Training loss: 2.185067  [52544/60000]\n",
      "Training loss: 2.204632  [52608/60000]\n",
      "Training loss: 2.214599  [52672/60000]\n",
      "Training loss: 2.204750  [52736/60000]\n",
      "Training loss: 2.197471  [52800/60000]\n",
      "Training loss: 2.189197  [52864/60000]\n",
      "Training loss: 2.204539  [52928/60000]\n",
      "Training loss: 2.204184  [52992/60000]\n",
      "Training loss: 2.213599  [53056/60000]\n",
      "Training loss: 2.193199  [53120/60000]\n",
      "Training loss: 2.203474  [53184/60000]\n",
      "Training loss: 2.207032  [53248/60000]\n",
      "Training loss: 2.213440  [53312/60000]\n",
      "Training loss: 2.208077  [53376/60000]\n",
      "Training loss: 2.197773  [53440/60000]\n",
      "Training loss: 2.213291  [53504/60000]\n",
      "Training loss: 2.217981  [53568/60000]\n",
      "Training loss: 2.209936  [53632/60000]\n",
      "Training loss: 2.196489  [53696/60000]\n",
      "Training loss: 2.203841  [53760/60000]\n",
      "Training loss: 2.214686  [53824/60000]\n",
      "Training loss: 2.191937  [53888/60000]\n",
      "Training loss: 2.190149  [53952/60000]\n",
      "Training loss: 2.201016  [54016/60000]\n",
      "Training loss: 2.215505  [54080/60000]\n",
      "Training loss: 2.207006  [54144/60000]\n",
      "Training loss: 2.202818  [54208/60000]\n",
      "Training loss: 2.201179  [54272/60000]\n",
      "Training loss: 2.206253  [54336/60000]\n",
      "Training loss: 2.196685  [54400/60000]\n",
      "Training loss: 2.194498  [54464/60000]\n",
      "Training loss: 2.197720  [54528/60000]\n",
      "Training loss: 2.226040  [54592/60000]\n",
      "Training loss: 2.197364  [54656/60000]\n",
      "Training loss: 2.197167  [54720/60000]\n",
      "Training loss: 2.200281  [54784/60000]\n",
      "Training loss: 2.201516  [54848/60000]\n",
      "Training loss: 2.212170  [54912/60000]\n",
      "Training loss: 2.193269  [54976/60000]\n",
      "Training loss: 2.207599  [55040/60000]\n",
      "Training loss: 2.214798  [55104/60000]\n",
      "Training loss: 2.206425  [55168/60000]\n",
      "Training loss: 2.213291  [55232/60000]\n",
      "Training loss: 2.200668  [55296/60000]\n",
      "Training loss: 2.184423  [55360/60000]\n",
      "Training loss: 2.211933  [55424/60000]\n",
      "Training loss: 2.215397  [55488/60000]\n",
      "Training loss: 2.207192  [55552/60000]\n",
      "Training loss: 2.182772  [55616/60000]\n",
      "Training loss: 2.208686  [55680/60000]\n",
      "Training loss: 2.207030  [55744/60000]\n",
      "Training loss: 2.205726  [55808/60000]\n",
      "Training loss: 2.206395  [55872/60000]\n",
      "Training loss: 2.201818  [55936/60000]\n",
      "Training loss: 2.208897  [56000/60000]\n",
      "Training loss: 2.198954  [56064/60000]\n",
      "Training loss: 2.188019  [56128/60000]\n",
      "Training loss: 2.189618  [56192/60000]\n",
      "Training loss: 2.195851  [56256/60000]\n",
      "Training loss: 2.210042  [56320/60000]\n",
      "Training loss: 2.193373  [56384/60000]\n",
      "Training loss: 2.206867  [56448/60000]\n",
      "Training loss: 2.221186  [56512/60000]\n",
      "Training loss: 2.202777  [56576/60000]\n",
      "Training loss: 2.185184  [56640/60000]\n",
      "Training loss: 2.209428  [56704/60000]\n",
      "Training loss: 2.198496  [56768/60000]\n",
      "Training loss: 2.196842  [56832/60000]\n",
      "Training loss: 2.179391  [56896/60000]\n",
      "Training loss: 2.200602  [56960/60000]\n",
      "Training loss: 2.192973  [57024/60000]\n",
      "Training loss: 2.212361  [57088/60000]\n",
      "Training loss: 2.209411  [57152/60000]\n",
      "Training loss: 2.202880  [57216/60000]\n",
      "Training loss: 2.199278  [57280/60000]\n",
      "Training loss: 2.202766  [57344/60000]\n",
      "Training loss: 2.198842  [57408/60000]\n",
      "Training loss: 2.193693  [57472/60000]\n",
      "Training loss: 2.201415  [57536/60000]\n",
      "Training loss: 2.207373  [57600/60000]\n",
      "Training loss: 2.200018  [57664/60000]\n",
      "Training loss: 2.201355  [57728/60000]\n",
      "Training loss: 2.185049  [57792/60000]\n",
      "Training loss: 2.200813  [57856/60000]\n",
      "Training loss: 2.204297  [57920/60000]\n",
      "Training loss: 2.210639  [57984/60000]\n",
      "Training loss: 2.197645  [58048/60000]\n",
      "Training loss: 2.199347  [58112/60000]\n",
      "Training loss: 2.198929  [58176/60000]\n",
      "Training loss: 2.193967  [58240/60000]\n",
      "Training loss: 2.208729  [58304/60000]\n",
      "Training loss: 2.189526  [58368/60000]\n",
      "Training loss: 2.198113  [58432/60000]\n",
      "Training loss: 2.195554  [58496/60000]\n",
      "Training loss: 2.201737  [58560/60000]\n",
      "Training loss: 2.193569  [58624/60000]\n",
      "Training loss: 2.198511  [58688/60000]\n",
      "Training loss: 2.196869  [58752/60000]\n",
      "Training loss: 2.199003  [58816/60000]\n",
      "Training loss: 2.215196  [58880/60000]\n",
      "Training loss: 2.190785  [58944/60000]\n",
      "Training loss: 2.201068  [59008/60000]\n",
      "Training loss: 2.194432  [59072/60000]\n",
      "Training loss: 2.179517  [59136/60000]\n",
      "Training loss: 2.207845  [59200/60000]\n",
      "Training loss: 2.201390  [59264/60000]\n",
      "Training loss: 2.183996  [59328/60000]\n",
      "Training loss: 2.196073  [59392/60000]\n",
      "Training loss: 2.219573  [59456/60000]\n",
      "Training loss: 2.206494  [59520/60000]\n",
      "Training loss: 2.198651  [59584/60000]\n",
      "Training loss: 2.206184  [59648/60000]\n",
      "Training loss: 2.205986  [59712/60000]\n",
      "Training loss: 2.208263  [59776/60000]\n",
      "Training loss: 2.188176  [59840/60000]\n",
      "Training loss: 2.206272  [59904/60000]\n",
      "Training loss: 2.196836  [29984/60000]\n",
      "\n",
      " Finished Epoch:  1\n",
      "--------------------------------------------\n",
      "Training loss: 2.210598  [   64/60000]\n",
      "Training loss: 2.182075  [  128/60000]\n",
      "Training loss: 2.190586  [  192/60000]\n",
      "Training loss: 2.185741  [  256/60000]\n",
      "Training loss: 2.197867  [  320/60000]\n",
      "Training loss: 2.209690  [  384/60000]\n",
      "Training loss: 2.201519  [  448/60000]\n",
      "Training loss: 2.200717  [  512/60000]\n",
      "Training loss: 2.217719  [  576/60000]\n",
      "Training loss: 2.207288  [  640/60000]\n",
      "Training loss: 2.195356  [  704/60000]\n",
      "Training loss: 2.212364  [  768/60000]\n",
      "Training loss: 2.207102  [  832/60000]\n",
      "Training loss: 2.192558  [  896/60000]\n",
      "Training loss: 2.192117  [  960/60000]\n",
      "Training loss: 2.186065  [ 1024/60000]\n",
      "Training loss: 2.181224  [ 1088/60000]\n",
      "Training loss: 2.195176  [ 1152/60000]\n",
      "Training loss: 2.204257  [ 1216/60000]\n",
      "Training loss: 2.182976  [ 1280/60000]\n",
      "Training loss: 2.214927  [ 1344/60000]\n",
      "Training loss: 2.202762  [ 1408/60000]\n",
      "Training loss: 2.209838  [ 1472/60000]\n",
      "Training loss: 2.215032  [ 1536/60000]\n",
      "Training loss: 2.199358  [ 1600/60000]\n",
      "Training loss: 2.195327  [ 1664/60000]\n",
      "Training loss: 2.191254  [ 1728/60000]\n",
      "Training loss: 2.206976  [ 1792/60000]\n",
      "Training loss: 2.201574  [ 1856/60000]\n",
      "Training loss: 2.190583  [ 1920/60000]\n",
      "Training loss: 2.180719  [ 1984/60000]\n",
      "Training loss: 2.206353  [ 2048/60000]\n",
      "Training loss: 2.199469  [ 2112/60000]\n",
      "Training loss: 2.210974  [ 2176/60000]\n",
      "Training loss: 2.182540  [ 2240/60000]\n",
      "Training loss: 2.194784  [ 2304/60000]\n",
      "Training loss: 2.179178  [ 2368/60000]\n",
      "Training loss: 2.202607  [ 2432/60000]\n",
      "Training loss: 2.201572  [ 2496/60000]\n",
      "Training loss: 2.200684  [ 2560/60000]\n",
      "Training loss: 2.185439  [ 2624/60000]\n",
      "Training loss: 2.184663  [ 2688/60000]\n",
      "Training loss: 2.203234  [ 2752/60000]\n",
      "Training loss: 2.191271  [ 2816/60000]\n",
      "Training loss: 2.205447  [ 2880/60000]\n",
      "Training loss: 2.179646  [ 2944/60000]\n",
      "Training loss: 2.193003  [ 3008/60000]\n",
      "Training loss: 2.194684  [ 3072/60000]\n",
      "Training loss: 2.190995  [ 3136/60000]\n",
      "Training loss: 2.205116  [ 3200/60000]\n",
      "Training loss: 2.188637  [ 3264/60000]\n",
      "Training loss: 2.177927  [ 3328/60000]\n",
      "Training loss: 2.190998  [ 3392/60000]\n",
      "Training loss: 2.192090  [ 3456/60000]\n",
      "Training loss: 2.174882  [ 3520/60000]\n",
      "Training loss: 2.209107  [ 3584/60000]\n",
      "Training loss: 2.200526  [ 3648/60000]\n",
      "Training loss: 2.190019  [ 3712/60000]\n",
      "Training loss: 2.199379  [ 3776/60000]\n",
      "Training loss: 2.172979  [ 3840/60000]\n",
      "Training loss: 2.207624  [ 3904/60000]\n",
      "Training loss: 2.202907  [ 3968/60000]\n",
      "Training loss: 2.191148  [ 4032/60000]\n",
      "Training loss: 2.209323  [ 4096/60000]\n",
      "Training loss: 2.186888  [ 4160/60000]\n",
      "Training loss: 2.198942  [ 4224/60000]\n",
      "Training loss: 2.199155  [ 4288/60000]\n",
      "Training loss: 2.208208  [ 4352/60000]\n",
      "Training loss: 2.208093  [ 4416/60000]\n",
      "Training loss: 2.190189  [ 4480/60000]\n",
      "Training loss: 2.200071  [ 4544/60000]\n",
      "Training loss: 2.179018  [ 4608/60000]\n",
      "Training loss: 2.185602  [ 4672/60000]\n",
      "Training loss: 2.185710  [ 4736/60000]\n",
      "Training loss: 2.202643  [ 4800/60000]\n",
      "Training loss: 2.186202  [ 4864/60000]\n",
      "Training loss: 2.201473  [ 4928/60000]\n",
      "Training loss: 2.191706  [ 4992/60000]\n",
      "Training loss: 2.180855  [ 5056/60000]\n",
      "Training loss: 2.201971  [ 5120/60000]\n",
      "Training loss: 2.205997  [ 5184/60000]\n",
      "Training loss: 2.205613  [ 5248/60000]\n",
      "Training loss: 2.195893  [ 5312/60000]\n",
      "Training loss: 2.187812  [ 5376/60000]\n",
      "Training loss: 2.192559  [ 5440/60000]\n",
      "Training loss: 2.206786  [ 5504/60000]\n",
      "Training loss: 2.188337  [ 5568/60000]\n",
      "Training loss: 2.195040  [ 5632/60000]\n",
      "Training loss: 2.197897  [ 5696/60000]\n",
      "Training loss: 2.198249  [ 5760/60000]\n",
      "Training loss: 2.186813  [ 5824/60000]\n",
      "Training loss: 2.183676  [ 5888/60000]\n",
      "Training loss: 2.200083  [ 5952/60000]\n",
      "Training loss: 2.196821  [ 6016/60000]\n",
      "Training loss: 2.184980  [ 6080/60000]\n",
      "Training loss: 2.196607  [ 6144/60000]\n",
      "Training loss: 2.204061  [ 6208/60000]\n",
      "Training loss: 2.181450  [ 6272/60000]\n",
      "Training loss: 2.212826  [ 6336/60000]\n",
      "Training loss: 2.181493  [ 6400/60000]\n",
      "Training loss: 2.197109  [ 6464/60000]\n",
      "Training loss: 2.186246  [ 6528/60000]\n",
      "Training loss: 2.184069  [ 6592/60000]\n",
      "Training loss: 2.178804  [ 6656/60000]\n",
      "Training loss: 2.178267  [ 6720/60000]\n",
      "Training loss: 2.192122  [ 6784/60000]\n",
      "Training loss: 2.176776  [ 6848/60000]\n",
      "Training loss: 2.200267  [ 6912/60000]\n",
      "Training loss: 2.196906  [ 6976/60000]\n",
      "Training loss: 2.194836  [ 7040/60000]\n",
      "Training loss: 2.192157  [ 7104/60000]\n",
      "Training loss: 2.188188  [ 7168/60000]\n",
      "Training loss: 2.210732  [ 7232/60000]\n",
      "Training loss: 2.195431  [ 7296/60000]\n",
      "Training loss: 2.191874  [ 7360/60000]\n",
      "Training loss: 2.196655  [ 7424/60000]\n",
      "Training loss: 2.170433  [ 7488/60000]\n",
      "Training loss: 2.184699  [ 7552/60000]\n",
      "Training loss: 2.184173  [ 7616/60000]\n",
      "Training loss: 2.191732  [ 7680/60000]\n",
      "Training loss: 2.164598  [ 7744/60000]\n",
      "Training loss: 2.196096  [ 7808/60000]\n",
      "Training loss: 2.201437  [ 7872/60000]\n",
      "Training loss: 2.210244  [ 7936/60000]\n",
      "Training loss: 2.206370  [ 8000/60000]\n",
      "Training loss: 2.163366  [ 8064/60000]\n",
      "Training loss: 2.171309  [ 8128/60000]\n",
      "Training loss: 2.203661  [ 8192/60000]\n",
      "Training loss: 2.184676  [ 8256/60000]\n",
      "Training loss: 2.183530  [ 8320/60000]\n",
      "Training loss: 2.198619  [ 8384/60000]\n",
      "Training loss: 2.181187  [ 8448/60000]\n",
      "Training loss: 2.168632  [ 8512/60000]\n",
      "Training loss: 2.183710  [ 8576/60000]\n",
      "Training loss: 2.176027  [ 8640/60000]\n",
      "Training loss: 2.203096  [ 8704/60000]\n",
      "Training loss: 2.194389  [ 8768/60000]\n",
      "Training loss: 2.180453  [ 8832/60000]\n",
      "Training loss: 2.177398  [ 8896/60000]\n",
      "Training loss: 2.197119  [ 8960/60000]\n",
      "Training loss: 2.193924  [ 9024/60000]\n",
      "Training loss: 2.207263  [ 9088/60000]\n",
      "Training loss: 2.191269  [ 9152/60000]\n",
      "Training loss: 2.169759  [ 9216/60000]\n",
      "Training loss: 2.189193  [ 9280/60000]\n",
      "Training loss: 2.177575  [ 9344/60000]\n",
      "Training loss: 2.202062  [ 9408/60000]\n",
      "Training loss: 2.187726  [ 9472/60000]\n",
      "Training loss: 2.161411  [ 9536/60000]\n",
      "Training loss: 2.178685  [ 9600/60000]\n",
      "Training loss: 2.205411  [ 9664/60000]\n",
      "Training loss: 2.182577  [ 9728/60000]\n",
      "Training loss: 2.186021  [ 9792/60000]\n",
      "Training loss: 2.199057  [ 9856/60000]\n",
      "Training loss: 2.186330  [ 9920/60000]\n",
      "Training loss: 2.194893  [ 9984/60000]\n",
      "Training loss: 2.185615  [10048/60000]\n",
      "Training loss: 2.186320  [10112/60000]\n",
      "Training loss: 2.191112  [10176/60000]\n",
      "Training loss: 2.178070  [10240/60000]\n",
      "Training loss: 2.195822  [10304/60000]\n",
      "Training loss: 2.185492  [10368/60000]\n",
      "Training loss: 2.180571  [10432/60000]\n",
      "Training loss: 2.183288  [10496/60000]\n",
      "Training loss: 2.182054  [10560/60000]\n",
      "Training loss: 2.194290  [10624/60000]\n",
      "Training loss: 2.185560  [10688/60000]\n",
      "Training loss: 2.159192  [10752/60000]\n",
      "Training loss: 2.174774  [10816/60000]\n",
      "Training loss: 2.187796  [10880/60000]\n",
      "Training loss: 2.176488  [10944/60000]\n",
      "Training loss: 2.169817  [11008/60000]\n",
      "Training loss: 2.182216  [11072/60000]\n",
      "Training loss: 2.199977  [11136/60000]\n",
      "Training loss: 2.174290  [11200/60000]\n",
      "Training loss: 2.188831  [11264/60000]\n",
      "Training loss: 2.175472  [11328/60000]\n",
      "Training loss: 2.173627  [11392/60000]\n",
      "Training loss: 2.198195  [11456/60000]\n",
      "Training loss: 2.162891  [11520/60000]\n",
      "Training loss: 2.182292  [11584/60000]\n",
      "Training loss: 2.174932  [11648/60000]\n",
      "Training loss: 2.195841  [11712/60000]\n",
      "Training loss: 2.169646  [11776/60000]\n",
      "Training loss: 2.188053  [11840/60000]\n",
      "Training loss: 2.186220  [11904/60000]\n",
      "Training loss: 2.203060  [11968/60000]\n",
      "Training loss: 2.169399  [12032/60000]\n",
      "Training loss: 2.187598  [12096/60000]\n",
      "Training loss: 2.205809  [12160/60000]\n",
      "Training loss: 2.197627  [12224/60000]\n",
      "Training loss: 2.187387  [12288/60000]\n",
      "Training loss: 2.181085  [12352/60000]\n",
      "Training loss: 2.202140  [12416/60000]\n",
      "Training loss: 2.189716  [12480/60000]\n",
      "Training loss: 2.173401  [12544/60000]\n",
      "Training loss: 2.186027  [12608/60000]\n",
      "Training loss: 2.188654  [12672/60000]\n",
      "Training loss: 2.191380  [12736/60000]\n",
      "Training loss: 2.183131  [12800/60000]\n",
      "Training loss: 2.178890  [12864/60000]\n",
      "Training loss: 2.175457  [12928/60000]\n",
      "Training loss: 2.200807  [12992/60000]\n",
      "Training loss: 2.197967  [13056/60000]\n",
      "Training loss: 2.175133  [13120/60000]\n",
      "Training loss: 2.182126  [13184/60000]\n",
      "Training loss: 2.168643  [13248/60000]\n",
      "Training loss: 2.182483  [13312/60000]\n",
      "Training loss: 2.178282  [13376/60000]\n",
      "Training loss: 2.174929  [13440/60000]\n",
      "Training loss: 2.180112  [13504/60000]\n",
      "Training loss: 2.179498  [13568/60000]\n",
      "Training loss: 2.162190  [13632/60000]\n",
      "Training loss: 2.192863  [13696/60000]\n",
      "Training loss: 2.183132  [13760/60000]\n",
      "Training loss: 2.182657  [13824/60000]\n",
      "Training loss: 2.203794  [13888/60000]\n",
      "Training loss: 2.192112  [13952/60000]\n",
      "Training loss: 2.194394  [14016/60000]\n",
      "Training loss: 2.192418  [14080/60000]\n",
      "Training loss: 2.173343  [14144/60000]\n",
      "Training loss: 2.182108  [14208/60000]\n",
      "Training loss: 2.174810  [14272/60000]\n",
      "Training loss: 2.175099  [14336/60000]\n",
      "Training loss: 2.180053  [14400/60000]\n",
      "Training loss: 2.172955  [14464/60000]\n",
      "Training loss: 2.177475  [14528/60000]\n",
      "Training loss: 2.190897  [14592/60000]\n",
      "Training loss: 2.178175  [14656/60000]\n",
      "Training loss: 2.194779  [14720/60000]\n",
      "Training loss: 2.182105  [14784/60000]\n",
      "Training loss: 2.191397  [14848/60000]\n",
      "Training loss: 2.180270  [14912/60000]\n",
      "Training loss: 2.191609  [14976/60000]\n",
      "Training loss: 2.198255  [15040/60000]\n",
      "Training loss: 2.179451  [15104/60000]\n",
      "Training loss: 2.185612  [15168/60000]\n",
      "Training loss: 2.173123  [15232/60000]\n",
      "Training loss: 2.169564  [15296/60000]\n",
      "Training loss: 2.173854  [15360/60000]\n",
      "Training loss: 2.177616  [15424/60000]\n",
      "Training loss: 2.182943  [15488/60000]\n",
      "Training loss: 2.173537  [15552/60000]\n",
      "Training loss: 2.176405  [15616/60000]\n",
      "Training loss: 2.187112  [15680/60000]\n",
      "Training loss: 2.182925  [15744/60000]\n",
      "Training loss: 2.179441  [15808/60000]\n",
      "Training loss: 2.186639  [15872/60000]\n",
      "Training loss: 2.183598  [15936/60000]\n",
      "Training loss: 2.175557  [16000/60000]\n",
      "Training loss: 2.181154  [16064/60000]\n",
      "Training loss: 2.178344  [16128/60000]\n",
      "Training loss: 2.196662  [16192/60000]\n",
      "Training loss: 2.161568  [16256/60000]\n",
      "Training loss: 2.175706  [16320/60000]\n",
      "Training loss: 2.192093  [16384/60000]\n",
      "Training loss: 2.166302  [16448/60000]\n",
      "Training loss: 2.185883  [16512/60000]\n",
      "Training loss: 2.164411  [16576/60000]\n",
      "Training loss: 2.164847  [16640/60000]\n",
      "Training loss: 2.170376  [16704/60000]\n",
      "Training loss: 2.173090  [16768/60000]\n",
      "Training loss: 2.189912  [16832/60000]\n",
      "Training loss: 2.185709  [16896/60000]\n",
      "Training loss: 2.163591  [16960/60000]\n",
      "Training loss: 2.179220  [17024/60000]\n",
      "Training loss: 2.187829  [17088/60000]\n",
      "Training loss: 2.177713  [17152/60000]\n",
      "Training loss: 2.179642  [17216/60000]\n",
      "Training loss: 2.157200  [17280/60000]\n",
      "Training loss: 2.161853  [17344/60000]\n",
      "Training loss: 2.181777  [17408/60000]\n",
      "Training loss: 2.169659  [17472/60000]\n",
      "Training loss: 2.174789  [17536/60000]\n",
      "Training loss: 2.175220  [17600/60000]\n",
      "Training loss: 2.180782  [17664/60000]\n",
      "Training loss: 2.168374  [17728/60000]\n",
      "Training loss: 2.175725  [17792/60000]\n",
      "Training loss: 2.176134  [17856/60000]\n",
      "Training loss: 2.163202  [17920/60000]\n",
      "Training loss: 2.166631  [17984/60000]\n",
      "Training loss: 2.156619  [18048/60000]\n",
      "Training loss: 2.156197  [18112/60000]\n",
      "Training loss: 2.161499  [18176/60000]\n",
      "Training loss: 2.158437  [18240/60000]\n",
      "Training loss: 2.176475  [18304/60000]\n",
      "Training loss: 2.168090  [18368/60000]\n",
      "Training loss: 2.167492  [18432/60000]\n",
      "Training loss: 2.191492  [18496/60000]\n",
      "Training loss: 2.195717  [18560/60000]\n",
      "Training loss: 2.185628  [18624/60000]\n",
      "Training loss: 2.166143  [18688/60000]\n",
      "Training loss: 2.192822  [18752/60000]\n",
      "Training loss: 2.168150  [18816/60000]\n",
      "Training loss: 2.170201  [18880/60000]\n",
      "Training loss: 2.184442  [18944/60000]\n",
      "Training loss: 2.172333  [19008/60000]\n",
      "Training loss: 2.172458  [19072/60000]\n",
      "Training loss: 2.163253  [19136/60000]\n",
      "Training loss: 2.175211  [19200/60000]\n",
      "Training loss: 2.182655  [19264/60000]\n",
      "Training loss: 2.163475  [19328/60000]\n",
      "Training loss: 2.157662  [19392/60000]\n",
      "Training loss: 2.170068  [19456/60000]\n",
      "Training loss: 2.155836  [19520/60000]\n",
      "Training loss: 2.170674  [19584/60000]\n",
      "Training loss: 2.167069  [19648/60000]\n",
      "Training loss: 2.170228  [19712/60000]\n",
      "Training loss: 2.183027  [19776/60000]\n",
      "Training loss: 2.187346  [19840/60000]\n",
      "Training loss: 2.167795  [19904/60000]\n",
      "Training loss: 2.185147  [19968/60000]\n",
      "Training loss: 2.173802  [20032/60000]\n",
      "Training loss: 2.175831  [20096/60000]\n",
      "Training loss: 2.159443  [20160/60000]\n",
      "Training loss: 2.189847  [20224/60000]\n",
      "Training loss: 2.175153  [20288/60000]\n",
      "Training loss: 2.150209  [20352/60000]\n",
      "Training loss: 2.165231  [20416/60000]\n",
      "Training loss: 2.170417  [20480/60000]\n",
      "Training loss: 2.153594  [20544/60000]\n",
      "Training loss: 2.180569  [20608/60000]\n",
      "Training loss: 2.165466  [20672/60000]\n",
      "Training loss: 2.140976  [20736/60000]\n",
      "Training loss: 2.156945  [20800/60000]\n",
      "Training loss: 2.175812  [20864/60000]\n",
      "Training loss: 2.177843  [20928/60000]\n",
      "Training loss: 2.167678  [20992/60000]\n",
      "Training loss: 2.169689  [21056/60000]\n",
      "Training loss: 2.162554  [21120/60000]\n",
      "Training loss: 2.156989  [21184/60000]\n",
      "Training loss: 2.180185  [21248/60000]\n",
      "Training loss: 2.161845  [21312/60000]\n",
      "Training loss: 2.175614  [21376/60000]\n",
      "Training loss: 2.163900  [21440/60000]\n",
      "Training loss: 2.146691  [21504/60000]\n",
      "Training loss: 2.175464  [21568/60000]\n",
      "Training loss: 2.182850  [21632/60000]\n",
      "Training loss: 2.154147  [21696/60000]\n",
      "Training loss: 2.166750  [21760/60000]\n",
      "Training loss: 2.173229  [21824/60000]\n",
      "Training loss: 2.182948  [21888/60000]\n",
      "Training loss: 2.156212  [21952/60000]\n",
      "Training loss: 2.163579  [22016/60000]\n",
      "Training loss: 2.161341  [22080/60000]\n",
      "Training loss: 2.177947  [22144/60000]\n",
      "Training loss: 2.165073  [22208/60000]\n",
      "Training loss: 2.179842  [22272/60000]\n",
      "Training loss: 2.183699  [22336/60000]\n",
      "Training loss: 2.182936  [22400/60000]\n",
      "Training loss: 2.166257  [22464/60000]\n",
      "Training loss: 2.163813  [22528/60000]\n",
      "Training loss: 2.165000  [22592/60000]\n",
      "Training loss: 2.187361  [22656/60000]\n",
      "Training loss: 2.158495  [22720/60000]\n",
      "Training loss: 2.185160  [22784/60000]\n",
      "Training loss: 2.186714  [22848/60000]\n",
      "Training loss: 2.154802  [22912/60000]\n",
      "Training loss: 2.178860  [22976/60000]\n",
      "Training loss: 2.181157  [23040/60000]\n",
      "Training loss: 2.167099  [23104/60000]\n",
      "Training loss: 2.176964  [23168/60000]\n",
      "Training loss: 2.171215  [23232/60000]\n",
      "Training loss: 2.156079  [23296/60000]\n",
      "Training loss: 2.165174  [23360/60000]\n",
      "Training loss: 2.178351  [23424/60000]\n",
      "Training loss: 2.164441  [23488/60000]\n",
      "Training loss: 2.160020  [23552/60000]\n",
      "Training loss: 2.177445  [23616/60000]\n",
      "Training loss: 2.162625  [23680/60000]\n",
      "Training loss: 2.169790  [23744/60000]\n",
      "Training loss: 2.156568  [23808/60000]\n",
      "Training loss: 2.148397  [23872/60000]\n",
      "Training loss: 2.151423  [23936/60000]\n",
      "Training loss: 2.146083  [24000/60000]\n",
      "Training loss: 2.149468  [24064/60000]\n",
      "Training loss: 2.153493  [24128/60000]\n",
      "Training loss: 2.168599  [24192/60000]\n",
      "Training loss: 2.154254  [24256/60000]\n",
      "Training loss: 2.154438  [24320/60000]\n",
      "Training loss: 2.186904  [24384/60000]\n",
      "Training loss: 2.164104  [24448/60000]\n",
      "Training loss: 2.172212  [24512/60000]\n",
      "Training loss: 2.163970  [24576/60000]\n",
      "Training loss: 2.164226  [24640/60000]\n",
      "Training loss: 2.163705  [24704/60000]\n",
      "Training loss: 2.176402  [24768/60000]\n",
      "Training loss: 2.168763  [24832/60000]\n",
      "Training loss: 2.129664  [24896/60000]\n",
      "Training loss: 2.164958  [24960/60000]\n",
      "Training loss: 2.166850  [25024/60000]\n",
      "Training loss: 2.183082  [25088/60000]\n",
      "Training loss: 2.129361  [25152/60000]\n",
      "Training loss: 2.162104  [25216/60000]\n",
      "Training loss: 2.169266  [25280/60000]\n",
      "Training loss: 2.163131  [25344/60000]\n",
      "Training loss: 2.155401  [25408/60000]\n",
      "Training loss: 2.197049  [25472/60000]\n",
      "Training loss: 2.150940  [25536/60000]\n",
      "Training loss: 2.170461  [25600/60000]\n",
      "Training loss: 2.159602  [25664/60000]\n",
      "Training loss: 2.154470  [25728/60000]\n",
      "Training loss: 2.180496  [25792/60000]\n",
      "Training loss: 2.146778  [25856/60000]\n",
      "Training loss: 2.148233  [25920/60000]\n",
      "Training loss: 2.167125  [25984/60000]\n",
      "Training loss: 2.151592  [26048/60000]\n",
      "Training loss: 2.164909  [26112/60000]\n",
      "Training loss: 2.133581  [26176/60000]\n",
      "Training loss: 2.133358  [26240/60000]\n",
      "Training loss: 2.168412  [26304/60000]\n",
      "Training loss: 2.156961  [26368/60000]\n",
      "Training loss: 2.164748  [26432/60000]\n",
      "Training loss: 2.167990  [26496/60000]\n",
      "Training loss: 2.166337  [26560/60000]\n",
      "Training loss: 2.146961  [26624/60000]\n",
      "Training loss: 2.171621  [26688/60000]\n",
      "Training loss: 2.155681  [26752/60000]\n",
      "Training loss: 2.161095  [26816/60000]\n",
      "Training loss: 2.155868  [26880/60000]\n",
      "Training loss: 2.166771  [26944/60000]\n",
      "Training loss: 2.165962  [27008/60000]\n",
      "Training loss: 2.159624  [27072/60000]\n",
      "Training loss: 2.171261  [27136/60000]\n",
      "Training loss: 2.170033  [27200/60000]\n",
      "Training loss: 2.156489  [27264/60000]\n",
      "Training loss: 2.152439  [27328/60000]\n",
      "Training loss: 2.156779  [27392/60000]\n",
      "Training loss: 2.158509  [27456/60000]\n",
      "Training loss: 2.158430  [27520/60000]\n",
      "Training loss: 2.153518  [27584/60000]\n",
      "Training loss: 2.170250  [27648/60000]\n",
      "Training loss: 2.148743  [27712/60000]\n",
      "Training loss: 2.158313  [27776/60000]\n",
      "Training loss: 2.148349  [27840/60000]\n",
      "Training loss: 2.150571  [27904/60000]\n",
      "Training loss: 2.180371  [27968/60000]\n",
      "Training loss: 2.156105  [28032/60000]\n",
      "Training loss: 2.164389  [28096/60000]\n",
      "Training loss: 2.154054  [28160/60000]\n",
      "Training loss: 2.154413  [28224/60000]\n",
      "Training loss: 2.156463  [28288/60000]\n",
      "Training loss: 2.155750  [28352/60000]\n",
      "Training loss: 2.152587  [28416/60000]\n",
      "Training loss: 2.140418  [28480/60000]\n",
      "Training loss: 2.156809  [28544/60000]\n",
      "Training loss: 2.172674  [28608/60000]\n",
      "Training loss: 2.149261  [28672/60000]\n",
      "Training loss: 2.173088  [28736/60000]\n",
      "Training loss: 2.141357  [28800/60000]\n",
      "Training loss: 2.180900  [28864/60000]\n",
      "Training loss: 2.147403  [28928/60000]\n",
      "Training loss: 2.139257  [28992/60000]\n",
      "Training loss: 2.175441  [29056/60000]\n",
      "Training loss: 2.167316  [29120/60000]\n",
      "Training loss: 2.144733  [29184/60000]\n",
      "Training loss: 2.140317  [29248/60000]\n",
      "Training loss: 2.147257  [29312/60000]\n",
      "Training loss: 2.150139  [29376/60000]\n",
      "Training loss: 2.176316  [29440/60000]\n",
      "Training loss: 2.157466  [29504/60000]\n",
      "Training loss: 2.142591  [29568/60000]\n",
      "Training loss: 2.154565  [29632/60000]\n",
      "Training loss: 2.140314  [29696/60000]\n",
      "Training loss: 2.151255  [29760/60000]\n",
      "Training loss: 2.135161  [29824/60000]\n",
      "Training loss: 2.160868  [29888/60000]\n",
      "Training loss: 2.138829  [29952/60000]\n",
      "Training loss: 2.151670  [30016/60000]\n",
      "Training loss: 2.176292  [30080/60000]\n",
      "Training loss: 2.165094  [30144/60000]\n",
      "Training loss: 2.148204  [30208/60000]\n",
      "Training loss: 2.125732  [30272/60000]\n",
      "Training loss: 2.149580  [30336/60000]\n",
      "Training loss: 2.157838  [30400/60000]\n",
      "Training loss: 2.157598  [30464/60000]\n",
      "Training loss: 2.131346  [30528/60000]\n",
      "Training loss: 2.169884  [30592/60000]\n",
      "Training loss: 2.156164  [30656/60000]\n",
      "Training loss: 2.167080  [30720/60000]\n",
      "Training loss: 2.164849  [30784/60000]\n",
      "Training loss: 2.139585  [30848/60000]\n",
      "Training loss: 2.171067  [30912/60000]\n",
      "Training loss: 2.167717  [30976/60000]\n",
      "Training loss: 2.173459  [31040/60000]\n",
      "Training loss: 2.153790  [31104/60000]\n",
      "Training loss: 2.147468  [31168/60000]\n",
      "Training loss: 2.148702  [31232/60000]\n",
      "Training loss: 2.153071  [31296/60000]\n",
      "Training loss: 2.145493  [31360/60000]\n",
      "Training loss: 2.142645  [31424/60000]\n",
      "Training loss: 2.142875  [31488/60000]\n",
      "Training loss: 2.151242  [31552/60000]\n",
      "Training loss: 2.171323  [31616/60000]\n",
      "Training loss: 2.139254  [31680/60000]\n",
      "Training loss: 2.138726  [31744/60000]\n",
      "Training loss: 2.168641  [31808/60000]\n",
      "Training loss: 2.138558  [31872/60000]\n",
      "Training loss: 2.148671  [31936/60000]\n",
      "Training loss: 2.173039  [32000/60000]\n",
      "Training loss: 2.167561  [32064/60000]\n",
      "Training loss: 2.148446  [32128/60000]\n",
      "Training loss: 2.131916  [32192/60000]\n",
      "Training loss: 2.134343  [32256/60000]\n",
      "Training loss: 2.159036  [32320/60000]\n",
      "Training loss: 2.157662  [32384/60000]\n",
      "Training loss: 2.150571  [32448/60000]\n",
      "Training loss: 2.161242  [32512/60000]\n",
      "Training loss: 2.151221  [32576/60000]\n",
      "Training loss: 2.161587  [32640/60000]\n",
      "Training loss: 2.165572  [32704/60000]\n",
      "Training loss: 2.169589  [32768/60000]\n",
      "Training loss: 2.156505  [32832/60000]\n",
      "Training loss: 2.154859  [32896/60000]\n",
      "Training loss: 2.153111  [32960/60000]\n",
      "Training loss: 2.151115  [33024/60000]\n",
      "Training loss: 2.148990  [33088/60000]\n",
      "Training loss: 2.150270  [33152/60000]\n",
      "Training loss: 2.138943  [33216/60000]\n",
      "Training loss: 2.146509  [33280/60000]\n",
      "Training loss: 2.156632  [33344/60000]\n",
      "Training loss: 2.125704  [33408/60000]\n",
      "Training loss: 2.140618  [33472/60000]\n",
      "Training loss: 2.167462  [33536/60000]\n",
      "Training loss: 2.136307  [33600/60000]\n",
      "Training loss: 2.169405  [33664/60000]\n",
      "Training loss: 2.136158  [33728/60000]\n",
      "Training loss: 2.139874  [33792/60000]\n",
      "Training loss: 2.156001  [33856/60000]\n",
      "Training loss: 2.144387  [33920/60000]\n",
      "Training loss: 2.150679  [33984/60000]\n",
      "Training loss: 2.143666  [34048/60000]\n",
      "Training loss: 2.131217  [34112/60000]\n",
      "Training loss: 2.122799  [34176/60000]\n",
      "Training loss: 2.158961  [34240/60000]\n",
      "Training loss: 2.164652  [34304/60000]\n",
      "Training loss: 2.133217  [34368/60000]\n",
      "Training loss: 2.149332  [34432/60000]\n",
      "Training loss: 2.140530  [34496/60000]\n",
      "Training loss: 2.161851  [34560/60000]\n",
      "Training loss: 2.145217  [34624/60000]\n",
      "Training loss: 2.140956  [34688/60000]\n",
      "Training loss: 2.159364  [34752/60000]\n",
      "Training loss: 2.127548  [34816/60000]\n",
      "Training loss: 2.138424  [34880/60000]\n",
      "Training loss: 2.131278  [34944/60000]\n",
      "Training loss: 2.155208  [35008/60000]\n",
      "Training loss: 2.168748  [35072/60000]\n",
      "Training loss: 2.139853  [35136/60000]\n",
      "Training loss: 2.147031  [35200/60000]\n",
      "Training loss: 2.166626  [35264/60000]\n",
      "Training loss: 2.157205  [35328/60000]\n",
      "Training loss: 2.170987  [35392/60000]\n",
      "Training loss: 2.148038  [35456/60000]\n",
      "Training loss: 2.137979  [35520/60000]\n",
      "Training loss: 2.139455  [35584/60000]\n",
      "Training loss: 2.145014  [35648/60000]\n",
      "Training loss: 2.142505  [35712/60000]\n",
      "Training loss: 2.133921  [35776/60000]\n",
      "Training loss: 2.142416  [35840/60000]\n",
      "Training loss: 2.138143  [35904/60000]\n",
      "Training loss: 2.132258  [35968/60000]\n",
      "Training loss: 2.121571  [36032/60000]\n",
      "Training loss: 2.146674  [36096/60000]\n",
      "Training loss: 2.143253  [36160/60000]\n",
      "Training loss: 2.161258  [36224/60000]\n",
      "Training loss: 2.159223  [36288/60000]\n",
      "Training loss: 2.132145  [36352/60000]\n",
      "Training loss: 2.123307  [36416/60000]\n",
      "Training loss: 2.128649  [36480/60000]\n",
      "Training loss: 2.129812  [36544/60000]\n",
      "Training loss: 2.125300  [36608/60000]\n",
      "Training loss: 2.117068  [36672/60000]\n",
      "Training loss: 2.128750  [36736/60000]\n",
      "Training loss: 2.128340  [36800/60000]\n",
      "Training loss: 2.146780  [36864/60000]\n",
      "Training loss: 2.140813  [36928/60000]\n",
      "Training loss: 2.140302  [36992/60000]\n",
      "Training loss: 2.145681  [37056/60000]\n",
      "Training loss: 2.160353  [37120/60000]\n",
      "Training loss: 2.131649  [37184/60000]\n",
      "Training loss: 2.148872  [37248/60000]\n",
      "Training loss: 2.134016  [37312/60000]\n",
      "Training loss: 2.127348  [37376/60000]\n",
      "Training loss: 2.135334  [37440/60000]\n",
      "Training loss: 2.142355  [37504/60000]\n",
      "Training loss: 2.140006  [37568/60000]\n",
      "Training loss: 2.154649  [37632/60000]\n",
      "Training loss: 2.162364  [37696/60000]\n",
      "Training loss: 2.137596  [37760/60000]\n",
      "Training loss: 2.154306  [37824/60000]\n",
      "Training loss: 2.121868  [37888/60000]\n",
      "Training loss: 2.164328  [37952/60000]\n",
      "Training loss: 2.151897  [38016/60000]\n",
      "Training loss: 2.150727  [38080/60000]\n",
      "Training loss: 2.142915  [38144/60000]\n",
      "Training loss: 2.138562  [38208/60000]\n",
      "Training loss: 2.168903  [38272/60000]\n",
      "Training loss: 2.104142  [38336/60000]\n",
      "Training loss: 2.098413  [38400/60000]\n",
      "Training loss: 2.140030  [38464/60000]\n",
      "Training loss: 2.159500  [38528/60000]\n",
      "Training loss: 2.141051  [38592/60000]\n",
      "Training loss: 2.143438  [38656/60000]\n",
      "Training loss: 2.128763  [38720/60000]\n",
      "Training loss: 2.125888  [38784/60000]\n",
      "Training loss: 2.155499  [38848/60000]\n",
      "Training loss: 2.102006  [38912/60000]\n",
      "Training loss: 2.120760  [38976/60000]\n",
      "Training loss: 2.116404  [39040/60000]\n",
      "Training loss: 2.128808  [39104/60000]\n",
      "Training loss: 2.159961  [39168/60000]\n",
      "Training loss: 2.157662  [39232/60000]\n",
      "Training loss: 2.132698  [39296/60000]\n",
      "Training loss: 2.129941  [39360/60000]\n",
      "Training loss: 2.144372  [39424/60000]\n",
      "Training loss: 2.133646  [39488/60000]\n",
      "Training loss: 2.105138  [39552/60000]\n",
      "Training loss: 2.132675  [39616/60000]\n",
      "Training loss: 2.140804  [39680/60000]\n",
      "Training loss: 2.130853  [39744/60000]\n",
      "Training loss: 2.124659  [39808/60000]\n",
      "Training loss: 2.138856  [39872/60000]\n",
      "Training loss: 2.111380  [39936/60000]\n",
      "Training loss: 2.139164  [40000/60000]\n",
      "Training loss: 2.154706  [40064/60000]\n",
      "Training loss: 2.120609  [40128/60000]\n",
      "Training loss: 2.142210  [40192/60000]\n",
      "Training loss: 2.142980  [40256/60000]\n",
      "Training loss: 2.146116  [40320/60000]\n",
      "Training loss: 2.115506  [40384/60000]\n",
      "Training loss: 2.133812  [40448/60000]\n",
      "Training loss: 2.126773  [40512/60000]\n",
      "Training loss: 2.150548  [40576/60000]\n",
      "Training loss: 2.144530  [40640/60000]\n",
      "Training loss: 2.154360  [40704/60000]\n",
      "Training loss: 2.150331  [40768/60000]\n",
      "Training loss: 2.138071  [40832/60000]\n",
      "Training loss: 2.126438  [40896/60000]\n",
      "Training loss: 2.133405  [40960/60000]\n",
      "Training loss: 2.156656  [41024/60000]\n",
      "Training loss: 2.143286  [41088/60000]\n",
      "Training loss: 2.138219  [41152/60000]\n",
      "Training loss: 2.145094  [41216/60000]\n",
      "Training loss: 2.141103  [41280/60000]\n",
      "Training loss: 2.130820  [41344/60000]\n",
      "Training loss: 2.133954  [41408/60000]\n",
      "Training loss: 2.111071  [41472/60000]\n",
      "Training loss: 2.109756  [41536/60000]\n",
      "Training loss: 2.114838  [41600/60000]\n",
      "Training loss: 2.128534  [41664/60000]\n",
      "Training loss: 2.113219  [41728/60000]\n",
      "Training loss: 2.140399  [41792/60000]\n",
      "Training loss: 2.159644  [41856/60000]\n",
      "Training loss: 2.118172  [41920/60000]\n",
      "Training loss: 2.116940  [41984/60000]\n",
      "Training loss: 2.139800  [42048/60000]\n",
      "Training loss: 2.115693  [42112/60000]\n",
      "Training loss: 2.120713  [42176/60000]\n",
      "Training loss: 2.143234  [42240/60000]\n",
      "Training loss: 2.127887  [42304/60000]\n",
      "Training loss: 2.135436  [42368/60000]\n",
      "Training loss: 2.137697  [42432/60000]\n",
      "Training loss: 2.149990  [42496/60000]\n",
      "Training loss: 2.133822  [42560/60000]\n",
      "Training loss: 2.140191  [42624/60000]\n",
      "Training loss: 2.141773  [42688/60000]\n",
      "Training loss: 2.125630  [42752/60000]\n",
      "Training loss: 2.133285  [42816/60000]\n",
      "Training loss: 2.114793  [42880/60000]\n",
      "Training loss: 2.153163  [42944/60000]\n",
      "Training loss: 2.125170  [43008/60000]\n",
      "Training loss: 2.140637  [43072/60000]\n",
      "Training loss: 2.120294  [43136/60000]\n",
      "Training loss: 2.141907  [43200/60000]\n",
      "Training loss: 2.149191  [43264/60000]\n",
      "Training loss: 2.119617  [43328/60000]\n",
      "Training loss: 2.135783  [43392/60000]\n",
      "Training loss: 2.112584  [43456/60000]\n",
      "Training loss: 2.140667  [43520/60000]\n",
      "Training loss: 2.120087  [43584/60000]\n",
      "Training loss: 2.141796  [43648/60000]\n",
      "Training loss: 2.132996  [43712/60000]\n",
      "Training loss: 2.131832  [43776/60000]\n",
      "Training loss: 2.123079  [43840/60000]\n",
      "Training loss: 2.108759  [43904/60000]\n",
      "Training loss: 2.103803  [43968/60000]\n",
      "Training loss: 2.151325  [44032/60000]\n",
      "Training loss: 2.119858  [44096/60000]\n",
      "Training loss: 2.081409  [44160/60000]\n",
      "Training loss: 2.119323  [44224/60000]\n",
      "Training loss: 2.130165  [44288/60000]\n",
      "Training loss: 2.099690  [44352/60000]\n",
      "Training loss: 2.115941  [44416/60000]\n",
      "Training loss: 2.140760  [44480/60000]\n",
      "Training loss: 2.143380  [44544/60000]\n",
      "Training loss: 2.123483  [44608/60000]\n",
      "Training loss: 2.144535  [44672/60000]\n",
      "Training loss: 2.117818  [44736/60000]\n",
      "Training loss: 2.146696  [44800/60000]\n",
      "Training loss: 2.108793  [44864/60000]\n",
      "Training loss: 2.136653  [44928/60000]\n",
      "Training loss: 2.099193  [44992/60000]\n",
      "Training loss: 2.157503  [45056/60000]\n",
      "Training loss: 2.114935  [45120/60000]\n",
      "Training loss: 2.141114  [45184/60000]\n",
      "Training loss: 2.116734  [45248/60000]\n",
      "Training loss: 2.119559  [45312/60000]\n",
      "Training loss: 2.135530  [45376/60000]\n",
      "Training loss: 2.130067  [45440/60000]\n",
      "Training loss: 2.149692  [45504/60000]\n",
      "Training loss: 2.137884  [45568/60000]\n",
      "Training loss: 2.117956  [45632/60000]\n",
      "Training loss: 2.121417  [45696/60000]\n",
      "Training loss: 2.104184  [45760/60000]\n",
      "Training loss: 2.111611  [45824/60000]\n",
      "Training loss: 2.152266  [45888/60000]\n",
      "Training loss: 2.137445  [45952/60000]\n",
      "Training loss: 2.128838  [46016/60000]\n",
      "Training loss: 2.117152  [46080/60000]\n",
      "Training loss: 2.095910  [46144/60000]\n",
      "Training loss: 2.142882  [46208/60000]\n",
      "Training loss: 2.107452  [46272/60000]\n",
      "Training loss: 2.151459  [46336/60000]\n",
      "Training loss: 2.110256  [46400/60000]\n",
      "Training loss: 2.130645  [46464/60000]\n",
      "Training loss: 2.130968  [46528/60000]\n",
      "Training loss: 2.132875  [46592/60000]\n",
      "Training loss: 2.101056  [46656/60000]\n",
      "Training loss: 2.106316  [46720/60000]\n",
      "Training loss: 2.133305  [46784/60000]\n",
      "Training loss: 2.138967  [46848/60000]\n",
      "Training loss: 2.091262  [46912/60000]\n",
      "Training loss: 2.124316  [46976/60000]\n",
      "Training loss: 2.117108  [47040/60000]\n",
      "Training loss: 2.117779  [47104/60000]\n",
      "Training loss: 2.106354  [47168/60000]\n",
      "Training loss: 2.108352  [47232/60000]\n",
      "Training loss: 2.145042  [47296/60000]\n",
      "Training loss: 2.113259  [47360/60000]\n",
      "Training loss: 2.147772  [47424/60000]\n",
      "Training loss: 2.088106  [47488/60000]\n",
      "Training loss: 2.133632  [47552/60000]\n",
      "Training loss: 2.118439  [47616/60000]\n",
      "Training loss: 2.114837  [47680/60000]\n",
      "Training loss: 2.103990  [47744/60000]\n",
      "Training loss: 2.120439  [47808/60000]\n",
      "Training loss: 2.095832  [47872/60000]\n",
      "Training loss: 2.115124  [47936/60000]\n",
      "Training loss: 2.124416  [48000/60000]\n",
      "Training loss: 2.115419  [48064/60000]\n",
      "Training loss: 2.151818  [48128/60000]\n",
      "Training loss: 2.119943  [48192/60000]\n",
      "Training loss: 2.134121  [48256/60000]\n",
      "Training loss: 2.096103  [48320/60000]\n",
      "Training loss: 2.100529  [48384/60000]\n",
      "Training loss: 2.118817  [48448/60000]\n",
      "Training loss: 2.101721  [48512/60000]\n",
      "Training loss: 2.114891  [48576/60000]\n",
      "Training loss: 2.124650  [48640/60000]\n",
      "Training loss: 2.136482  [48704/60000]\n",
      "Training loss: 2.131304  [48768/60000]\n",
      "Training loss: 2.108969  [48832/60000]\n",
      "Training loss: 2.098000  [48896/60000]\n",
      "Training loss: 2.103329  [48960/60000]\n",
      "Training loss: 2.118329  [49024/60000]\n",
      "Training loss: 2.132602  [49088/60000]\n",
      "Training loss: 2.129655  [49152/60000]\n",
      "Training loss: 2.131386  [49216/60000]\n",
      "Training loss: 2.127514  [49280/60000]\n",
      "Training loss: 2.093822  [49344/60000]\n",
      "Training loss: 2.118575  [49408/60000]\n",
      "Training loss: 2.099494  [49472/60000]\n",
      "Training loss: 2.136382  [49536/60000]\n",
      "Training loss: 2.100781  [49600/60000]\n",
      "Training loss: 2.077593  [49664/60000]\n",
      "Training loss: 2.133246  [49728/60000]\n",
      "Training loss: 2.106461  [49792/60000]\n",
      "Training loss: 2.122771  [49856/60000]\n",
      "Training loss: 2.132199  [49920/60000]\n",
      "Training loss: 2.087476  [49984/60000]\n",
      "Training loss: 2.130925  [50048/60000]\n",
      "Training loss: 2.101237  [50112/60000]\n",
      "Training loss: 2.114988  [50176/60000]\n",
      "Training loss: 2.076718  [50240/60000]\n",
      "Training loss: 2.104455  [50304/60000]\n",
      "Training loss: 2.107758  [50368/60000]\n",
      "Training loss: 2.126247  [50432/60000]\n",
      "Training loss: 2.120635  [50496/60000]\n",
      "Training loss: 2.078071  [50560/60000]\n",
      "Training loss: 2.136139  [50624/60000]\n",
      "Training loss: 2.134158  [50688/60000]\n",
      "Training loss: 2.106601  [50752/60000]\n",
      "Training loss: 2.094220  [50816/60000]\n",
      "Training loss: 2.124477  [50880/60000]\n",
      "Training loss: 2.097198  [50944/60000]\n",
      "Training loss: 2.120598  [51008/60000]\n",
      "Training loss: 2.108768  [51072/60000]\n",
      "Training loss: 2.125616  [51136/60000]\n",
      "Training loss: 2.106767  [51200/60000]\n",
      "Training loss: 2.094266  [51264/60000]\n",
      "Training loss: 2.102195  [51328/60000]\n",
      "Training loss: 2.125112  [51392/60000]\n",
      "Training loss: 2.118475  [51456/60000]\n",
      "Training loss: 2.120186  [51520/60000]\n",
      "Training loss: 2.100128  [51584/60000]\n",
      "Training loss: 2.108628  [51648/60000]\n",
      "Training loss: 2.108165  [51712/60000]\n",
      "Training loss: 2.096763  [51776/60000]\n",
      "Training loss: 2.095451  [51840/60000]\n",
      "Training loss: 2.088573  [51904/60000]\n",
      "Training loss: 2.105865  [51968/60000]\n",
      "Training loss: 2.127400  [52032/60000]\n",
      "Training loss: 2.078627  [52096/60000]\n",
      "Training loss: 2.103627  [52160/60000]\n",
      "Training loss: 2.076664  [52224/60000]\n",
      "Training loss: 2.122813  [52288/60000]\n",
      "Training loss: 2.112370  [52352/60000]\n",
      "Training loss: 2.138060  [52416/60000]\n",
      "Training loss: 2.109760  [52480/60000]\n",
      "Training loss: 2.104254  [52544/60000]\n",
      "Training loss: 2.111472  [52608/60000]\n",
      "Training loss: 2.101054  [52672/60000]\n",
      "Training loss: 2.091382  [52736/60000]\n",
      "Training loss: 2.100930  [52800/60000]\n",
      "Training loss: 2.125437  [52864/60000]\n",
      "Training loss: 2.104508  [52928/60000]\n",
      "Training loss: 2.102450  [52992/60000]\n",
      "Training loss: 2.118860  [53056/60000]\n",
      "Training loss: 2.073027  [53120/60000]\n",
      "Training loss: 2.059124  [53184/60000]\n",
      "Training loss: 2.085135  [53248/60000]\n",
      "Training loss: 2.097907  [53312/60000]\n",
      "Training loss: 2.086248  [53376/60000]\n",
      "Training loss: 2.053882  [53440/60000]\n",
      "Training loss: 2.142723  [53504/60000]\n",
      "Training loss: 2.073502  [53568/60000]\n",
      "Training loss: 2.098158  [53632/60000]\n",
      "Training loss: 2.101506  [53696/60000]\n",
      "Training loss: 2.107002  [53760/60000]\n",
      "Training loss: 2.101715  [53824/60000]\n",
      "Training loss: 2.109254  [53888/60000]\n",
      "Training loss: 2.125858  [53952/60000]\n",
      "Training loss: 2.095402  [54016/60000]\n",
      "Training loss: 2.091849  [54080/60000]\n",
      "Training loss: 2.127035  [54144/60000]\n",
      "Training loss: 2.107007  [54208/60000]\n",
      "Training loss: 2.108905  [54272/60000]\n",
      "Training loss: 2.150269  [54336/60000]\n",
      "Training loss: 2.116052  [54400/60000]\n",
      "Training loss: 2.103927  [54464/60000]\n",
      "Training loss: 2.086133  [54528/60000]\n",
      "Training loss: 2.114857  [54592/60000]\n",
      "Training loss: 2.118398  [54656/60000]\n",
      "Training loss: 2.094011  [54720/60000]\n",
      "Training loss: 2.126463  [54784/60000]\n",
      "Training loss: 2.122176  [54848/60000]\n",
      "Training loss: 2.102010  [54912/60000]\n",
      "Training loss: 2.134933  [54976/60000]\n",
      "Training loss: 2.095086  [55040/60000]\n",
      "Training loss: 2.119161  [55104/60000]\n",
      "Training loss: 2.071315  [55168/60000]\n",
      "Training loss: 2.087932  [55232/60000]\n",
      "Training loss: 2.116198  [55296/60000]\n",
      "Training loss: 2.115340  [55360/60000]\n",
      "Training loss: 2.059560  [55424/60000]\n",
      "Training loss: 2.110369  [55488/60000]\n",
      "Training loss: 2.091677  [55552/60000]\n",
      "Training loss: 2.110101  [55616/60000]\n",
      "Training loss: 2.113884  [55680/60000]\n",
      "Training loss: 2.113868  [55744/60000]\n",
      "Training loss: 2.106021  [55808/60000]\n",
      "Training loss: 2.083843  [55872/60000]\n",
      "Training loss: 2.062964  [55936/60000]\n",
      "Training loss: 2.115731  [56000/60000]\n",
      "Training loss: 2.134187  [56064/60000]\n",
      "Training loss: 2.086364  [56128/60000]\n",
      "Training loss: 2.123562  [56192/60000]\n",
      "Training loss: 2.111687  [56256/60000]\n",
      "Training loss: 2.114671  [56320/60000]\n",
      "Training loss: 2.095146  [56384/60000]\n",
      "Training loss: 2.094383  [56448/60000]\n",
      "Training loss: 2.099102  [56512/60000]\n",
      "Training loss: 2.118805  [56576/60000]\n",
      "Training loss: 2.092466  [56640/60000]\n",
      "Training loss: 2.085270  [56704/60000]\n",
      "Training loss: 2.095592  [56768/60000]\n",
      "Training loss: 2.068961  [56832/60000]\n",
      "Training loss: 2.102582  [56896/60000]\n",
      "Training loss: 2.076275  [56960/60000]\n",
      "Training loss: 2.109700  [57024/60000]\n",
      "Training loss: 2.110117  [57088/60000]\n",
      "Training loss: 2.099872  [57152/60000]\n",
      "Training loss: 2.119254  [57216/60000]\n",
      "Training loss: 2.101891  [57280/60000]\n",
      "Training loss: 2.106878  [57344/60000]\n",
      "Training loss: 2.073516  [57408/60000]\n",
      "Training loss: 2.110826  [57472/60000]\n",
      "Training loss: 2.118317  [57536/60000]\n",
      "Training loss: 2.067963  [57600/60000]\n",
      "Training loss: 2.086095  [57664/60000]\n",
      "Training loss: 2.055047  [57728/60000]\n",
      "Training loss: 2.088940  [57792/60000]\n",
      "Training loss: 2.098369  [57856/60000]\n",
      "Training loss: 2.111378  [57920/60000]\n",
      "Training loss: 2.132042  [57984/60000]\n",
      "Training loss: 2.120232  [58048/60000]\n",
      "Training loss: 2.082866  [58112/60000]\n",
      "Training loss: 2.056701  [58176/60000]\n",
      "Training loss: 2.123949  [58240/60000]\n",
      "Training loss: 2.062762  [58304/60000]\n",
      "Training loss: 2.093353  [58368/60000]\n",
      "Training loss: 2.099192  [58432/60000]\n",
      "Training loss: 2.072909  [58496/60000]\n",
      "Training loss: 2.096536  [58560/60000]\n",
      "Training loss: 2.088016  [58624/60000]\n",
      "Training loss: 2.072033  [58688/60000]\n",
      "Training loss: 2.095798  [58752/60000]\n",
      "Training loss: 2.066319  [58816/60000]\n",
      "Training loss: 2.056888  [58880/60000]\n",
      "Training loss: 2.083820  [58944/60000]\n",
      "Training loss: 2.077441  [59008/60000]\n",
      "Training loss: 2.095459  [59072/60000]\n",
      "Training loss: 2.097028  [59136/60000]\n",
      "Training loss: 2.117649  [59200/60000]\n",
      "Training loss: 2.086509  [59264/60000]\n",
      "Training loss: 2.070553  [59328/60000]\n",
      "Training loss: 2.108887  [59392/60000]\n",
      "Training loss: 2.137836  [59456/60000]\n",
      "Training loss: 2.103550  [59520/60000]\n",
      "Training loss: 2.080331  [59584/60000]\n",
      "Training loss: 2.109581  [59648/60000]\n",
      "Training loss: 2.068000  [59712/60000]\n",
      "Training loss: 2.088222  [59776/60000]\n",
      "Training loss: 2.090581  [59840/60000]\n",
      "Training loss: 2.100282  [59904/60000]\n",
      "Training loss: 2.117790  [29984/60000]\n",
      "\n",
      " Finished Epoch:  2\n",
      "--------------------------------------------\n",
      "Training loss: 2.093259  [   64/60000]\n",
      "Training loss: 2.094851  [  128/60000]\n",
      "Training loss: 2.116131  [  192/60000]\n",
      "Training loss: 2.101463  [  256/60000]\n",
      "Training loss: 2.060246  [  320/60000]\n",
      "Training loss: 2.120355  [  384/60000]\n",
      "Training loss: 2.093120  [  448/60000]\n",
      "Training loss: 2.095732  [  512/60000]\n",
      "Training loss: 2.067655  [  576/60000]\n",
      "Training loss: 2.103647  [  640/60000]\n",
      "Training loss: 2.096969  [  704/60000]\n",
      "Training loss: 2.093268  [  768/60000]\n",
      "Training loss: 2.108514  [  832/60000]\n",
      "Training loss: 2.077070  [  896/60000]\n",
      "Training loss: 2.106830  [  960/60000]\n",
      "Training loss: 2.072958  [ 1024/60000]\n",
      "Training loss: 2.078119  [ 1088/60000]\n",
      "Training loss: 2.072219  [ 1152/60000]\n",
      "Training loss: 2.083973  [ 1216/60000]\n",
      "Training loss: 2.071201  [ 1280/60000]\n",
      "Training loss: 2.083721  [ 1344/60000]\n",
      "Training loss: 2.109160  [ 1408/60000]\n",
      "Training loss: 2.090893  [ 1472/60000]\n",
      "Training loss: 2.102504  [ 1536/60000]\n",
      "Training loss: 2.083928  [ 1600/60000]\n",
      "Training loss: 2.079084  [ 1664/60000]\n",
      "Training loss: 2.095399  [ 1728/60000]\n",
      "Training loss: 2.106612  [ 1792/60000]\n",
      "Training loss: 2.058388  [ 1856/60000]\n",
      "Training loss: 2.081270  [ 1920/60000]\n",
      "Training loss: 2.060685  [ 1984/60000]\n",
      "Training loss: 2.105330  [ 2048/60000]\n",
      "Training loss: 2.113884  [ 2112/60000]\n",
      "Training loss: 2.106158  [ 2176/60000]\n",
      "Training loss: 2.076234  [ 2240/60000]\n",
      "Training loss: 2.088164  [ 2304/60000]\n",
      "Training loss: 2.070639  [ 2368/60000]\n",
      "Training loss: 2.093524  [ 2432/60000]\n",
      "Training loss: 2.070838  [ 2496/60000]\n",
      "Training loss: 2.056395  [ 2560/60000]\n",
      "Training loss: 2.079044  [ 2624/60000]\n",
      "Training loss: 2.103621  [ 2688/60000]\n",
      "Training loss: 2.118096  [ 2752/60000]\n",
      "Training loss: 2.093198  [ 2816/60000]\n",
      "Training loss: 2.081333  [ 2880/60000]\n",
      "Training loss: 2.099443  [ 2944/60000]\n",
      "Training loss: 2.070848  [ 3008/60000]\n",
      "Training loss: 2.086499  [ 3072/60000]\n",
      "Training loss: 2.084101  [ 3136/60000]\n",
      "Training loss: 2.077409  [ 3200/60000]\n",
      "Training loss: 2.062651  [ 3264/60000]\n",
      "Training loss: 2.040692  [ 3328/60000]\n",
      "Training loss: 2.067870  [ 3392/60000]\n",
      "Training loss: 2.110387  [ 3456/60000]\n",
      "Training loss: 2.063300  [ 3520/60000]\n",
      "Training loss: 2.073023  [ 3584/60000]\n",
      "Training loss: 2.103293  [ 3648/60000]\n",
      "Training loss: 2.124316  [ 3712/60000]\n",
      "Training loss: 2.084227  [ 3776/60000]\n",
      "Training loss: 2.098137  [ 3840/60000]\n",
      "Training loss: 2.053779  [ 3904/60000]\n",
      "Training loss: 2.094382  [ 3968/60000]\n",
      "Training loss: 2.105151  [ 4032/60000]\n",
      "Training loss: 2.100401  [ 4096/60000]\n",
      "Training loss: 2.088092  [ 4160/60000]\n",
      "Training loss: 2.092760  [ 4224/60000]\n",
      "Training loss: 2.087845  [ 4288/60000]\n",
      "Training loss: 2.094008  [ 4352/60000]\n",
      "Training loss: 2.092451  [ 4416/60000]\n",
      "Training loss: 2.105220  [ 4480/60000]\n",
      "Training loss: 2.082468  [ 4544/60000]\n",
      "Training loss: 2.102526  [ 4608/60000]\n",
      "Training loss: 2.080737  [ 4672/60000]\n",
      "Training loss: 2.046766  [ 4736/60000]\n",
      "Training loss: 2.108181  [ 4800/60000]\n",
      "Training loss: 2.103069  [ 4864/60000]\n",
      "Training loss: 2.077168  [ 4928/60000]\n",
      "Training loss: 2.083058  [ 4992/60000]\n",
      "Training loss: 2.051480  [ 5056/60000]\n",
      "Training loss: 2.069500  [ 5120/60000]\n",
      "Training loss: 2.067040  [ 5184/60000]\n",
      "Training loss: 2.104166  [ 5248/60000]\n",
      "Training loss: 2.108404  [ 5312/60000]\n",
      "Training loss: 2.081944  [ 5376/60000]\n",
      "Training loss: 2.117774  [ 5440/60000]\n",
      "Training loss: 2.096898  [ 5504/60000]\n",
      "Training loss: 2.095203  [ 5568/60000]\n",
      "Training loss: 2.096485  [ 5632/60000]\n",
      "Training loss: 2.085694  [ 5696/60000]\n",
      "Training loss: 2.088450  [ 5760/60000]\n",
      "Training loss: 2.085474  [ 5824/60000]\n",
      "Training loss: 2.048056  [ 5888/60000]\n",
      "Training loss: 2.115369  [ 5952/60000]\n",
      "Training loss: 2.079713  [ 6016/60000]\n",
      "Training loss: 2.090537  [ 6080/60000]\n",
      "Training loss: 2.091230  [ 6144/60000]\n",
      "Training loss: 2.097103  [ 6208/60000]\n",
      "Training loss: 2.078267  [ 6272/60000]\n",
      "Training loss: 2.075924  [ 6336/60000]\n",
      "Training loss: 2.101922  [ 6400/60000]\n",
      "Training loss: 2.053090  [ 6464/60000]\n",
      "Training loss: 2.090468  [ 6528/60000]\n",
      "Training loss: 2.071323  [ 6592/60000]\n",
      "Training loss: 2.073027  [ 6656/60000]\n",
      "Training loss: 2.092252  [ 6720/60000]\n",
      "Training loss: 2.046011  [ 6784/60000]\n",
      "Training loss: 2.080297  [ 6848/60000]\n",
      "Training loss: 2.101895  [ 6912/60000]\n",
      "Training loss: 2.069902  [ 6976/60000]\n",
      "Training loss: 2.067087  [ 7040/60000]\n",
      "Training loss: 2.113170  [ 7104/60000]\n",
      "Training loss: 2.074880  [ 7168/60000]\n",
      "Training loss: 2.073694  [ 7232/60000]\n",
      "Training loss: 2.066838  [ 7296/60000]\n",
      "Training loss: 2.100642  [ 7360/60000]\n",
      "Training loss: 2.074764  [ 7424/60000]\n",
      "Training loss: 2.059054  [ 7488/60000]\n",
      "Training loss: 2.028301  [ 7552/60000]\n",
      "Training loss: 2.064456  [ 7616/60000]\n",
      "Training loss: 2.080561  [ 7680/60000]\n",
      "Training loss: 2.078472  [ 7744/60000]\n",
      "Training loss: 2.082767  [ 7808/60000]\n",
      "Training loss: 2.048112  [ 7872/60000]\n",
      "Training loss: 2.087777  [ 7936/60000]\n",
      "Training loss: 2.068114  [ 8000/60000]\n",
      "Training loss: 2.082002  [ 8064/60000]\n",
      "Training loss: 2.065882  [ 8128/60000]\n",
      "Training loss: 2.104736  [ 8192/60000]\n",
      "Training loss: 2.114994  [ 8256/60000]\n",
      "Training loss: 2.116839  [ 8320/60000]\n",
      "Training loss: 2.076824  [ 8384/60000]\n",
      "Training loss: 2.053070  [ 8448/60000]\n",
      "Training loss: 2.082708  [ 8512/60000]\n",
      "Training loss: 2.067237  [ 8576/60000]\n",
      "Training loss: 2.084224  [ 8640/60000]\n",
      "Training loss: 2.029044  [ 8704/60000]\n",
      "Training loss: 2.056996  [ 8768/60000]\n",
      "Training loss: 2.083124  [ 8832/60000]\n",
      "Training loss: 2.060015  [ 8896/60000]\n",
      "Training loss: 2.095182  [ 8960/60000]\n",
      "Training loss: 2.060121  [ 9024/60000]\n",
      "Training loss: 2.101840  [ 9088/60000]\n",
      "Training loss: 2.088682  [ 9152/60000]\n",
      "Training loss: 2.045059  [ 9216/60000]\n",
      "Training loss: 2.076526  [ 9280/60000]\n",
      "Training loss: 2.059875  [ 9344/60000]\n",
      "Training loss: 2.073034  [ 9408/60000]\n",
      "Training loss: 2.063619  [ 9472/60000]\n",
      "Training loss: 2.057042  [ 9536/60000]\n",
      "Training loss: 2.074197  [ 9600/60000]\n",
      "Training loss: 2.056210  [ 9664/60000]\n",
      "Training loss: 2.068438  [ 9728/60000]\n",
      "Training loss: 2.088549  [ 9792/60000]\n",
      "Training loss: 2.074076  [ 9856/60000]\n",
      "Training loss: 2.055527  [ 9920/60000]\n",
      "Training loss: 2.087416  [ 9984/60000]\n",
      "Training loss: 2.089124  [10048/60000]\n",
      "Training loss: 2.078493  [10112/60000]\n",
      "Training loss: 2.073470  [10176/60000]\n",
      "Training loss: 2.103921  [10240/60000]\n",
      "Training loss: 2.089534  [10304/60000]\n",
      "Training loss: 2.091674  [10368/60000]\n",
      "Training loss: 2.094942  [10432/60000]\n",
      "Training loss: 2.079127  [10496/60000]\n",
      "Training loss: 2.083196  [10560/60000]\n",
      "Training loss: 2.039655  [10624/60000]\n",
      "Training loss: 2.056047  [10688/60000]\n",
      "Training loss: 2.097785  [10752/60000]\n",
      "Training loss: 2.095727  [10816/60000]\n",
      "Training loss: 2.080968  [10880/60000]\n",
      "Training loss: 2.062277  [10944/60000]\n",
      "Training loss: 2.050198  [11008/60000]\n",
      "Training loss: 2.103622  [11072/60000]\n",
      "Training loss: 2.052548  [11136/60000]\n",
      "Training loss: 2.109649  [11200/60000]\n",
      "Training loss: 2.068472  [11264/60000]\n",
      "Training loss: 2.067282  [11328/60000]\n",
      "Training loss: 2.065565  [11392/60000]\n",
      "Training loss: 2.045108  [11456/60000]\n",
      "Training loss: 2.074606  [11520/60000]\n",
      "Training loss: 2.047837  [11584/60000]\n",
      "Training loss: 2.081015  [11648/60000]\n",
      "Training loss: 2.029496  [11712/60000]\n",
      "Training loss: 2.086485  [11776/60000]\n",
      "Training loss: 2.093677  [11840/60000]\n",
      "Training loss: 2.051396  [11904/60000]\n",
      "Training loss: 2.052539  [11968/60000]\n",
      "Training loss: 2.077382  [12032/60000]\n",
      "Training loss: 2.041341  [12096/60000]\n",
      "Training loss: 2.073922  [12160/60000]\n",
      "Training loss: 2.061813  [12224/60000]\n",
      "Training loss: 2.077190  [12288/60000]\n",
      "Training loss: 2.069320  [12352/60000]\n",
      "Training loss: 2.073892  [12416/60000]\n",
      "Training loss: 2.071685  [12480/60000]\n",
      "Training loss: 2.041894  [12544/60000]\n",
      "Training loss: 2.052020  [12608/60000]\n",
      "Training loss: 2.074560  [12672/60000]\n",
      "Training loss: 2.018691  [12736/60000]\n",
      "Training loss: 2.081422  [12800/60000]\n",
      "Training loss: 2.062248  [12864/60000]\n",
      "Training loss: 2.081235  [12928/60000]\n",
      "Training loss: 2.089866  [12992/60000]\n",
      "Training loss: 2.058801  [13056/60000]\n",
      "Training loss: 2.076367  [13120/60000]\n",
      "Training loss: 2.067117  [13184/60000]\n",
      "Training loss: 2.033735  [13248/60000]\n",
      "Training loss: 2.072160  [13312/60000]\n",
      "Training loss: 2.005390  [13376/60000]\n",
      "Training loss: 2.032147  [13440/60000]\n",
      "Training loss: 2.026218  [13504/60000]\n",
      "Training loss: 2.069238  [13568/60000]\n",
      "Training loss: 2.053635  [13632/60000]\n",
      "Training loss: 2.066073  [13696/60000]\n",
      "Training loss: 2.043850  [13760/60000]\n",
      "Training loss: 2.057769  [13824/60000]\n",
      "Training loss: 2.055526  [13888/60000]\n",
      "Training loss: 2.052153  [13952/60000]\n",
      "Training loss: 2.045484  [14016/60000]\n",
      "Training loss: 2.014045  [14080/60000]\n",
      "Training loss: 2.054797  [14144/60000]\n",
      "Training loss: 2.076816  [14208/60000]\n",
      "Training loss: 2.045272  [14272/60000]\n",
      "Training loss: 2.044574  [14336/60000]\n",
      "Training loss: 2.010688  [14400/60000]\n",
      "Training loss: 2.056248  [14464/60000]\n",
      "Training loss: 2.090475  [14528/60000]\n",
      "Training loss: 2.055190  [14592/60000]\n",
      "Training loss: 2.043852  [14656/60000]\n",
      "Training loss: 2.046810  [14720/60000]\n",
      "Training loss: 2.059988  [14784/60000]\n",
      "Training loss: 2.097250  [14848/60000]\n",
      "Training loss: 2.017562  [14912/60000]\n",
      "Training loss: 2.083443  [14976/60000]\n",
      "Training loss: 2.075773  [15040/60000]\n",
      "Training loss: 2.021135  [15104/60000]\n",
      "Training loss: 2.029623  [15168/60000]\n",
      "Training loss: 2.043237  [15232/60000]\n",
      "Training loss: 2.057499  [15296/60000]\n",
      "Training loss: 2.068244  [15360/60000]\n",
      "Training loss: 2.084634  [15424/60000]\n",
      "Training loss: 2.054508  [15488/60000]\n",
      "Training loss: 2.008088  [15552/60000]\n",
      "Training loss: 2.093746  [15616/60000]\n",
      "Training loss: 2.046807  [15680/60000]\n",
      "Training loss: 2.062451  [15744/60000]\n",
      "Training loss: 2.061267  [15808/60000]\n",
      "Training loss: 2.035715  [15872/60000]\n",
      "Training loss: 2.038314  [15936/60000]\n",
      "Training loss: 2.049322  [16000/60000]\n",
      "Training loss: 2.062676  [16064/60000]\n",
      "Training loss: 2.029863  [16128/60000]\n",
      "Training loss: 2.041574  [16192/60000]\n",
      "Training loss: 2.066118  [16256/60000]\n",
      "Training loss: 2.050401  [16320/60000]\n",
      "Training loss: 2.085644  [16384/60000]\n",
      "Training loss: 2.065433  [16448/60000]\n",
      "Training loss: 1.977781  [16512/60000]\n",
      "Training loss: 2.046378  [16576/60000]\n",
      "Training loss: 2.038066  [16640/60000]\n",
      "Training loss: 2.052647  [16704/60000]\n",
      "Training loss: 2.045694  [16768/60000]\n",
      "Training loss: 2.029903  [16832/60000]\n",
      "Training loss: 2.036360  [16896/60000]\n",
      "Training loss: 2.034052  [16960/60000]\n",
      "Training loss: 2.071281  [17024/60000]\n",
      "Training loss: 2.064514  [17088/60000]\n",
      "Training loss: 2.058803  [17152/60000]\n",
      "Training loss: 2.038642  [17216/60000]\n",
      "Training loss: 2.071214  [17280/60000]\n",
      "Training loss: 2.056211  [17344/60000]\n",
      "Training loss: 2.048456  [17408/60000]\n",
      "Training loss: 2.035964  [17472/60000]\n",
      "Training loss: 2.005159  [17536/60000]\n",
      "Training loss: 2.070810  [17600/60000]\n",
      "Training loss: 2.052885  [17664/60000]\n",
      "Training loss: 2.052948  [17728/60000]\n",
      "Training loss: 2.046610  [17792/60000]\n",
      "Training loss: 2.076488  [17856/60000]\n",
      "Training loss: 2.088698  [17920/60000]\n",
      "Training loss: 2.039343  [17984/60000]\n",
      "Training loss: 2.060514  [18048/60000]\n",
      "Training loss: 2.020974  [18112/60000]\n",
      "Training loss: 2.055378  [18176/60000]\n",
      "Training loss: 2.002666  [18240/60000]\n",
      "Training loss: 2.081138  [18304/60000]\n",
      "Training loss: 2.004738  [18368/60000]\n",
      "Training loss: 2.044562  [18432/60000]\n",
      "Training loss: 2.032114  [18496/60000]\n",
      "Training loss: 2.078090  [18560/60000]\n",
      "Training loss: 1.998408  [18624/60000]\n",
      "Training loss: 1.978740  [18688/60000]\n",
      "Training loss: 2.038818  [18752/60000]\n",
      "Training loss: 2.053359  [18816/60000]\n",
      "Training loss: 2.057487  [18880/60000]\n",
      "Training loss: 2.062804  [18944/60000]\n",
      "Training loss: 2.036835  [19008/60000]\n",
      "Training loss: 2.042618  [19072/60000]\n",
      "Training loss: 2.022385  [19136/60000]\n",
      "Training loss: 2.067472  [19200/60000]\n",
      "Training loss: 2.019885  [19264/60000]\n",
      "Training loss: 2.041169  [19328/60000]\n",
      "Training loss: 2.022668  [19392/60000]\n",
      "Training loss: 2.030244  [19456/60000]\n",
      "Training loss: 2.063866  [19520/60000]\n",
      "Training loss: 2.042258  [19584/60000]\n",
      "Training loss: 2.023459  [19648/60000]\n",
      "Training loss: 2.046539  [19712/60000]\n",
      "Training loss: 2.067742  [19776/60000]\n",
      "Training loss: 2.030332  [19840/60000]\n",
      "Training loss: 2.037540  [19904/60000]\n",
      "Training loss: 2.069916  [19968/60000]\n",
      "Training loss: 2.058190  [20032/60000]\n",
      "Training loss: 2.049374  [20096/60000]\n",
      "Training loss: 2.091335  [20160/60000]\n",
      "Training loss: 2.063207  [20224/60000]\n",
      "Training loss: 2.035499  [20288/60000]\n",
      "Training loss: 2.058337  [20352/60000]\n",
      "Training loss: 2.021701  [20416/60000]\n",
      "Training loss: 2.021363  [20480/60000]\n",
      "Training loss: 2.055285  [20544/60000]\n",
      "Training loss: 2.048434  [20608/60000]\n",
      "Training loss: 2.034878  [20672/60000]\n",
      "Training loss: 2.018554  [20736/60000]\n",
      "Training loss: 2.005961  [20800/60000]\n",
      "Training loss: 2.054892  [20864/60000]\n",
      "Training loss: 2.009606  [20928/60000]\n",
      "Training loss: 2.017297  [20992/60000]\n",
      "Training loss: 2.096745  [21056/60000]\n",
      "Training loss: 2.020842  [21120/60000]\n",
      "Training loss: 2.052340  [21184/60000]\n",
      "Training loss: 2.001040  [21248/60000]\n",
      "Training loss: 2.075693  [21312/60000]\n",
      "Training loss: 2.029287  [21376/60000]\n",
      "Training loss: 2.040280  [21440/60000]\n",
      "Training loss: 2.034595  [21504/60000]\n",
      "Training loss: 2.001053  [21568/60000]\n",
      "Training loss: 2.038122  [21632/60000]\n",
      "Training loss: 2.054400  [21696/60000]\n",
      "Training loss: 2.051188  [21760/60000]\n",
      "Training loss: 2.039761  [21824/60000]\n",
      "Training loss: 2.026330  [21888/60000]\n",
      "Training loss: 2.072967  [21952/60000]\n",
      "Training loss: 2.045098  [22016/60000]\n",
      "Training loss: 1.985445  [22080/60000]\n",
      "Training loss: 2.048529  [22144/60000]\n",
      "Training loss: 2.079202  [22208/60000]\n",
      "Training loss: 1.989609  [22272/60000]\n",
      "Training loss: 2.069354  [22336/60000]\n",
      "Training loss: 2.049022  [22400/60000]\n",
      "Training loss: 2.043444  [22464/60000]\n",
      "Training loss: 2.026726  [22528/60000]\n",
      "Training loss: 2.035093  [22592/60000]\n",
      "Training loss: 2.060220  [22656/60000]\n",
      "Training loss: 2.037541  [22720/60000]\n",
      "Training loss: 2.054251  [22784/60000]\n",
      "Training loss: 2.006756  [22848/60000]\n",
      "Training loss: 2.007162  [22912/60000]\n",
      "Training loss: 2.037704  [22976/60000]\n",
      "Training loss: 1.998442  [23040/60000]\n",
      "Training loss: 2.045418  [23104/60000]\n",
      "Training loss: 1.993746  [23168/60000]\n",
      "Training loss: 2.016835  [23232/60000]\n",
      "Training loss: 2.014481  [23296/60000]\n",
      "Training loss: 2.054912  [23360/60000]\n",
      "Training loss: 2.004101  [23424/60000]\n",
      "Training loss: 2.070213  [23488/60000]\n",
      "Training loss: 2.021385  [23552/60000]\n",
      "Training loss: 1.980455  [23616/60000]\n",
      "Training loss: 2.044381  [23680/60000]\n",
      "Training loss: 2.039558  [23744/60000]\n",
      "Training loss: 1.983714  [23808/60000]\n",
      "Training loss: 2.021585  [23872/60000]\n",
      "Training loss: 2.041957  [23936/60000]\n",
      "Training loss: 2.004664  [24000/60000]\n",
      "Training loss: 2.050945  [24064/60000]\n",
      "Training loss: 2.010899  [24128/60000]\n",
      "Training loss: 2.043922  [24192/60000]\n",
      "Training loss: 2.015489  [24256/60000]\n",
      "Training loss: 2.053174  [24320/60000]\n",
      "Training loss: 2.023874  [24384/60000]\n",
      "Training loss: 2.047647  [24448/60000]\n",
      "Training loss: 2.063825  [24512/60000]\n",
      "Training loss: 2.031670  [24576/60000]\n",
      "Training loss: 2.010417  [24640/60000]\n",
      "Training loss: 2.032547  [24704/60000]\n",
      "Training loss: 2.045489  [24768/60000]\n",
      "Training loss: 2.020388  [24832/60000]\n",
      "Training loss: 2.069418  [24896/60000]\n",
      "Training loss: 2.007599  [24960/60000]\n",
      "Training loss: 2.024924  [25024/60000]\n",
      "Training loss: 2.047219  [25088/60000]\n",
      "Training loss: 2.043152  [25152/60000]\n",
      "Training loss: 2.027488  [25216/60000]\n",
      "Training loss: 2.034748  [25280/60000]\n",
      "Training loss: 2.031702  [25344/60000]\n",
      "Training loss: 2.005738  [25408/60000]\n",
      "Training loss: 2.022019  [25472/60000]\n",
      "Training loss: 2.001828  [25536/60000]\n",
      "Training loss: 2.023956  [25600/60000]\n",
      "Training loss: 1.969905  [25664/60000]\n",
      "Training loss: 2.026661  [25728/60000]\n",
      "Training loss: 2.058582  [25792/60000]\n",
      "Training loss: 2.032999  [25856/60000]\n",
      "Training loss: 2.001021  [25920/60000]\n",
      "Training loss: 2.031586  [25984/60000]\n",
      "Training loss: 2.015870  [26048/60000]\n",
      "Training loss: 2.048663  [26112/60000]\n",
      "Training loss: 2.012336  [26176/60000]\n",
      "Training loss: 2.004215  [26240/60000]\n",
      "Training loss: 2.008757  [26304/60000]\n",
      "Training loss: 2.037992  [26368/60000]\n",
      "Training loss: 1.989827  [26432/60000]\n",
      "Training loss: 2.008558  [26496/60000]\n",
      "Training loss: 2.025097  [26560/60000]\n",
      "Training loss: 2.028636  [26624/60000]\n",
      "Training loss: 2.043495  [26688/60000]\n",
      "Training loss: 2.023169  [26752/60000]\n",
      "Training loss: 2.063151  [26816/60000]\n",
      "Training loss: 2.033701  [26880/60000]\n",
      "Training loss: 1.947164  [26944/60000]\n",
      "Training loss: 2.029777  [27008/60000]\n",
      "Training loss: 2.002149  [27072/60000]\n",
      "Training loss: 2.022175  [27136/60000]\n",
      "Training loss: 2.005171  [27200/60000]\n",
      "Training loss: 1.997864  [27264/60000]\n",
      "Training loss: 2.052153  [27328/60000]\n",
      "Training loss: 2.020288  [27392/60000]\n",
      "Training loss: 2.013197  [27456/60000]\n",
      "Training loss: 2.033603  [27520/60000]\n",
      "Training loss: 2.041692  [27584/60000]\n",
      "Training loss: 1.989051  [27648/60000]\n",
      "Training loss: 2.021728  [27712/60000]\n",
      "Training loss: 1.995239  [27776/60000]\n",
      "Training loss: 1.973966  [27840/60000]\n",
      "Training loss: 2.032460  [27904/60000]\n",
      "Training loss: 1.993537  [27968/60000]\n",
      "Training loss: 2.020494  [28032/60000]\n",
      "Training loss: 2.027477  [28096/60000]\n",
      "Training loss: 2.040311  [28160/60000]\n",
      "Training loss: 2.032020  [28224/60000]\n",
      "Training loss: 1.998874  [28288/60000]\n",
      "Training loss: 2.003174  [28352/60000]\n",
      "Training loss: 2.063653  [28416/60000]\n",
      "Training loss: 1.993989  [28480/60000]\n",
      "Training loss: 2.018553  [28544/60000]\n",
      "Training loss: 2.033288  [28608/60000]\n",
      "Training loss: 2.031933  [28672/60000]\n",
      "Training loss: 2.044607  [28736/60000]\n",
      "Training loss: 2.003685  [28800/60000]\n",
      "Training loss: 2.028991  [28864/60000]\n",
      "Training loss: 2.074340  [28928/60000]\n",
      "Training loss: 2.040851  [28992/60000]\n",
      "Training loss: 2.001166  [29056/60000]\n",
      "Training loss: 2.003525  [29120/60000]\n",
      "Training loss: 2.025238  [29184/60000]\n",
      "Training loss: 2.052385  [29248/60000]\n",
      "Training loss: 2.002207  [29312/60000]\n",
      "Training loss: 2.055567  [29376/60000]\n",
      "Training loss: 2.009427  [29440/60000]\n",
      "Training loss: 1.986339  [29504/60000]\n",
      "Training loss: 1.952808  [29568/60000]\n",
      "Training loss: 2.006205  [29632/60000]\n",
      "Training loss: 2.011524  [29696/60000]\n",
      "Training loss: 2.030048  [29760/60000]\n",
      "Training loss: 2.065350  [29824/60000]\n",
      "Training loss: 1.979263  [29888/60000]\n",
      "Training loss: 2.048618  [29952/60000]\n",
      "Training loss: 1.985088  [30016/60000]\n",
      "Training loss: 2.062508  [30080/60000]\n",
      "Training loss: 2.004704  [30144/60000]\n",
      "Training loss: 2.032655  [30208/60000]\n",
      "Training loss: 1.993595  [30272/60000]\n",
      "Training loss: 2.019975  [30336/60000]\n",
      "Training loss: 1.973290  [30400/60000]\n",
      "Training loss: 1.959473  [30464/60000]\n",
      "Training loss: 2.017875  [30528/60000]\n",
      "Training loss: 2.016406  [30592/60000]\n",
      "Training loss: 1.989490  [30656/60000]\n",
      "Training loss: 2.040877  [30720/60000]\n",
      "Training loss: 2.046124  [30784/60000]\n",
      "Training loss: 2.034061  [30848/60000]\n",
      "Training loss: 2.015106  [30912/60000]\n",
      "Training loss: 1.979714  [30976/60000]\n",
      "Training loss: 1.986868  [31040/60000]\n",
      "Training loss: 1.990161  [31104/60000]\n",
      "Training loss: 1.989418  [31168/60000]\n",
      "Training loss: 2.032528  [31232/60000]\n",
      "Training loss: 2.030926  [31296/60000]\n",
      "Training loss: 2.028481  [31360/60000]\n",
      "Training loss: 2.005502  [31424/60000]\n",
      "Training loss: 2.024268  [31488/60000]\n",
      "Training loss: 2.005567  [31552/60000]\n",
      "Training loss: 2.008077  [31616/60000]\n",
      "Training loss: 2.004012  [31680/60000]\n",
      "Training loss: 1.984706  [31744/60000]\n",
      "Training loss: 1.949160  [31808/60000]\n",
      "Training loss: 2.010695  [31872/60000]\n",
      "Training loss: 2.019413  [31936/60000]\n",
      "Training loss: 2.045583  [32000/60000]\n",
      "Training loss: 2.010819  [32064/60000]\n",
      "Training loss: 2.006557  [32128/60000]\n",
      "Training loss: 2.036549  [32192/60000]\n",
      "Training loss: 2.006157  [32256/60000]\n",
      "Training loss: 2.008509  [32320/60000]\n",
      "Training loss: 1.981923  [32384/60000]\n",
      "Training loss: 2.017895  [32448/60000]\n",
      "Training loss: 2.024110  [32512/60000]\n",
      "Training loss: 1.997043  [32576/60000]\n",
      "Training loss: 1.969030  [32640/60000]\n",
      "Training loss: 2.010275  [32704/60000]\n",
      "Training loss: 2.003253  [32768/60000]\n",
      "Training loss: 2.051332  [32832/60000]\n",
      "Training loss: 2.019099  [32896/60000]\n",
      "Training loss: 1.991821  [32960/60000]\n",
      "Training loss: 2.013144  [33024/60000]\n",
      "Training loss: 1.972073  [33088/60000]\n",
      "Training loss: 2.030735  [33152/60000]\n",
      "Training loss: 1.972808  [33216/60000]\n",
      "Training loss: 2.001005  [33280/60000]\n",
      "Training loss: 1.998464  [33344/60000]\n",
      "Training loss: 2.044419  [33408/60000]\n",
      "Training loss: 1.979323  [33472/60000]\n",
      "Training loss: 1.968958  [33536/60000]\n",
      "Training loss: 1.985108  [33600/60000]\n",
      "Training loss: 1.999347  [33664/60000]\n",
      "Training loss: 1.993219  [33728/60000]\n",
      "Training loss: 1.943282  [33792/60000]\n",
      "Training loss: 1.992494  [33856/60000]\n",
      "Training loss: 2.028029  [33920/60000]\n",
      "Training loss: 2.009729  [33984/60000]\n",
      "Training loss: 1.966042  [34048/60000]\n",
      "Training loss: 1.985386  [34112/60000]\n",
      "Training loss: 1.986829  [34176/60000]\n",
      "Training loss: 1.952102  [34240/60000]\n",
      "Training loss: 1.983274  [34304/60000]\n",
      "Training loss: 1.999409  [34368/60000]\n",
      "Training loss: 1.958737  [34432/60000]\n",
      "Training loss: 2.018828  [34496/60000]\n",
      "Training loss: 2.012013  [34560/60000]\n",
      "Training loss: 2.010846  [34624/60000]\n",
      "Training loss: 1.958958  [34688/60000]\n",
      "Training loss: 1.990712  [34752/60000]\n",
      "Training loss: 1.983988  [34816/60000]\n",
      "Training loss: 2.017059  [34880/60000]\n",
      "Training loss: 1.980345  [34944/60000]\n",
      "Training loss: 2.000182  [35008/60000]\n",
      "Training loss: 1.982399  [35072/60000]\n",
      "Training loss: 2.003388  [35136/60000]\n",
      "Training loss: 2.051087  [35200/60000]\n",
      "Training loss: 1.951898  [35264/60000]\n",
      "Training loss: 1.966982  [35328/60000]\n",
      "Training loss: 1.980327  [35392/60000]\n",
      "Training loss: 1.977438  [35456/60000]\n",
      "Training loss: 2.002417  [35520/60000]\n",
      "Training loss: 2.015930  [35584/60000]\n",
      "Training loss: 2.011868  [35648/60000]\n",
      "Training loss: 2.035342  [35712/60000]\n",
      "Training loss: 1.990536  [35776/60000]\n",
      "Training loss: 2.017003  [35840/60000]\n",
      "Training loss: 2.035027  [35904/60000]\n",
      "Training loss: 2.000091  [35968/60000]\n",
      "Training loss: 1.997359  [36032/60000]\n",
      "Training loss: 2.007706  [36096/60000]\n",
      "Training loss: 1.998456  [36160/60000]\n",
      "Training loss: 2.013111  [36224/60000]\n",
      "Training loss: 2.068067  [36288/60000]\n",
      "Training loss: 1.977242  [36352/60000]\n",
      "Training loss: 2.004815  [36416/60000]\n",
      "Training loss: 1.970894  [36480/60000]\n",
      "Training loss: 1.981329  [36544/60000]\n",
      "Training loss: 1.984443  [36608/60000]\n",
      "Training loss: 1.940860  [36672/60000]\n",
      "Training loss: 1.984653  [36736/60000]\n",
      "Training loss: 1.994624  [36800/60000]\n",
      "Training loss: 1.959415  [36864/60000]\n",
      "Training loss: 1.962373  [36928/60000]\n",
      "Training loss: 1.990309  [36992/60000]\n",
      "Training loss: 1.968755  [37056/60000]\n",
      "Training loss: 1.991620  [37120/60000]\n",
      "Training loss: 2.000813  [37184/60000]\n",
      "Training loss: 2.005600  [37248/60000]\n",
      "Training loss: 2.039471  [37312/60000]\n",
      "Training loss: 1.982522  [37376/60000]\n",
      "Training loss: 1.896884  [37440/60000]\n",
      "Training loss: 1.980407  [37504/60000]\n",
      "Training loss: 1.976020  [37568/60000]\n",
      "Training loss: 1.999429  [37632/60000]\n",
      "Training loss: 1.954769  [37696/60000]\n",
      "Training loss: 1.965002  [37760/60000]\n",
      "Training loss: 1.984805  [37824/60000]\n",
      "Training loss: 1.981002  [37888/60000]\n",
      "Training loss: 1.986825  [37952/60000]\n",
      "Training loss: 1.960638  [38016/60000]\n",
      "Training loss: 1.923641  [38080/60000]\n",
      "Training loss: 2.002602  [38144/60000]\n",
      "Training loss: 1.984840  [38208/60000]\n",
      "Training loss: 1.933484  [38272/60000]\n",
      "Training loss: 2.045077  [38336/60000]\n",
      "Training loss: 1.933263  [38400/60000]\n",
      "Training loss: 2.008429  [38464/60000]\n",
      "Training loss: 1.977363  [38528/60000]\n",
      "Training loss: 1.960811  [38592/60000]\n",
      "Training loss: 2.013841  [38656/60000]\n",
      "Training loss: 1.998130  [38720/60000]\n",
      "Training loss: 2.020404  [38784/60000]\n",
      "Training loss: 1.958100  [38848/60000]\n",
      "Training loss: 1.969269  [38912/60000]\n",
      "Training loss: 1.939361  [38976/60000]\n",
      "Training loss: 1.997678  [39040/60000]\n",
      "Training loss: 1.969935  [39104/60000]\n",
      "Training loss: 1.961483  [39168/60000]\n",
      "Training loss: 1.961526  [39232/60000]\n",
      "Training loss: 2.009872  [39296/60000]\n",
      "Training loss: 1.972026  [39360/60000]\n",
      "Training loss: 1.943605  [39424/60000]\n",
      "Training loss: 1.993202  [39488/60000]\n",
      "Training loss: 2.057746  [39552/60000]\n",
      "Training loss: 1.975933  [39616/60000]\n",
      "Training loss: 1.988788  [39680/60000]\n",
      "Training loss: 1.976401  [39744/60000]\n",
      "Training loss: 1.944228  [39808/60000]\n",
      "Training loss: 2.010641  [39872/60000]\n",
      "Training loss: 2.011132  [39936/60000]\n",
      "Training loss: 2.008695  [40000/60000]\n",
      "Training loss: 1.988841  [40064/60000]\n",
      "Training loss: 2.019625  [40128/60000]\n",
      "Training loss: 1.999664  [40192/60000]\n",
      "Training loss: 1.961880  [40256/60000]\n",
      "Training loss: 1.959437  [40320/60000]\n",
      "Training loss: 1.993146  [40384/60000]\n",
      "Training loss: 1.975240  [40448/60000]\n",
      "Training loss: 1.997342  [40512/60000]\n",
      "Training loss: 1.990599  [40576/60000]\n",
      "Training loss: 1.994030  [40640/60000]\n",
      "Training loss: 1.987252  [40704/60000]\n",
      "Training loss: 1.998440  [40768/60000]\n",
      "Training loss: 1.984922  [40832/60000]\n",
      "Training loss: 1.974891  [40896/60000]\n",
      "Training loss: 1.982372  [40960/60000]\n",
      "Training loss: 1.977747  [41024/60000]\n",
      "Training loss: 2.022066  [41088/60000]\n",
      "Training loss: 1.957109  [41152/60000]\n",
      "Training loss: 1.973108  [41216/60000]\n",
      "Training loss: 1.950441  [41280/60000]\n",
      "Training loss: 1.908605  [41344/60000]\n",
      "Training loss: 1.917983  [41408/60000]\n",
      "Training loss: 1.979122  [41472/60000]\n",
      "Training loss: 1.943908  [41536/60000]\n",
      "Training loss: 1.959074  [41600/60000]\n",
      "Training loss: 2.001245  [41664/60000]\n",
      "Training loss: 1.988216  [41728/60000]\n",
      "Training loss: 1.967360  [41792/60000]\n",
      "Training loss: 1.993330  [41856/60000]\n",
      "Training loss: 1.966020  [41920/60000]\n",
      "Training loss: 1.989966  [41984/60000]\n",
      "Training loss: 1.935655  [42048/60000]\n",
      "Training loss: 1.957693  [42112/60000]\n",
      "Training loss: 1.917778  [42176/60000]\n",
      "Training loss: 1.971916  [42240/60000]\n",
      "Training loss: 2.003672  [42304/60000]\n",
      "Training loss: 1.970175  [42368/60000]\n",
      "Training loss: 1.978376  [42432/60000]\n",
      "Training loss: 1.905980  [42496/60000]\n",
      "Training loss: 1.996360  [42560/60000]\n",
      "Training loss: 1.925928  [42624/60000]\n",
      "Training loss: 2.010832  [42688/60000]\n",
      "Training loss: 1.999188  [42752/60000]\n",
      "Training loss: 1.956518  [42816/60000]\n",
      "Training loss: 1.981209  [42880/60000]\n",
      "Training loss: 2.018008  [42944/60000]\n",
      "Training loss: 1.998538  [43008/60000]\n",
      "Training loss: 1.970402  [43072/60000]\n",
      "Training loss: 1.942393  [43136/60000]\n",
      "Training loss: 1.971997  [43200/60000]\n",
      "Training loss: 2.010646  [43264/60000]\n",
      "Training loss: 1.977869  [43328/60000]\n",
      "Training loss: 1.997728  [43392/60000]\n",
      "Training loss: 1.969974  [43456/60000]\n",
      "Training loss: 1.966992  [43520/60000]\n",
      "Training loss: 1.992363  [43584/60000]\n",
      "Training loss: 1.999179  [43648/60000]\n",
      "Training loss: 1.935854  [43712/60000]\n",
      "Training loss: 1.941107  [43776/60000]\n",
      "Training loss: 1.968242  [43840/60000]\n",
      "Training loss: 1.958072  [43904/60000]\n",
      "Training loss: 1.993232  [43968/60000]\n",
      "Training loss: 1.963865  [44032/60000]\n",
      "Training loss: 1.995843  [44096/60000]\n",
      "Training loss: 1.975761  [44160/60000]\n",
      "Training loss: 2.040531  [44224/60000]\n",
      "Training loss: 1.968237  [44288/60000]\n",
      "Training loss: 1.987687  [44352/60000]\n",
      "Training loss: 1.961767  [44416/60000]\n",
      "Training loss: 1.986222  [44480/60000]\n",
      "Training loss: 1.988326  [44544/60000]\n",
      "Training loss: 1.938566  [44608/60000]\n",
      "Training loss: 1.917370  [44672/60000]\n",
      "Training loss: 1.904500  [44736/60000]\n",
      "Training loss: 1.970749  [44800/60000]\n",
      "Training loss: 1.953653  [44864/60000]\n",
      "Training loss: 1.981903  [44928/60000]\n",
      "Training loss: 1.974008  [44992/60000]\n",
      "Training loss: 1.927422  [45056/60000]\n",
      "Training loss: 1.960334  [45120/60000]\n",
      "Training loss: 1.985832  [45184/60000]\n",
      "Training loss: 1.960221  [45248/60000]\n",
      "Training loss: 1.923156  [45312/60000]\n",
      "Training loss: 1.974747  [45376/60000]\n",
      "Training loss: 2.006469  [45440/60000]\n",
      "Training loss: 1.968412  [45504/60000]\n",
      "Training loss: 1.910601  [45568/60000]\n",
      "Training loss: 1.966476  [45632/60000]\n",
      "Training loss: 1.952880  [45696/60000]\n",
      "Training loss: 1.998657  [45760/60000]\n",
      "Training loss: 1.955882  [45824/60000]\n",
      "Training loss: 1.977253  [45888/60000]\n",
      "Training loss: 1.922426  [45952/60000]\n",
      "Training loss: 1.949233  [46016/60000]\n",
      "Training loss: 1.956788  [46080/60000]\n",
      "Training loss: 1.954838  [46144/60000]\n",
      "Training loss: 1.955698  [46208/60000]\n",
      "Training loss: 1.948794  [46272/60000]\n",
      "Training loss: 1.957574  [46336/60000]\n",
      "Training loss: 1.997330  [46400/60000]\n",
      "Training loss: 1.996946  [46464/60000]\n",
      "Training loss: 1.935402  [46528/60000]\n",
      "Training loss: 1.915092  [46592/60000]\n",
      "Training loss: 1.965628  [46656/60000]\n",
      "Training loss: 1.953589  [46720/60000]\n",
      "Training loss: 1.988505  [46784/60000]\n",
      "Training loss: 1.891389  [46848/60000]\n",
      "Training loss: 1.959131  [46912/60000]\n",
      "Training loss: 1.990554  [46976/60000]\n",
      "Training loss: 1.994405  [47040/60000]\n",
      "Training loss: 1.913676  [47104/60000]\n",
      "Training loss: 2.002246  [47168/60000]\n",
      "Training loss: 1.966079  [47232/60000]\n",
      "Training loss: 1.999097  [47296/60000]\n",
      "Training loss: 1.912751  [47360/60000]\n",
      "Training loss: 1.974452  [47424/60000]\n",
      "Training loss: 1.971966  [47488/60000]\n",
      "Training loss: 1.952892  [47552/60000]\n",
      "Training loss: 1.981054  [47616/60000]\n",
      "Training loss: 1.934867  [47680/60000]\n",
      "Training loss: 1.925856  [47744/60000]\n",
      "Training loss: 1.954220  [47808/60000]\n",
      "Training loss: 1.960092  [47872/60000]\n",
      "Training loss: 1.985770  [47936/60000]\n",
      "Training loss: 2.014059  [48000/60000]\n",
      "Training loss: 1.979232  [48064/60000]\n",
      "Training loss: 1.877164  [48128/60000]\n",
      "Training loss: 1.960402  [48192/60000]\n",
      "Training loss: 2.020171  [48256/60000]\n",
      "Training loss: 1.937630  [48320/60000]\n",
      "Training loss: 1.959624  [48384/60000]\n",
      "Training loss: 1.990170  [48448/60000]\n",
      "Training loss: 1.964593  [48512/60000]\n",
      "Training loss: 1.946779  [48576/60000]\n",
      "Training loss: 1.957112  [48640/60000]\n",
      "Training loss: 1.948736  [48704/60000]\n",
      "Training loss: 1.978736  [48768/60000]\n",
      "Training loss: 1.979453  [48832/60000]\n",
      "Training loss: 1.922941  [48896/60000]\n",
      "Training loss: 1.954696  [48960/60000]\n",
      "Training loss: 1.959126  [49024/60000]\n",
      "Training loss: 1.963221  [49088/60000]\n",
      "Training loss: 1.954692  [49152/60000]\n",
      "Training loss: 1.957527  [49216/60000]\n",
      "Training loss: 1.918157  [49280/60000]\n",
      "Training loss: 1.949401  [49344/60000]\n",
      "Training loss: 1.983344  [49408/60000]\n",
      "Training loss: 1.893869  [49472/60000]\n",
      "Training loss: 1.901063  [49536/60000]\n",
      "Training loss: 2.013778  [49600/60000]\n",
      "Training loss: 1.955723  [49664/60000]\n",
      "Training loss: 1.917265  [49728/60000]\n",
      "Training loss: 1.945669  [49792/60000]\n",
      "Training loss: 1.927234  [49856/60000]\n",
      "Training loss: 1.953343  [49920/60000]\n",
      "Training loss: 1.948140  [49984/60000]\n",
      "Training loss: 1.930014  [50048/60000]\n",
      "Training loss: 1.979691  [50112/60000]\n",
      "Training loss: 1.896082  [50176/60000]\n",
      "Training loss: 1.919533  [50240/60000]\n",
      "Training loss: 1.911252  [50304/60000]\n",
      "Training loss: 1.944790  [50368/60000]\n",
      "Training loss: 1.923020  [50432/60000]\n",
      "Training loss: 1.938189  [50496/60000]\n",
      "Training loss: 1.940534  [50560/60000]\n",
      "Training loss: 1.927820  [50624/60000]\n",
      "Training loss: 1.986505  [50688/60000]\n",
      "Training loss: 1.931602  [50752/60000]\n",
      "Training loss: 1.958625  [50816/60000]\n",
      "Training loss: 1.908834  [50880/60000]\n",
      "Training loss: 1.991431  [50944/60000]\n",
      "Training loss: 1.898289  [51008/60000]\n",
      "Training loss: 1.906277  [51072/60000]\n",
      "Training loss: 1.892541  [51136/60000]\n",
      "Training loss: 1.935374  [51200/60000]\n",
      "Training loss: 1.952137  [51264/60000]\n",
      "Training loss: 1.934246  [51328/60000]\n",
      "Training loss: 1.927073  [51392/60000]\n",
      "Training loss: 1.999275  [51456/60000]\n",
      "Training loss: 1.929623  [51520/60000]\n",
      "Training loss: 1.934973  [51584/60000]\n",
      "Training loss: 1.961088  [51648/60000]\n",
      "Training loss: 1.921240  [51712/60000]\n",
      "Training loss: 1.904620  [51776/60000]\n",
      "Training loss: 1.937074  [51840/60000]\n",
      "Training loss: 1.958953  [51904/60000]\n",
      "Training loss: 1.888148  [51968/60000]\n",
      "Training loss: 1.951741  [52032/60000]\n",
      "Training loss: 1.998818  [52096/60000]\n",
      "Training loss: 1.908912  [52160/60000]\n",
      "Training loss: 1.923713  [52224/60000]\n",
      "Training loss: 1.963268  [52288/60000]\n",
      "Training loss: 1.916819  [52352/60000]\n",
      "Training loss: 1.916507  [52416/60000]\n",
      "Training loss: 1.861704  [52480/60000]\n",
      "Training loss: 1.958703  [52544/60000]\n",
      "Training loss: 2.007884  [52608/60000]\n",
      "Training loss: 2.002615  [52672/60000]\n",
      "Training loss: 1.872151  [52736/60000]\n",
      "Training loss: 1.926197  [52800/60000]\n",
      "Training loss: 1.926297  [52864/60000]\n",
      "Training loss: 1.985963  [52928/60000]\n",
      "Training loss: 1.895367  [52992/60000]\n",
      "Training loss: 1.941942  [53056/60000]\n",
      "Training loss: 1.935839  [53120/60000]\n",
      "Training loss: 1.973108  [53184/60000]\n",
      "Training loss: 1.977334  [53248/60000]\n",
      "Training loss: 1.942986  [53312/60000]\n",
      "Training loss: 1.938010  [53376/60000]\n",
      "Training loss: 1.934266  [53440/60000]\n",
      "Training loss: 1.925482  [53504/60000]\n",
      "Training loss: 1.935196  [53568/60000]\n",
      "Training loss: 1.883086  [53632/60000]\n",
      "Training loss: 1.944399  [53696/60000]\n",
      "Training loss: 1.964250  [53760/60000]\n",
      "Training loss: 1.945162  [53824/60000]\n",
      "Training loss: 2.010780  [53888/60000]\n",
      "Training loss: 1.914398  [53952/60000]\n",
      "Training loss: 1.906940  [54016/60000]\n",
      "Training loss: 1.924719  [54080/60000]\n",
      "Training loss: 1.960934  [54144/60000]\n",
      "Training loss: 1.896886  [54208/60000]\n",
      "Training loss: 1.842248  [54272/60000]\n",
      "Training loss: 1.922352  [54336/60000]\n",
      "Training loss: 1.918254  [54400/60000]\n",
      "Training loss: 1.936417  [54464/60000]\n",
      "Training loss: 1.912619  [54528/60000]\n",
      "Training loss: 1.891923  [54592/60000]\n",
      "Training loss: 1.970158  [54656/60000]\n",
      "Training loss: 1.892578  [54720/60000]\n",
      "Training loss: 1.918885  [54784/60000]\n",
      "Training loss: 1.929229  [54848/60000]\n",
      "Training loss: 1.942763  [54912/60000]\n",
      "Training loss: 1.914218  [54976/60000]\n",
      "Training loss: 1.895668  [55040/60000]\n",
      "Training loss: 1.934495  [55104/60000]\n",
      "Training loss: 1.927257  [55168/60000]\n",
      "Training loss: 1.936460  [55232/60000]\n",
      "Training loss: 1.921467  [55296/60000]\n",
      "Training loss: 1.928366  [55360/60000]\n",
      "Training loss: 1.983602  [55424/60000]\n",
      "Training loss: 1.945040  [55488/60000]\n",
      "Training loss: 1.966169  [55552/60000]\n",
      "Training loss: 1.911630  [55616/60000]\n",
      "Training loss: 1.933556  [55680/60000]\n",
      "Training loss: 1.945282  [55744/60000]\n",
      "Training loss: 1.880564  [55808/60000]\n",
      "Training loss: 1.903619  [55872/60000]\n",
      "Training loss: 1.879862  [55936/60000]\n",
      "Training loss: 1.917049  [56000/60000]\n",
      "Training loss: 1.923571  [56064/60000]\n",
      "Training loss: 1.934990  [56128/60000]\n",
      "Training loss: 1.880711  [56192/60000]\n",
      "Training loss: 1.966583  [56256/60000]\n",
      "Training loss: 1.955389  [56320/60000]\n",
      "Training loss: 1.878953  [56384/60000]\n",
      "Training loss: 1.937860  [56448/60000]\n",
      "Training loss: 1.903551  [56512/60000]\n",
      "Training loss: 1.921456  [56576/60000]\n",
      "Training loss: 1.969062  [56640/60000]\n",
      "Training loss: 1.927052  [56704/60000]\n",
      "Training loss: 1.880500  [56768/60000]\n",
      "Training loss: 1.942092  [56832/60000]\n",
      "Training loss: 1.958488  [56896/60000]\n",
      "Training loss: 1.906405  [56960/60000]\n",
      "Training loss: 1.896059  [57024/60000]\n",
      "Training loss: 1.925470  [57088/60000]\n",
      "Training loss: 1.892353  [57152/60000]\n",
      "Training loss: 1.977816  [57216/60000]\n",
      "Training loss: 1.902026  [57280/60000]\n",
      "Training loss: 1.976296  [57344/60000]\n",
      "Training loss: 1.866754  [57408/60000]\n",
      "Training loss: 1.933482  [57472/60000]\n",
      "Training loss: 1.930443  [57536/60000]\n",
      "Training loss: 1.961322  [57600/60000]\n",
      "Training loss: 1.868970  [57664/60000]\n",
      "Training loss: 1.883280  [57728/60000]\n",
      "Training loss: 1.934865  [57792/60000]\n",
      "Training loss: 1.930077  [57856/60000]\n",
      "Training loss: 1.909921  [57920/60000]\n",
      "Training loss: 1.918597  [57984/60000]\n",
      "Training loss: 1.916191  [58048/60000]\n",
      "Training loss: 1.940858  [58112/60000]\n",
      "Training loss: 1.879810  [58176/60000]\n",
      "Training loss: 1.951366  [58240/60000]\n",
      "Training loss: 1.928830  [58304/60000]\n",
      "Training loss: 1.915274  [58368/60000]\n",
      "Training loss: 1.883248  [58432/60000]\n",
      "Training loss: 1.908328  [58496/60000]\n",
      "Training loss: 1.876101  [58560/60000]\n",
      "Training loss: 1.902158  [58624/60000]\n",
      "Training loss: 1.911470  [58688/60000]\n",
      "Training loss: 1.883923  [58752/60000]\n",
      "Training loss: 1.880573  [58816/60000]\n",
      "Training loss: 1.816267  [58880/60000]\n",
      "Training loss: 1.861219  [58944/60000]\n",
      "Training loss: 1.944684  [59008/60000]\n",
      "Training loss: 1.940908  [59072/60000]\n",
      "Training loss: 2.005756  [59136/60000]\n",
      "Training loss: 1.940659  [59200/60000]\n",
      "Training loss: 1.889641  [59264/60000]\n",
      "Training loss: 1.927535  [59328/60000]\n",
      "Training loss: 1.887961  [59392/60000]\n",
      "Training loss: 1.881602  [59456/60000]\n",
      "Training loss: 1.974819  [59520/60000]\n",
      "Training loss: 1.872560  [59584/60000]\n",
      "Training loss: 1.870914  [59648/60000]\n",
      "Training loss: 1.915703  [59712/60000]\n",
      "Training loss: 1.891244  [59776/60000]\n",
      "Training loss: 1.915378  [59840/60000]\n",
      "Training loss: 1.909698  [59904/60000]\n",
      "Training loss: 1.926370  [29984/60000]\n",
      "\n",
      " Finished Epoch:  3\n",
      "--------------------------------------------\n",
      "Training loss: 1.897018  [   64/60000]\n",
      "Training loss: 1.836569  [  128/60000]\n",
      "Training loss: 1.908304  [  192/60000]\n",
      "Training loss: 1.895258  [  256/60000]\n",
      "Training loss: 1.954533  [  320/60000]\n",
      "Training loss: 1.853934  [  384/60000]\n",
      "Training loss: 1.985575  [  448/60000]\n",
      "Training loss: 1.908540  [  512/60000]\n",
      "Training loss: 1.893861  [  576/60000]\n",
      "Training loss: 1.913791  [  640/60000]\n",
      "Training loss: 1.921509  [  704/60000]\n",
      "Training loss: 1.851893  [  768/60000]\n",
      "Training loss: 1.925301  [  832/60000]\n",
      "Training loss: 1.882457  [  896/60000]\n",
      "Training loss: 1.881992  [  960/60000]\n",
      "Training loss: 1.882363  [ 1024/60000]\n",
      "Training loss: 1.899004  [ 1088/60000]\n",
      "Training loss: 1.905873  [ 1152/60000]\n",
      "Training loss: 1.903517  [ 1216/60000]\n",
      "Training loss: 1.914694  [ 1280/60000]\n",
      "Training loss: 1.938533  [ 1344/60000]\n",
      "Training loss: 1.873414  [ 1408/60000]\n",
      "Training loss: 1.899874  [ 1472/60000]\n",
      "Training loss: 1.876099  [ 1536/60000]\n",
      "Training loss: 1.874222  [ 1600/60000]\n",
      "Training loss: 1.916198  [ 1664/60000]\n",
      "Training loss: 1.873762  [ 1728/60000]\n",
      "Training loss: 1.808061  [ 1792/60000]\n",
      "Training loss: 1.904904  [ 1856/60000]\n",
      "Training loss: 1.839622  [ 1920/60000]\n",
      "Training loss: 1.844459  [ 1984/60000]\n",
      "Training loss: 1.898405  [ 2048/60000]\n",
      "Training loss: 1.840520  [ 2112/60000]\n",
      "Training loss: 1.851894  [ 2176/60000]\n",
      "Training loss: 1.885832  [ 2240/60000]\n",
      "Training loss: 1.929900  [ 2304/60000]\n",
      "Training loss: 1.814419  [ 2368/60000]\n",
      "Training loss: 1.894919  [ 2432/60000]\n",
      "Training loss: 1.927630  [ 2496/60000]\n",
      "Training loss: 1.882682  [ 2560/60000]\n",
      "Training loss: 1.899894  [ 2624/60000]\n",
      "Training loss: 1.841480  [ 2688/60000]\n",
      "Training loss: 1.867161  [ 2752/60000]\n",
      "Training loss: 1.893473  [ 2816/60000]\n",
      "Training loss: 1.919026  [ 2880/60000]\n",
      "Training loss: 1.885009  [ 2944/60000]\n",
      "Training loss: 1.831645  [ 3008/60000]\n",
      "Training loss: 1.942330  [ 3072/60000]\n",
      "Training loss: 1.928914  [ 3136/60000]\n",
      "Training loss: 1.882951  [ 3200/60000]\n",
      "Training loss: 1.843521  [ 3264/60000]\n",
      "Training loss: 1.904662  [ 3328/60000]\n",
      "Training loss: 1.846844  [ 3392/60000]\n",
      "Training loss: 1.867345  [ 3456/60000]\n",
      "Training loss: 1.938395  [ 3520/60000]\n",
      "Training loss: 1.963304  [ 3584/60000]\n",
      "Training loss: 1.911349  [ 3648/60000]\n",
      "Training loss: 1.886384  [ 3712/60000]\n",
      "Training loss: 1.901003  [ 3776/60000]\n",
      "Training loss: 1.786479  [ 3840/60000]\n",
      "Training loss: 1.886659  [ 3904/60000]\n",
      "Training loss: 1.859741  [ 3968/60000]\n",
      "Training loss: 1.855141  [ 4032/60000]\n",
      "Training loss: 1.874603  [ 4096/60000]\n",
      "Training loss: 1.871936  [ 4160/60000]\n",
      "Training loss: 1.904676  [ 4224/60000]\n",
      "Training loss: 1.910698  [ 4288/60000]\n",
      "Training loss: 1.864587  [ 4352/60000]\n",
      "Training loss: 1.882505  [ 4416/60000]\n",
      "Training loss: 1.870322  [ 4480/60000]\n",
      "Training loss: 1.892401  [ 4544/60000]\n",
      "Training loss: 1.887299  [ 4608/60000]\n",
      "Training loss: 1.857289  [ 4672/60000]\n",
      "Training loss: 1.942671  [ 4736/60000]\n",
      "Training loss: 1.878894  [ 4800/60000]\n",
      "Training loss: 1.927340  [ 4864/60000]\n",
      "Training loss: 1.879600  [ 4928/60000]\n",
      "Training loss: 1.928572  [ 4992/60000]\n",
      "Training loss: 1.863579  [ 5056/60000]\n",
      "Training loss: 1.933601  [ 5120/60000]\n",
      "Training loss: 1.867853  [ 5184/60000]\n",
      "Training loss: 1.926995  [ 5248/60000]\n",
      "Training loss: 1.853699  [ 5312/60000]\n",
      "Training loss: 1.898628  [ 5376/60000]\n",
      "Training loss: 1.909445  [ 5440/60000]\n",
      "Training loss: 1.898302  [ 5504/60000]\n",
      "Training loss: 1.901536  [ 5568/60000]\n",
      "Training loss: 1.883311  [ 5632/60000]\n",
      "Training loss: 1.922054  [ 5696/60000]\n",
      "Training loss: 1.854263  [ 5760/60000]\n",
      "Training loss: 1.961586  [ 5824/60000]\n",
      "Training loss: 1.891093  [ 5888/60000]\n",
      "Training loss: 1.865965  [ 5952/60000]\n",
      "Training loss: 1.885125  [ 6016/60000]\n",
      "Training loss: 1.851704  [ 6080/60000]\n",
      "Training loss: 1.844468  [ 6144/60000]\n",
      "Training loss: 1.917487  [ 6208/60000]\n",
      "Training loss: 1.837448  [ 6272/60000]\n",
      "Training loss: 1.921254  [ 6336/60000]\n",
      "Training loss: 1.778291  [ 6400/60000]\n",
      "Training loss: 1.935260  [ 6464/60000]\n",
      "Training loss: 1.895378  [ 6528/60000]\n",
      "Training loss: 1.901598  [ 6592/60000]\n",
      "Training loss: 1.829466  [ 6656/60000]\n",
      "Training loss: 1.801587  [ 6720/60000]\n",
      "Training loss: 1.958863  [ 6784/60000]\n",
      "Training loss: 1.892601  [ 6848/60000]\n",
      "Training loss: 1.908049  [ 6912/60000]\n",
      "Training loss: 1.911600  [ 6976/60000]\n",
      "Training loss: 1.840961  [ 7040/60000]\n",
      "Training loss: 1.847405  [ 7104/60000]\n",
      "Training loss: 1.835654  [ 7168/60000]\n",
      "Training loss: 1.837790  [ 7232/60000]\n",
      "Training loss: 1.932372  [ 7296/60000]\n",
      "Training loss: 1.897507  [ 7360/60000]\n",
      "Training loss: 1.830877  [ 7424/60000]\n",
      "Training loss: 1.905040  [ 7488/60000]\n",
      "Training loss: 1.802565  [ 7552/60000]\n",
      "Training loss: 1.851774  [ 7616/60000]\n",
      "Training loss: 1.866933  [ 7680/60000]\n",
      "Training loss: 1.873110  [ 7744/60000]\n",
      "Training loss: 1.888956  [ 7808/60000]\n",
      "Training loss: 1.821327  [ 7872/60000]\n",
      "Training loss: 1.932885  [ 7936/60000]\n",
      "Training loss: 1.903556  [ 8000/60000]\n",
      "Training loss: 1.861413  [ 8064/60000]\n",
      "Training loss: 1.845341  [ 8128/60000]\n",
      "Training loss: 1.943068  [ 8192/60000]\n",
      "Training loss: 1.887299  [ 8256/60000]\n",
      "Training loss: 1.849510  [ 8320/60000]\n",
      "Training loss: 1.894297  [ 8384/60000]\n",
      "Training loss: 1.908492  [ 8448/60000]\n",
      "Training loss: 1.900651  [ 8512/60000]\n",
      "Training loss: 1.881284  [ 8576/60000]\n",
      "Training loss: 1.944312  [ 8640/60000]\n",
      "Training loss: 1.841774  [ 8704/60000]\n",
      "Training loss: 1.914977  [ 8768/60000]\n",
      "Training loss: 1.862609  [ 8832/60000]\n",
      "Training loss: 1.862230  [ 8896/60000]\n",
      "Training loss: 1.843817  [ 8960/60000]\n",
      "Training loss: 1.900219  [ 9024/60000]\n",
      "Training loss: 1.868387  [ 9088/60000]\n",
      "Training loss: 1.854769  [ 9152/60000]\n",
      "Training loss: 1.848481  [ 9216/60000]\n",
      "Training loss: 1.880313  [ 9280/60000]\n",
      "Training loss: 1.800695  [ 9344/60000]\n",
      "Training loss: 1.782107  [ 9408/60000]\n",
      "Training loss: 1.814416  [ 9472/60000]\n",
      "Training loss: 1.893586  [ 9536/60000]\n",
      "Training loss: 1.870649  [ 9600/60000]\n",
      "Training loss: 1.829266  [ 9664/60000]\n",
      "Training loss: 1.852028  [ 9728/60000]\n",
      "Training loss: 1.888093  [ 9792/60000]\n",
      "Training loss: 1.883065  [ 9856/60000]\n",
      "Training loss: 1.800867  [ 9920/60000]\n",
      "Training loss: 1.846967  [ 9984/60000]\n",
      "Training loss: 1.812143  [10048/60000]\n",
      "Training loss: 1.890388  [10112/60000]\n",
      "Training loss: 1.884927  [10176/60000]\n",
      "Training loss: 1.854789  [10240/60000]\n",
      "Training loss: 1.878991  [10304/60000]\n",
      "Training loss: 1.801464  [10368/60000]\n",
      "Training loss: 1.959217  [10432/60000]\n",
      "Training loss: 1.864173  [10496/60000]\n",
      "Training loss: 1.877813  [10560/60000]\n",
      "Training loss: 1.817530  [10624/60000]\n",
      "Training loss: 1.876451  [10688/60000]\n",
      "Training loss: 1.777739  [10752/60000]\n",
      "Training loss: 1.861619  [10816/60000]\n",
      "Training loss: 1.887812  [10880/60000]\n",
      "Training loss: 1.887225  [10944/60000]\n",
      "Training loss: 1.903611  [11008/60000]\n",
      "Training loss: 1.824285  [11072/60000]\n",
      "Training loss: 1.900107  [11136/60000]\n",
      "Training loss: 1.823126  [11200/60000]\n",
      "Training loss: 1.868124  [11264/60000]\n",
      "Training loss: 1.834490  [11328/60000]\n",
      "Training loss: 1.897144  [11392/60000]\n",
      "Training loss: 1.924126  [11456/60000]\n",
      "Training loss: 1.865044  [11520/60000]\n",
      "Training loss: 1.869171  [11584/60000]\n",
      "Training loss: 1.870701  [11648/60000]\n",
      "Training loss: 1.831329  [11712/60000]\n",
      "Training loss: 1.855735  [11776/60000]\n",
      "Training loss: 1.881905  [11840/60000]\n",
      "Training loss: 1.907795  [11904/60000]\n",
      "Training loss: 1.917845  [11968/60000]\n",
      "Training loss: 1.879645  [12032/60000]\n",
      "Training loss: 1.840288  [12096/60000]\n",
      "Training loss: 1.880235  [12160/60000]\n",
      "Training loss: 1.824139  [12224/60000]\n",
      "Training loss: 1.892813  [12288/60000]\n",
      "Training loss: 1.821766  [12352/60000]\n",
      "Training loss: 1.803908  [12416/60000]\n",
      "Training loss: 1.827389  [12480/60000]\n",
      "Training loss: 1.843621  [12544/60000]\n",
      "Training loss: 1.775386  [12608/60000]\n",
      "Training loss: 1.818735  [12672/60000]\n",
      "Training loss: 1.889059  [12736/60000]\n",
      "Training loss: 1.811356  [12800/60000]\n",
      "Training loss: 1.869614  [12864/60000]\n",
      "Training loss: 1.794205  [12928/60000]\n",
      "Training loss: 1.851368  [12992/60000]\n",
      "Training loss: 1.869226  [13056/60000]\n",
      "Training loss: 1.855941  [13120/60000]\n",
      "Training loss: 1.841500  [13184/60000]\n",
      "Training loss: 1.854356  [13248/60000]\n",
      "Training loss: 1.861039  [13312/60000]\n",
      "Training loss: 1.829018  [13376/60000]\n",
      "Training loss: 1.901429  [13440/60000]\n",
      "Training loss: 1.807238  [13504/60000]\n",
      "Training loss: 1.868885  [13568/60000]\n",
      "Training loss: 1.888369  [13632/60000]\n",
      "Training loss: 1.841513  [13696/60000]\n",
      "Training loss: 1.857470  [13760/60000]\n",
      "Training loss: 1.823680  [13824/60000]\n",
      "Training loss: 1.803161  [13888/60000]\n",
      "Training loss: 1.865395  [13952/60000]\n",
      "Training loss: 1.843546  [14016/60000]\n",
      "Training loss: 1.872870  [14080/60000]\n",
      "Training loss: 1.723423  [14144/60000]\n",
      "Training loss: 1.766164  [14208/60000]\n",
      "Training loss: 1.871073  [14272/60000]\n",
      "Training loss: 1.868532  [14336/60000]\n",
      "Training loss: 1.808410  [14400/60000]\n",
      "Training loss: 1.828873  [14464/60000]\n",
      "Training loss: 1.865568  [14528/60000]\n",
      "Training loss: 1.849087  [14592/60000]\n",
      "Training loss: 1.797170  [14656/60000]\n",
      "Training loss: 1.845979  [14720/60000]\n",
      "Training loss: 1.793867  [14784/60000]\n",
      "Training loss: 1.768607  [14848/60000]\n",
      "Training loss: 1.865815  [14912/60000]\n",
      "Training loss: 1.832356  [14976/60000]\n",
      "Training loss: 1.748537  [15040/60000]\n",
      "Training loss: 1.909285  [15104/60000]\n",
      "Training loss: 1.810698  [15168/60000]\n",
      "Training loss: 1.800897  [15232/60000]\n",
      "Training loss: 1.878523  [15296/60000]\n",
      "Training loss: 1.831589  [15360/60000]\n",
      "Training loss: 1.763302  [15424/60000]\n",
      "Training loss: 1.868860  [15488/60000]\n",
      "Training loss: 1.818119  [15552/60000]\n",
      "Training loss: 1.826157  [15616/60000]\n",
      "Training loss: 1.744281  [15680/60000]\n",
      "Training loss: 1.821489  [15744/60000]\n",
      "Training loss: 1.932777  [15808/60000]\n",
      "Training loss: 1.863762  [15872/60000]\n",
      "Training loss: 1.820983  [15936/60000]\n",
      "Training loss: 1.882233  [16000/60000]\n",
      "Training loss: 1.864652  [16064/60000]\n",
      "Training loss: 1.883091  [16128/60000]\n",
      "Training loss: 1.787461  [16192/60000]\n",
      "Training loss: 1.896479  [16256/60000]\n",
      "Training loss: 1.766218  [16320/60000]\n",
      "Training loss: 1.823165  [16384/60000]\n",
      "Training loss: 1.868685  [16448/60000]\n",
      "Training loss: 1.858445  [16512/60000]\n",
      "Training loss: 1.886005  [16576/60000]\n",
      "Training loss: 1.789128  [16640/60000]\n",
      "Training loss: 1.854526  [16704/60000]\n",
      "Training loss: 1.818200  [16768/60000]\n",
      "Training loss: 1.832654  [16832/60000]\n",
      "Training loss: 1.870578  [16896/60000]\n",
      "Training loss: 1.851336  [16960/60000]\n",
      "Training loss: 1.873829  [17024/60000]\n",
      "Training loss: 1.845957  [17088/60000]\n",
      "Training loss: 1.791059  [17152/60000]\n",
      "Training loss: 1.836736  [17216/60000]\n",
      "Training loss: 1.834530  [17280/60000]\n",
      "Training loss: 1.748261  [17344/60000]\n",
      "Training loss: 1.846796  [17408/60000]\n",
      "Training loss: 1.802281  [17472/60000]\n",
      "Training loss: 1.831963  [17536/60000]\n",
      "Training loss: 1.798534  [17600/60000]\n",
      "Training loss: 1.791294  [17664/60000]\n",
      "Training loss: 1.817979  [17728/60000]\n",
      "Training loss: 1.865536  [17792/60000]\n",
      "Training loss: 1.827805  [17856/60000]\n",
      "Training loss: 1.853090  [17920/60000]\n",
      "Training loss: 1.844917  [17984/60000]\n",
      "Training loss: 1.810224  [18048/60000]\n",
      "Training loss: 1.863611  [18112/60000]\n",
      "Training loss: 1.785541  [18176/60000]\n",
      "Training loss: 1.830352  [18240/60000]\n",
      "Training loss: 1.831083  [18304/60000]\n",
      "Training loss: 1.803632  [18368/60000]\n",
      "Training loss: 1.827098  [18432/60000]\n",
      "Training loss: 1.843704  [18496/60000]\n",
      "Training loss: 1.846026  [18560/60000]\n",
      "Training loss: 1.804916  [18624/60000]\n",
      "Training loss: 1.803433  [18688/60000]\n",
      "Training loss: 1.884048  [18752/60000]\n",
      "Training loss: 1.831831  [18816/60000]\n",
      "Training loss: 1.773469  [18880/60000]\n",
      "Training loss: 1.874577  [18944/60000]\n",
      "Training loss: 1.794711  [19008/60000]\n",
      "Training loss: 1.808143  [19072/60000]\n",
      "Training loss: 1.813662  [19136/60000]\n",
      "Training loss: 1.841734  [19200/60000]\n",
      "Training loss: 1.761782  [19264/60000]\n",
      "Training loss: 1.847694  [19328/60000]\n",
      "Training loss: 1.861387  [19392/60000]\n",
      "Training loss: 1.774173  [19456/60000]\n",
      "Training loss: 1.796887  [19520/60000]\n",
      "Training loss: 1.844296  [19584/60000]\n",
      "Training loss: 1.887980  [19648/60000]\n",
      "Training loss: 1.793745  [19712/60000]\n",
      "Training loss: 1.834906  [19776/60000]\n",
      "Training loss: 1.832396  [19840/60000]\n",
      "Training loss: 1.810027  [19904/60000]\n",
      "Training loss: 1.884278  [19968/60000]\n",
      "Training loss: 1.799264  [20032/60000]\n",
      "Training loss: 1.838940  [20096/60000]\n",
      "Training loss: 1.792758  [20160/60000]\n",
      "Training loss: 1.868333  [20224/60000]\n",
      "Training loss: 1.816935  [20288/60000]\n",
      "Training loss: 1.805076  [20352/60000]\n",
      "Training loss: 1.833738  [20416/60000]\n",
      "Training loss: 1.821823  [20480/60000]\n",
      "Training loss: 1.789988  [20544/60000]\n",
      "Training loss: 1.827832  [20608/60000]\n",
      "Training loss: 1.815766  [20672/60000]\n",
      "Training loss: 1.799088  [20736/60000]\n",
      "Training loss: 1.819960  [20800/60000]\n",
      "Training loss: 1.804304  [20864/60000]\n",
      "Training loss: 1.847045  [20928/60000]\n",
      "Training loss: 1.900862  [20992/60000]\n",
      "Training loss: 1.753879  [21056/60000]\n",
      "Training loss: 1.757069  [21120/60000]\n",
      "Training loss: 1.824355  [21184/60000]\n",
      "Training loss: 1.804547  [21248/60000]\n",
      "Training loss: 1.816454  [21312/60000]\n",
      "Training loss: 1.804880  [21376/60000]\n",
      "Training loss: 1.835594  [21440/60000]\n",
      "Training loss: 1.692788  [21504/60000]\n",
      "Training loss: 1.780701  [21568/60000]\n",
      "Training loss: 1.832084  [21632/60000]\n",
      "Training loss: 1.853338  [21696/60000]\n",
      "Training loss: 1.899864  [21760/60000]\n",
      "Training loss: 1.830203  [21824/60000]\n",
      "Training loss: 1.812821  [21888/60000]\n",
      "Training loss: 1.787848  [21952/60000]\n",
      "Training loss: 1.847445  [22016/60000]\n",
      "Training loss: 1.795005  [22080/60000]\n",
      "Training loss: 1.766091  [22144/60000]\n",
      "Training loss: 1.813141  [22208/60000]\n",
      "Training loss: 1.776434  [22272/60000]\n",
      "Training loss: 1.820511  [22336/60000]\n",
      "Training loss: 1.833669  [22400/60000]\n",
      "Training loss: 1.748122  [22464/60000]\n",
      "Training loss: 1.783191  [22528/60000]\n",
      "Training loss: 1.837359  [22592/60000]\n",
      "Training loss: 1.844843  [22656/60000]\n",
      "Training loss: 1.781117  [22720/60000]\n",
      "Training loss: 1.836649  [22784/60000]\n",
      "Training loss: 1.833115  [22848/60000]\n",
      "Training loss: 1.728571  [22912/60000]\n",
      "Training loss: 1.824984  [22976/60000]\n",
      "Training loss: 1.834191  [23040/60000]\n",
      "Training loss: 1.811873  [23104/60000]\n",
      "Training loss: 1.764385  [23168/60000]\n",
      "Training loss: 1.812168  [23232/60000]\n",
      "Training loss: 1.820728  [23296/60000]\n",
      "Training loss: 1.896526  [23360/60000]\n",
      "Training loss: 1.834859  [23424/60000]\n",
      "Training loss: 1.787134  [23488/60000]\n",
      "Training loss: 1.847247  [23552/60000]\n",
      "Training loss: 1.770793  [23616/60000]\n",
      "Training loss: 1.709898  [23680/60000]\n",
      "Training loss: 1.768443  [23744/60000]\n",
      "Training loss: 1.820845  [23808/60000]\n",
      "Training loss: 1.785347  [23872/60000]\n",
      "Training loss: 1.781799  [23936/60000]\n",
      "Training loss: 1.790532  [24000/60000]\n",
      "Training loss: 1.845312  [24064/60000]\n",
      "Training loss: 1.766688  [24128/60000]\n",
      "Training loss: 1.834036  [24192/60000]\n",
      "Training loss: 1.844352  [24256/60000]\n",
      "Training loss: 1.765494  [24320/60000]\n",
      "Training loss: 1.794696  [24384/60000]\n",
      "Training loss: 1.796101  [24448/60000]\n",
      "Training loss: 1.734483  [24512/60000]\n",
      "Training loss: 1.843507  [24576/60000]\n",
      "Training loss: 1.850488  [24640/60000]\n",
      "Training loss: 1.859091  [24704/60000]\n",
      "Training loss: 1.755991  [24768/60000]\n",
      "Training loss: 1.741351  [24832/60000]\n",
      "Training loss: 1.817354  [24896/60000]\n",
      "Training loss: 1.741201  [24960/60000]\n",
      "Training loss: 1.897775  [25024/60000]\n",
      "Training loss: 1.776105  [25088/60000]\n",
      "Training loss: 1.812403  [25152/60000]\n",
      "Training loss: 1.819088  [25216/60000]\n",
      "Training loss: 1.753077  [25280/60000]\n",
      "Training loss: 1.754629  [25344/60000]\n",
      "Training loss: 1.763253  [25408/60000]\n",
      "Training loss: 1.807205  [25472/60000]\n",
      "Training loss: 1.714204  [25536/60000]\n",
      "Training loss: 1.754762  [25600/60000]\n",
      "Training loss: 1.752283  [25664/60000]\n",
      "Training loss: 1.770544  [25728/60000]\n",
      "Training loss: 1.788496  [25792/60000]\n",
      "Training loss: 1.790508  [25856/60000]\n",
      "Training loss: 1.845383  [25920/60000]\n",
      "Training loss: 1.848743  [25984/60000]\n",
      "Training loss: 1.865752  [26048/60000]\n",
      "Training loss: 1.789721  [26112/60000]\n",
      "Training loss: 1.779303  [26176/60000]\n",
      "Training loss: 1.722888  [26240/60000]\n",
      "Training loss: 1.684799  [26304/60000]\n",
      "Training loss: 1.731446  [26368/60000]\n",
      "Training loss: 1.871150  [26432/60000]\n",
      "Training loss: 1.823450  [26496/60000]\n",
      "Training loss: 1.807341  [26560/60000]\n",
      "Training loss: 1.716329  [26624/60000]\n",
      "Training loss: 1.884992  [26688/60000]\n",
      "Training loss: 1.866065  [26752/60000]\n",
      "Training loss: 1.826700  [26816/60000]\n",
      "Training loss: 1.791978  [26880/60000]\n",
      "Training loss: 1.719559  [26944/60000]\n",
      "Training loss: 1.724175  [27008/60000]\n",
      "Training loss: 1.844700  [27072/60000]\n",
      "Training loss: 1.851125  [27136/60000]\n",
      "Training loss: 1.811367  [27200/60000]\n",
      "Training loss: 1.794274  [27264/60000]\n",
      "Training loss: 1.730337  [27328/60000]\n",
      "Training loss: 1.804359  [27392/60000]\n",
      "Training loss: 1.789763  [27456/60000]\n",
      "Training loss: 1.817679  [27520/60000]\n",
      "Training loss: 1.778061  [27584/60000]\n",
      "Training loss: 1.785445  [27648/60000]\n",
      "Training loss: 1.786897  [27712/60000]\n",
      "Training loss: 1.756821  [27776/60000]\n",
      "Training loss: 1.744919  [27840/60000]\n",
      "Training loss: 1.759433  [27904/60000]\n",
      "Training loss: 1.758708  [27968/60000]\n",
      "Training loss: 1.777236  [28032/60000]\n",
      "Training loss: 1.742867  [28096/60000]\n",
      "Training loss: 1.760978  [28160/60000]\n",
      "Training loss: 1.836409  [28224/60000]\n",
      "Training loss: 1.759641  [28288/60000]\n",
      "Training loss: 1.850640  [28352/60000]\n",
      "Training loss: 1.819989  [28416/60000]\n",
      "Training loss: 1.813732  [28480/60000]\n",
      "Training loss: 1.798247  [28544/60000]\n",
      "Training loss: 1.759294  [28608/60000]\n",
      "Training loss: 1.825593  [28672/60000]\n",
      "Training loss: 1.730122  [28736/60000]\n",
      "Training loss: 1.720025  [28800/60000]\n",
      "Training loss: 1.784673  [28864/60000]\n",
      "Training loss: 1.816699  [28928/60000]\n",
      "Training loss: 1.825332  [28992/60000]\n",
      "Training loss: 1.677415  [29056/60000]\n",
      "Training loss: 1.804636  [29120/60000]\n",
      "Training loss: 1.760966  [29184/60000]\n",
      "Training loss: 1.703727  [29248/60000]\n",
      "Training loss: 1.771825  [29312/60000]\n",
      "Training loss: 1.784741  [29376/60000]\n",
      "Training loss: 1.740746  [29440/60000]\n",
      "Training loss: 1.856053  [29504/60000]\n",
      "Training loss: 1.869346  [29568/60000]\n",
      "Training loss: 1.820109  [29632/60000]\n",
      "Training loss: 1.855166  [29696/60000]\n",
      "Training loss: 1.710352  [29760/60000]\n",
      "Training loss: 1.814824  [29824/60000]\n",
      "Training loss: 1.803893  [29888/60000]\n",
      "Training loss: 1.824382  [29952/60000]\n",
      "Training loss: 1.781085  [30016/60000]\n",
      "Training loss: 1.757596  [30080/60000]\n",
      "Training loss: 1.766279  [30144/60000]\n",
      "Training loss: 1.737556  [30208/60000]\n",
      "Training loss: 1.738340  [30272/60000]\n",
      "Training loss: 1.736762  [30336/60000]\n",
      "Training loss: 1.762426  [30400/60000]\n",
      "Training loss: 1.841308  [30464/60000]\n",
      "Training loss: 1.754125  [30528/60000]\n",
      "Training loss: 1.792240  [30592/60000]\n",
      "Training loss: 1.756471  [30656/60000]\n",
      "Training loss: 1.839795  [30720/60000]\n",
      "Training loss: 1.758953  [30784/60000]\n",
      "Training loss: 1.755958  [30848/60000]\n",
      "Training loss: 1.724109  [30912/60000]\n",
      "Training loss: 1.715983  [30976/60000]\n",
      "Training loss: 1.775743  [31040/60000]\n",
      "Training loss: 1.797617  [31104/60000]\n",
      "Training loss: 1.792972  [31168/60000]\n",
      "Training loss: 1.799246  [31232/60000]\n",
      "Training loss: 1.705393  [31296/60000]\n",
      "Training loss: 1.735511  [31360/60000]\n",
      "Training loss: 1.684678  [31424/60000]\n",
      "Training loss: 1.785138  [31488/60000]\n",
      "Training loss: 1.753814  [31552/60000]\n",
      "Training loss: 1.771992  [31616/60000]\n",
      "Training loss: 1.724594  [31680/60000]\n",
      "Training loss: 1.739525  [31744/60000]\n",
      "Training loss: 1.801429  [31808/60000]\n",
      "Training loss: 1.828646  [31872/60000]\n",
      "Training loss: 1.813251  [31936/60000]\n",
      "Training loss: 1.796735  [32000/60000]\n",
      "Training loss: 1.813038  [32064/60000]\n",
      "Training loss: 1.770486  [32128/60000]\n",
      "Training loss: 1.690805  [32192/60000]\n",
      "Training loss: 1.782369  [32256/60000]\n",
      "Training loss: 1.733907  [32320/60000]\n",
      "Training loss: 1.758367  [32384/60000]\n",
      "Training loss: 1.744191  [32448/60000]\n",
      "Training loss: 1.761708  [32512/60000]\n",
      "Training loss: 1.797853  [32576/60000]\n",
      "Training loss: 1.769813  [32640/60000]\n",
      "Training loss: 1.765467  [32704/60000]\n",
      "Training loss: 1.706048  [32768/60000]\n",
      "Training loss: 1.728620  [32832/60000]\n",
      "Training loss: 1.821192  [32896/60000]\n",
      "Training loss: 1.752117  [32960/60000]\n",
      "Training loss: 1.738937  [33024/60000]\n",
      "Training loss: 1.760079  [33088/60000]\n",
      "Training loss: 1.779187  [33152/60000]\n",
      "Training loss: 1.803888  [33216/60000]\n",
      "Training loss: 1.763700  [33280/60000]\n",
      "Training loss: 1.710391  [33344/60000]\n",
      "Training loss: 1.833520  [33408/60000]\n",
      "Training loss: 1.840410  [33472/60000]\n",
      "Training loss: 1.758555  [33536/60000]\n",
      "Training loss: 1.786277  [33600/60000]\n",
      "Training loss: 1.823814  [33664/60000]\n",
      "Training loss: 1.781410  [33728/60000]\n",
      "Training loss: 1.850761  [33792/60000]\n",
      "Training loss: 1.803658  [33856/60000]\n",
      "Training loss: 1.779545  [33920/60000]\n",
      "Training loss: 1.766174  [33984/60000]\n",
      "Training loss: 1.736062  [34048/60000]\n",
      "Training loss: 1.729831  [34112/60000]\n",
      "Training loss: 1.778584  [34176/60000]\n",
      "Training loss: 1.787433  [34240/60000]\n",
      "Training loss: 1.734641  [34304/60000]\n",
      "Training loss: 1.808382  [34368/60000]\n",
      "Training loss: 1.751854  [34432/60000]\n",
      "Training loss: 1.678075  [34496/60000]\n",
      "Training loss: 1.705983  [34560/60000]\n",
      "Training loss: 1.839253  [34624/60000]\n",
      "Training loss: 1.860733  [34688/60000]\n",
      "Training loss: 1.766975  [34752/60000]\n",
      "Training loss: 1.729546  [34816/60000]\n",
      "Training loss: 1.780343  [34880/60000]\n",
      "Training loss: 1.746060  [34944/60000]\n",
      "Training loss: 1.732947  [35008/60000]\n",
      "Training loss: 1.695948  [35072/60000]\n",
      "Training loss: 1.746222  [35136/60000]\n",
      "Training loss: 1.745274  [35200/60000]\n",
      "Training loss: 1.798691  [35264/60000]\n",
      "Training loss: 1.713558  [35328/60000]\n",
      "Training loss: 1.720777  [35392/60000]\n",
      "Training loss: 1.688364  [35456/60000]\n",
      "Training loss: 1.669580  [35520/60000]\n",
      "Training loss: 1.745080  [35584/60000]\n",
      "Training loss: 1.811524  [35648/60000]\n",
      "Training loss: 1.694319  [35712/60000]\n",
      "Training loss: 1.698535  [35776/60000]\n",
      "Training loss: 1.682196  [35840/60000]\n",
      "Training loss: 1.703651  [35904/60000]\n",
      "Training loss: 1.742218  [35968/60000]\n",
      "Training loss: 1.688881  [36032/60000]\n",
      "Training loss: 1.768573  [36096/60000]\n",
      "Training loss: 1.765991  [36160/60000]\n",
      "Training loss: 1.722291  [36224/60000]\n",
      "Training loss: 1.798990  [36288/60000]\n",
      "Training loss: 1.804191  [36352/60000]\n",
      "Training loss: 1.753443  [36416/60000]\n",
      "Training loss: 1.762528  [36480/60000]\n",
      "Training loss: 1.747902  [36544/60000]\n",
      "Training loss: 1.722957  [36608/60000]\n",
      "Training loss: 1.750992  [36672/60000]\n",
      "Training loss: 1.637452  [36736/60000]\n",
      "Training loss: 1.717055  [36800/60000]\n",
      "Training loss: 1.771321  [36864/60000]\n",
      "Training loss: 1.776323  [36928/60000]\n",
      "Training loss: 1.708026  [36992/60000]\n",
      "Training loss: 1.716576  [37056/60000]\n",
      "Training loss: 1.691324  [37120/60000]\n",
      "Training loss: 1.672661  [37184/60000]\n",
      "Training loss: 1.783599  [37248/60000]\n",
      "Training loss: 1.791854  [37312/60000]\n",
      "Training loss: 1.755235  [37376/60000]\n",
      "Training loss: 1.811981  [37440/60000]\n",
      "Training loss: 1.689095  [37504/60000]\n",
      "Training loss: 1.765005  [37568/60000]\n",
      "Training loss: 1.684051  [37632/60000]\n",
      "Training loss: 1.724677  [37696/60000]\n",
      "Training loss: 1.781033  [37760/60000]\n",
      "Training loss: 1.776909  [37824/60000]\n",
      "Training loss: 1.669550  [37888/60000]\n",
      "Training loss: 1.749684  [37952/60000]\n",
      "Training loss: 1.665571  [38016/60000]\n",
      "Training loss: 1.613771  [38080/60000]\n",
      "Training loss: 1.739785  [38144/60000]\n",
      "Training loss: 1.785642  [38208/60000]\n",
      "Training loss: 1.702845  [38272/60000]\n",
      "Training loss: 1.686113  [38336/60000]\n",
      "Training loss: 1.785082  [38400/60000]\n",
      "Training loss: 1.773265  [38464/60000]\n",
      "Training loss: 1.715142  [38528/60000]\n",
      "Training loss: 1.790368  [38592/60000]\n",
      "Training loss: 1.720965  [38656/60000]\n",
      "Training loss: 1.819310  [38720/60000]\n",
      "Training loss: 1.749706  [38784/60000]\n",
      "Training loss: 1.719312  [38848/60000]\n",
      "Training loss: 1.664301  [38912/60000]\n",
      "Training loss: 1.720891  [38976/60000]\n",
      "Training loss: 1.765248  [39040/60000]\n",
      "Training loss: 1.718106  [39104/60000]\n",
      "Training loss: 1.675070  [39168/60000]\n",
      "Training loss: 1.788686  [39232/60000]\n",
      "Training loss: 1.685803  [39296/60000]\n",
      "Training loss: 1.680532  [39360/60000]\n",
      "Training loss: 1.718499  [39424/60000]\n",
      "Training loss: 1.736488  [39488/60000]\n",
      "Training loss: 1.775581  [39552/60000]\n",
      "Training loss: 1.831524  [39616/60000]\n",
      "Training loss: 1.740840  [39680/60000]\n",
      "Training loss: 1.623431  [39744/60000]\n",
      "Training loss: 1.702685  [39808/60000]\n",
      "Training loss: 1.809726  [39872/60000]\n",
      "Training loss: 1.721521  [39936/60000]\n",
      "Training loss: 1.755384  [40000/60000]\n",
      "Training loss: 1.739626  [40064/60000]\n",
      "Training loss: 1.791993  [40128/60000]\n",
      "Training loss: 1.785048  [40192/60000]\n",
      "Training loss: 1.705299  [40256/60000]\n",
      "Training loss: 1.759550  [40320/60000]\n",
      "Training loss: 1.689281  [40384/60000]\n",
      "Training loss: 1.739685  [40448/60000]\n",
      "Training loss: 1.718721  [40512/60000]\n",
      "Training loss: 1.754936  [40576/60000]\n",
      "Training loss: 1.776658  [40640/60000]\n",
      "Training loss: 1.707555  [40704/60000]\n",
      "Training loss: 1.786932  [40768/60000]\n",
      "Training loss: 1.715358  [40832/60000]\n",
      "Training loss: 1.776812  [40896/60000]\n",
      "Training loss: 1.777625  [40960/60000]\n",
      "Training loss: 1.721159  [41024/60000]\n",
      "Training loss: 1.806672  [41088/60000]\n",
      "Training loss: 1.635723  [41152/60000]\n",
      "Training loss: 1.783592  [41216/60000]\n",
      "Training loss: 1.717028  [41280/60000]\n",
      "Training loss: 1.756779  [41344/60000]\n",
      "Training loss: 1.795663  [41408/60000]\n",
      "Training loss: 1.696333  [41472/60000]\n",
      "Training loss: 1.712333  [41536/60000]\n",
      "Training loss: 1.617577  [41600/60000]\n",
      "Training loss: 1.647372  [41664/60000]\n",
      "Training loss: 1.603780  [41728/60000]\n",
      "Training loss: 1.722770  [41792/60000]\n",
      "Training loss: 1.648760  [41856/60000]\n",
      "Training loss: 1.752791  [41920/60000]\n",
      "Training loss: 1.694302  [41984/60000]\n",
      "Training loss: 1.756497  [42048/60000]\n",
      "Training loss: 1.731830  [42112/60000]\n",
      "Training loss: 1.795866  [42176/60000]\n",
      "Training loss: 1.747491  [42240/60000]\n",
      "Training loss: 1.739320  [42304/60000]\n",
      "Training loss: 1.707439  [42368/60000]\n",
      "Training loss: 1.692050  [42432/60000]\n",
      "Training loss: 1.662228  [42496/60000]\n",
      "Training loss: 1.779674  [42560/60000]\n",
      "Training loss: 1.659140  [42624/60000]\n",
      "Training loss: 1.738863  [42688/60000]\n",
      "Training loss: 1.712897  [42752/60000]\n",
      "Training loss: 1.617601  [42816/60000]\n",
      "Training loss: 1.719830  [42880/60000]\n",
      "Training loss: 1.683553  [42944/60000]\n",
      "Training loss: 1.728585  [43008/60000]\n",
      "Training loss: 1.720768  [43072/60000]\n",
      "Training loss: 1.685142  [43136/60000]\n",
      "Training loss: 1.727297  [43200/60000]\n",
      "Training loss: 1.693288  [43264/60000]\n",
      "Training loss: 1.722840  [43328/60000]\n",
      "Training loss: 1.646980  [43392/60000]\n",
      "Training loss: 1.626162  [43456/60000]\n",
      "Training loss: 1.656872  [43520/60000]\n",
      "Training loss: 1.644936  [43584/60000]\n",
      "Training loss: 1.771354  [43648/60000]\n",
      "Training loss: 1.726781  [43712/60000]\n",
      "Training loss: 1.682672  [43776/60000]\n",
      "Training loss: 1.678707  [43840/60000]\n",
      "Training loss: 1.690997  [43904/60000]\n",
      "Training loss: 1.778026  [43968/60000]\n",
      "Training loss: 1.723526  [44032/60000]\n",
      "Training loss: 1.667805  [44096/60000]\n",
      "Training loss: 1.738389  [44160/60000]\n",
      "Training loss: 1.768255  [44224/60000]\n",
      "Training loss: 1.758570  [44288/60000]\n",
      "Training loss: 1.670739  [44352/60000]\n",
      "Training loss: 1.772460  [44416/60000]\n",
      "Training loss: 1.687071  [44480/60000]\n",
      "Training loss: 1.717220  [44544/60000]\n",
      "Training loss: 1.741900  [44608/60000]\n",
      "Training loss: 1.712052  [44672/60000]\n",
      "Training loss: 1.694280  [44736/60000]\n",
      "Training loss: 1.680819  [44800/60000]\n",
      "Training loss: 1.675827  [44864/60000]\n",
      "Training loss: 1.602743  [44928/60000]\n",
      "Training loss: 1.729130  [44992/60000]\n",
      "Training loss: 1.694714  [45056/60000]\n",
      "Training loss: 1.707422  [45120/60000]\n",
      "Training loss: 1.821504  [45184/60000]\n",
      "Training loss: 1.666875  [45248/60000]\n",
      "Training loss: 1.718363  [45312/60000]\n",
      "Training loss: 1.702307  [45376/60000]\n",
      "Training loss: 1.687119  [45440/60000]\n",
      "Training loss: 1.793876  [45504/60000]\n",
      "Training loss: 1.725503  [45568/60000]\n",
      "Training loss: 1.576670  [45632/60000]\n",
      "Training loss: 1.704409  [45696/60000]\n",
      "Training loss: 1.789013  [45760/60000]\n",
      "Training loss: 1.659197  [45824/60000]\n",
      "Training loss: 1.533486  [45888/60000]\n",
      "Training loss: 1.776728  [45952/60000]\n",
      "Training loss: 1.662929  [46016/60000]\n",
      "Training loss: 1.693838  [46080/60000]\n",
      "Training loss: 1.682939  [46144/60000]\n",
      "Training loss: 1.708576  [46208/60000]\n",
      "Training loss: 1.708290  [46272/60000]\n",
      "Training loss: 1.671153  [46336/60000]\n",
      "Training loss: 1.618802  [46400/60000]\n",
      "Training loss: 1.749650  [46464/60000]\n",
      "Training loss: 1.736440  [46528/60000]\n",
      "Training loss: 1.746785  [46592/60000]\n",
      "Training loss: 1.647135  [46656/60000]\n",
      "Training loss: 1.724623  [46720/60000]\n",
      "Training loss: 1.694418  [46784/60000]\n",
      "Training loss: 1.701805  [46848/60000]\n",
      "Training loss: 1.637804  [46912/60000]\n",
      "Training loss: 1.742848  [46976/60000]\n",
      "Training loss: 1.699559  [47040/60000]\n",
      "Training loss: 1.693152  [47104/60000]\n",
      "Training loss: 1.753332  [47168/60000]\n",
      "Training loss: 1.680569  [47232/60000]\n",
      "Training loss: 1.714826  [47296/60000]\n",
      "Training loss: 1.654997  [47360/60000]\n",
      "Training loss: 1.737820  [47424/60000]\n",
      "Training loss: 1.674416  [47488/60000]\n",
      "Training loss: 1.724672  [47552/60000]\n",
      "Training loss: 1.662692  [47616/60000]\n",
      "Training loss: 1.721704  [47680/60000]\n",
      "Training loss: 1.714297  [47744/60000]\n",
      "Training loss: 1.657521  [47808/60000]\n",
      "Training loss: 1.659966  [47872/60000]\n",
      "Training loss: 1.775595  [47936/60000]\n",
      "Training loss: 1.614275  [48000/60000]\n",
      "Training loss: 1.745716  [48064/60000]\n",
      "Training loss: 1.663095  [48128/60000]\n",
      "Training loss: 1.756999  [48192/60000]\n",
      "Training loss: 1.547968  [48256/60000]\n",
      "Training loss: 1.616169  [48320/60000]\n",
      "Training loss: 1.691384  [48384/60000]\n",
      "Training loss: 1.695079  [48448/60000]\n",
      "Training loss: 1.673328  [48512/60000]\n",
      "Training loss: 1.663274  [48576/60000]\n",
      "Training loss: 1.759690  [48640/60000]\n",
      "Training loss: 1.750295  [48704/60000]\n",
      "Training loss: 1.677316  [48768/60000]\n",
      "Training loss: 1.679533  [48832/60000]\n",
      "Training loss: 1.624236  [48896/60000]\n",
      "Training loss: 1.752529  [48960/60000]\n",
      "Training loss: 1.646562  [49024/60000]\n",
      "Training loss: 1.722270  [49088/60000]\n",
      "Training loss: 1.699697  [49152/60000]\n",
      "Training loss: 1.692686  [49216/60000]\n",
      "Training loss: 1.671061  [49280/60000]\n",
      "Training loss: 1.674914  [49344/60000]\n",
      "Training loss: 1.697586  [49408/60000]\n",
      "Training loss: 1.627583  [49472/60000]\n",
      "Training loss: 1.735353  [49536/60000]\n",
      "Training loss: 1.766752  [49600/60000]\n",
      "Training loss: 1.604012  [49664/60000]\n",
      "Training loss: 1.763399  [49728/60000]\n",
      "Training loss: 1.648230  [49792/60000]\n",
      "Training loss: 1.805345  [49856/60000]\n",
      "Training loss: 1.688681  [49920/60000]\n",
      "Training loss: 1.706394  [49984/60000]\n",
      "Training loss: 1.621794  [50048/60000]\n",
      "Training loss: 1.618718  [50112/60000]\n",
      "Training loss: 1.665685  [50176/60000]\n",
      "Training loss: 1.492762  [50240/60000]\n",
      "Training loss: 1.700408  [50304/60000]\n",
      "Training loss: 1.681007  [50368/60000]\n",
      "Training loss: 1.645408  [50432/60000]\n",
      "Training loss: 1.737284  [50496/60000]\n",
      "Training loss: 1.638437  [50560/60000]\n",
      "Training loss: 1.788674  [50624/60000]\n",
      "Training loss: 1.584493  [50688/60000]\n",
      "Training loss: 1.706074  [50752/60000]\n",
      "Training loss: 1.743303  [50816/60000]\n",
      "Training loss: 1.674758  [50880/60000]\n",
      "Training loss: 1.639023  [50944/60000]\n",
      "Training loss: 1.696512  [51008/60000]\n",
      "Training loss: 1.571631  [51072/60000]\n",
      "Training loss: 1.709055  [51136/60000]\n",
      "Training loss: 1.659648  [51200/60000]\n",
      "Training loss: 1.677796  [51264/60000]\n",
      "Training loss: 1.619054  [51328/60000]\n",
      "Training loss: 1.740615  [51392/60000]\n",
      "Training loss: 1.573576  [51456/60000]\n",
      "Training loss: 1.740920  [51520/60000]\n",
      "Training loss: 1.597232  [51584/60000]\n",
      "Training loss: 1.681791  [51648/60000]\n",
      "Training loss: 1.727064  [51712/60000]\n",
      "Training loss: 1.597290  [51776/60000]\n",
      "Training loss: 1.715778  [51840/60000]\n",
      "Training loss: 1.694029  [51904/60000]\n",
      "Training loss: 1.541762  [51968/60000]\n",
      "Training loss: 1.658143  [52032/60000]\n",
      "Training loss: 1.764468  [52096/60000]\n",
      "Training loss: 1.634420  [52160/60000]\n",
      "Training loss: 1.638813  [52224/60000]\n",
      "Training loss: 1.632926  [52288/60000]\n",
      "Training loss: 1.641678  [52352/60000]\n",
      "Training loss: 1.713375  [52416/60000]\n",
      "Training loss: 1.667492  [52480/60000]\n",
      "Training loss: 1.697109  [52544/60000]\n",
      "Training loss: 1.648981  [52608/60000]\n",
      "Training loss: 1.644697  [52672/60000]\n",
      "Training loss: 1.770084  [52736/60000]\n",
      "Training loss: 1.667633  [52800/60000]\n",
      "Training loss: 1.611200  [52864/60000]\n",
      "Training loss: 1.732808  [52928/60000]\n",
      "Training loss: 1.657520  [52992/60000]\n",
      "Training loss: 1.681525  [53056/60000]\n",
      "Training loss: 1.689776  [53120/60000]\n",
      "Training loss: 1.651891  [53184/60000]\n",
      "Training loss: 1.616027  [53248/60000]\n",
      "Training loss: 1.623122  [53312/60000]\n",
      "Training loss: 1.741397  [53376/60000]\n",
      "Training loss: 1.520383  [53440/60000]\n",
      "Training loss: 1.668786  [53504/60000]\n",
      "Training loss: 1.658536  [53568/60000]\n",
      "Training loss: 1.623561  [53632/60000]\n",
      "Training loss: 1.507564  [53696/60000]\n",
      "Training loss: 1.682375  [53760/60000]\n",
      "Training loss: 1.566083  [53824/60000]\n",
      "Training loss: 1.685985  [53888/60000]\n",
      "Training loss: 1.670541  [53952/60000]\n",
      "Training loss: 1.608890  [54016/60000]\n",
      "Training loss: 1.674695  [54080/60000]\n",
      "Training loss: 1.691582  [54144/60000]\n",
      "Training loss: 1.642702  [54208/60000]\n",
      "Training loss: 1.645807  [54272/60000]\n",
      "Training loss: 1.593857  [54336/60000]\n",
      "Training loss: 1.725842  [54400/60000]\n",
      "Training loss: 1.633195  [54464/60000]\n",
      "Training loss: 1.682322  [54528/60000]\n",
      "Training loss: 1.656562  [54592/60000]\n",
      "Training loss: 1.645122  [54656/60000]\n",
      "Training loss: 1.559881  [54720/60000]\n",
      "Training loss: 1.667822  [54784/60000]\n",
      "Training loss: 1.577573  [54848/60000]\n",
      "Training loss: 1.743244  [54912/60000]\n",
      "Training loss: 1.600107  [54976/60000]\n",
      "Training loss: 1.605982  [55040/60000]\n",
      "Training loss: 1.634459  [55104/60000]\n",
      "Training loss: 1.596722  [55168/60000]\n",
      "Training loss: 1.552822  [55232/60000]\n",
      "Training loss: 1.616588  [55296/60000]\n",
      "Training loss: 1.614672  [55360/60000]\n",
      "Training loss: 1.709132  [55424/60000]\n",
      "Training loss: 1.722001  [55488/60000]\n",
      "Training loss: 1.703204  [55552/60000]\n",
      "Training loss: 1.656165  [55616/60000]\n",
      "Training loss: 1.759281  [55680/60000]\n",
      "Training loss: 1.656830  [55744/60000]\n",
      "Training loss: 1.709601  [55808/60000]\n",
      "Training loss: 1.618164  [55872/60000]\n",
      "Training loss: 1.647654  [55936/60000]\n",
      "Training loss: 1.583077  [56000/60000]\n",
      "Training loss: 1.681374  [56064/60000]\n",
      "Training loss: 1.599269  [56128/60000]\n",
      "Training loss: 1.704398  [56192/60000]\n",
      "Training loss: 1.614222  [56256/60000]\n",
      "Training loss: 1.711407  [56320/60000]\n",
      "Training loss: 1.686380  [56384/60000]\n",
      "Training loss: 1.690743  [56448/60000]\n",
      "Training loss: 1.623855  [56512/60000]\n",
      "Training loss: 1.635097  [56576/60000]\n",
      "Training loss: 1.652929  [56640/60000]\n",
      "Training loss: 1.606913  [56704/60000]\n",
      "Training loss: 1.560492  [56768/60000]\n",
      "Training loss: 1.644545  [56832/60000]\n",
      "Training loss: 1.502493  [56896/60000]\n",
      "Training loss: 1.611108  [56960/60000]\n",
      "Training loss: 1.646550  [57024/60000]\n",
      "Training loss: 1.623475  [57088/60000]\n",
      "Training loss: 1.654664  [57152/60000]\n",
      "Training loss: 1.591309  [57216/60000]\n",
      "Training loss: 1.593532  [57280/60000]\n",
      "Training loss: 1.611563  [57344/60000]\n",
      "Training loss: 1.712766  [57408/60000]\n",
      "Training loss: 1.580608  [57472/60000]\n",
      "Training loss: 1.695784  [57536/60000]\n",
      "Training loss: 1.617358  [57600/60000]\n",
      "Training loss: 1.650058  [57664/60000]\n",
      "Training loss: 1.591384  [57728/60000]\n",
      "Training loss: 1.595553  [57792/60000]\n",
      "Training loss: 1.564770  [57856/60000]\n",
      "Training loss: 1.621831  [57920/60000]\n",
      "Training loss: 1.518860  [57984/60000]\n",
      "Training loss: 1.665598  [58048/60000]\n",
      "Training loss: 1.582086  [58112/60000]\n",
      "Training loss: 1.631180  [58176/60000]\n",
      "Training loss: 1.606003  [58240/60000]\n",
      "Training loss: 1.529182  [58304/60000]\n",
      "Training loss: 1.534056  [58368/60000]\n",
      "Training loss: 1.655256  [58432/60000]\n",
      "Training loss: 1.599365  [58496/60000]\n",
      "Training loss: 1.605271  [58560/60000]\n",
      "Training loss: 1.571658  [58624/60000]\n",
      "Training loss: 1.608602  [58688/60000]\n",
      "Training loss: 1.638669  [58752/60000]\n",
      "Training loss: 1.680715  [58816/60000]\n",
      "Training loss: 1.611302  [58880/60000]\n",
      "Training loss: 1.659410  [58944/60000]\n",
      "Training loss: 1.632972  [59008/60000]\n",
      "Training loss: 1.645097  [59072/60000]\n",
      "Training loss: 1.606413  [59136/60000]\n",
      "Training loss: 1.714064  [59200/60000]\n",
      "Training loss: 1.622940  [59264/60000]\n",
      "Training loss: 1.736275  [59328/60000]\n",
      "Training loss: 1.636432  [59392/60000]\n",
      "Training loss: 1.676479  [59456/60000]\n",
      "Training loss: 1.602346  [59520/60000]\n",
      "Training loss: 1.655004  [59584/60000]\n",
      "Training loss: 1.676298  [59648/60000]\n",
      "Training loss: 1.562243  [59712/60000]\n",
      "Training loss: 1.552167  [59776/60000]\n",
      "Training loss: 1.684839  [59840/60000]\n",
      "Training loss: 1.628281  [59904/60000]\n",
      "Training loss: 1.639298  [29984/60000]\n",
      "\n",
      " Finished Epoch:  4\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## run the model\n",
    "epochs = 4\n",
    "for t in range(epochs): ## set number of epochs\n",
    "    train(loaded_train, model, loss_function, optimizer)\n",
    "    test_loss, correct = test(loaded_test, model, loss_function)\n",
    "    print('\\n Finished Epoch: ' , (t+1))\n",
    "    print (\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2yziEWzCsmxj",
    "outputId": "41cc3a11-ca16-4dda-c2aa-1dd1aa7e743a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images =  60000\n",
      "Number of testing images =  10000\n",
      "Training data classes\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
      "Accuracy = 72.76 %\n",
      "Average loss = 1.61\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct*100\n",
    "print('Number of training images = ' , count_train)\n",
    "print('Number of testing images = ' , count_test)\n",
    "print('Training data classes')\n",
    "print(training_data.classes) #classes inside the data:\n",
    "print(\"Accuracy = %.2f %%\" %(accuracy))\n",
    "print(\"Average loss = %.2f\" %(test_loss))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN4UUisIY7voG1dAXlF7hwJ",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
